{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "class main(object):\n",
    "    \n",
    "    def __init__(self, n_class, len_seq, dim_mfcc, batch_train, learning_rate, start_step, step_train, path_tfr_train, dir_log,\n",
    "                 batch_test, test_step, step_test, path_tfr_test):\n",
    "        self.n_class = n_class\n",
    "        self.len_seq = len_seq\n",
    "        self.dim_mfcc = dim_mfcc\n",
    "        self.batch_train = batch_train\n",
    "        self.learning_rate = learning_rate\n",
    "        self.start_step = start_step\n",
    "        self.step_train = step_train\n",
    "        self.path_tfr_train = path_tfr_train\n",
    "        self.dir_log = dir_log\n",
    "        self.batch_test = batch_test\n",
    "        self.test_step = test_step\n",
    "        self.step_test = step_test\n",
    "        self.path_tfr_test = path_tfr_test\n",
    "\n",
    "    #TFRecordsからデータセット取り出し\n",
    "    def input(self, path_tfr):\n",
    "        file_name_queue = tf.train.string_input_producer([path_tfr])\n",
    "        reader = tf.TFRecordReader()\n",
    "        _, serialized_example = reader.read(file_name_queue)\n",
    "\n",
    "        features = tf.parse_single_example(\n",
    "            serialized_example,\n",
    "            features={\n",
    "                'label': tf.FixedLenFeature([], tf.int64),\n",
    "                'data': tf.FixedLenFeature([], tf.string),\n",
    "            })\n",
    "\n",
    "        datas = tf.decode_raw(features['data'], tf.float32)\n",
    "        labels = tf.cast(features['label'], tf.int32)\n",
    "\n",
    "        datas = tf.reshape(datas, [self.len_seq, self.dim_mfcc])\n",
    "        labels = tf.reshape(labels, [1])\n",
    "\n",
    "        datas, labels = tf.train.batch(\n",
    "            [datas, labels],\n",
    "            batch_size=self.batch_test, capacity=1000+self.batch_test*self.dim_mfcc\n",
    "        )\n",
    "\n",
    "        return datas, labels\n",
    "\n",
    "    #TFRecordsからデータセット取り出し(shuffle)\n",
    "    def input_shuffle(self, path_tfr):\n",
    "        file_name_queue = tf.train.string_input_producer([path_tfr])\n",
    "        reader = tf.TFRecordReader()\n",
    "        _, serialized_example = reader.read(file_name_queue)\n",
    "\n",
    "        features = tf.parse_single_example(\n",
    "            serialized_example,\n",
    "            features={\n",
    "                'label': tf.FixedLenFeature([], tf.int64),\n",
    "                'data': tf.FixedLenFeature([], tf.string),\n",
    "            })\n",
    "        \n",
    "        datas = tf.decode_raw(features['data'], tf.float32)\n",
    "        labels = tf.cast(features['label'], tf.int32)\n",
    "        \n",
    "        datas = tf.reshape(datas, [self.len_seq, self.dim_mfcc])\n",
    "        labels = tf.reshape(labels, [1])\n",
    "\n",
    "        datas, labels = tf.train.shuffle_batch(\n",
    "            [datas, labels],\n",
    "            batch_size=self.batch_train, capacity=1000+self.batch_train*self.dim_mfcc,\n",
    "            min_after_dequeue=1000\n",
    "        )\n",
    "            \n",
    "        return datas, labels\n",
    "    \n",
    "    #dirで指定されたパスが存在しない場合ディレクトリ作成\n",
    "    def make_dir(self,dir,format=False):\n",
    "        if not os.path.exists(dir):\n",
    "            os.makedirs(dir)\n",
    "        if format and os.path.exists(dir):\n",
    "            shutil.rmtree(dir)\n",
    "\n",
    "    #tensorboardのサマリに追加する\n",
    "    def variable_summaries(self, var):\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar('mean', mean)\n",
    "            with tf.name_scope('stddev'):\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            tf.summary.histogram('histogram', var)\n",
    "        \n",
    "    #重みベクトルを初期化して返す\n",
    "    def variable(self, name, shape, stddev):\n",
    "        var = tf.get_variable(name, shape=shape, initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "        return var\n",
    "        \n",
    "    #Linear\n",
    "    def Linear(self, x, n_inputs, n_units, stddev, l_name):\n",
    "        with tf.variable_scope(l_name) as scope:\n",
    "            weights = self.variable('weights', shape=[n_inputs, n_units], stddev=stddev)\n",
    "            biases = tf.get_variable('biases', shape=[n_units], initializer=tf.constant_initializer(0.0))\n",
    "            linear = tf.nn.bias_add(tf.matmul(x, weights), biases, name=scope.name)\n",
    "            self.variable_summaries(linear)\n",
    "            return linear\n",
    "        \n",
    "    #LSTM\n",
    "    def LSTM(self, x, n_units, l_name):\n",
    "        with tf.variable_scope(l_name) as scope:\n",
    "            lstm_cell = rnn.BasicLSTMCell(n_units, forget_bias=1.0)\n",
    "            outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "            self.variable_summaries(outputs[-1])\n",
    "            return outputs, states\n",
    "\n",
    "    #Model\n",
    "    def model(self, x):\n",
    "        x_unstack = tf.unstack(x, self.len_seq, 1)\n",
    "        x_unstack_rev = x_unstack[::-1]\n",
    "\n",
    "        lstm1 = self.LSTM(x_unstack, n_units=256, l_name='lstm1')\n",
    "        #lstm2 = self.LSTM(lstm1[0], n_units=256, l_name='lstm2')\n",
    "        \n",
    "        lstm_rev1 = self.LSTM(x_unstack_rev, n_units=256, l_name='lstm_rev1')\n",
    "        #lstm_rev2 = self.LSTM(lstm_rev1[0], n_units=256, l_name='lstm_rev2')\n",
    "\n",
    "        concatenate = tf.concat([lstm1[0][-1], lstm_rev1[0][-1]], axis=1)\n",
    "        \n",
    "        linear1 = self.Linear(concatenate, n_inputs=512, n_units=self.n_class, stddev=0.01, l_name='linear1')\n",
    "        return linear1\n",
    "        \n",
    "    #トレーニング\n",
    "    def train(self):\n",
    "        sess = tf.InteractiveSession()\n",
    "        \n",
    "        x = tf.placeholder(tf.float32, shape=[None, self.len_seq, self.dim_mfcc])\n",
    "        y_ = tf.placeholder(tf.float32, shape=[None, self.n_class])\n",
    "\n",
    "        preds = self.model(x)\n",
    "        \n",
    "        with tf.name_scope('cross_entropy'):\n",
    "            cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=preds, labels=y_))\n",
    "            tf.add_to_collection('losses', cross_entropy)\n",
    "            error=tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "            self.variable_summaries(error)\n",
    "\n",
    "        with tf.name_scope('accuracy'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(error)\n",
    "            correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(y_, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "            self.variable_summaries(accuracy)\n",
    "            \n",
    "        merged = tf.summary.merge_all()\n",
    "        dir_log = os.path.join(self.dir_log, 'train')\n",
    "        self.make_dir(dir_log)\n",
    "        writer = tf.summary.FileWriter(dir_log, sess.graph)\n",
    "        \n",
    "        saver = tf.train.Saver(max_to_keep=1000)\n",
    "        \n",
    "        datas, labels = self.input_shuffle(self.path_tfr_train)\n",
    "        labels = tf.one_hot(labels, depth=self.n_class, dtype=tf.float32)\n",
    "        \n",
    "        n_training_iters = self.step_train * self.batch_train\n",
    "        init_op = [tf.global_variables_initializer(), tf.local_variables_initializer()]\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_op)\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "            \n",
    "            if(not self.start_step == 1):\n",
    "                saver.restore(sess, os.path.join(self.dir_log, 'save_files/model.ckpt-'+str(self.start_step)))\n",
    "            step = self.start_step\n",
    "            while step * self.batch_train <= n_training_iters:\n",
    "                batch = sess.run([datas, labels])\n",
    "                batch[1] = batch[1].reshape([-1, self.n_class])\n",
    "                sess.run(optimizer, feed_dict={x: batch[0], y_:batch[1]})\n",
    "                if step % 100 == 0:\n",
    "                    run_options  = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                    run_metadata = tf.RunMetadata()\n",
    "                    summary = sess.run(merged,\n",
    "                        feed_dict={x: batch[0], y_:batch[1]},\n",
    "                        options=run_options, run_metadata=run_metadata)\n",
    "                    writer.add_summary(summary, step)\n",
    "                    acc = sess.run(accuracy, feed_dict={x: batch[0], y_: batch[1]})\n",
    "                    loss = sess.run(cross_entropy, feed_dict={x: batch[0], y_: batch[1]})\n",
    "                    print('step: {} / loss: {:.6f} / acc: {:.5f}'.format(step, loss, acc))\n",
    "                    dir_ckpt = os.path.join(self.dir_log, 'save_files')\n",
    "                    self.make_dir(dir_ckpt)\n",
    "                    saver.save(sess, os.path.join(dir_ckpt, 'model.ckpt'), global_step=(step))\n",
    "                step += 1\n",
    "\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "            writer.close()\n",
    "            \n",
    "    #テスト\n",
    "    def test(self):\n",
    "        sess = tf.InteractiveSession()\n",
    "\n",
    "        x = tf.placeholder(tf.float32, shape=[None, self.len_seq, self.dim_mfcc])\n",
    "        y_ = tf.placeholder(tf.float32, shape=[None, self.n_class])\n",
    "\n",
    "        preds = self.model(x)\n",
    "\n",
    "        with tf.name_scope('cross_entropy'):\n",
    "            cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=preds, labels=y_))\n",
    "            tf.add_to_collection('losses', cross_entropy)\n",
    "            error=tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "            self.variable_summaries(error)\n",
    "\n",
    "        with tf.name_scope('accuracy'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(error)\n",
    "            correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(y_, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "            self.variable_summaries(accuracy)\n",
    "\n",
    "        merged = tf.summary.merge_all()\n",
    "        dir_log = os.path.join(self.dir_log, 'test')\n",
    "        self.make_dir(dir_log)\n",
    "        writer = tf.summary.FileWriter(dir_log, sess.graph)\n",
    "\n",
    "        saver = tf.train.Saver(max_to_keep=1000)\n",
    "\n",
    "        datas, labels = self.input(self.path_tfr_test)\n",
    "        labels = tf.one_hot(labels, depth=self.n_class, dtype=tf.float32)\n",
    "\n",
    "        n_test_iters = self.step_test * self.batch_test\n",
    "        init_op = [tf.global_variables_initializer(), tf.local_variables_initializer()]\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_op)\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "            saver.restore(sess, os.path.join(self.dir_log, 'save_files/model.ckpt-'+str(self.test_step)))\n",
    "            step = 1\n",
    "            acc_all = 0.0\n",
    "            while step * self.batch_test <= n_test_iters:\n",
    "                batch = sess.run([datas, labels])\n",
    "                batch[1] = batch[1].reshape([-1, self.n_class])\n",
    "                sess.run(optimizer, feed_dict={x: batch[0], y_:batch[1]})\n",
    "                run_options  = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary = sess.run(merged,\n",
    "                    feed_dict={x: batch[0], y_:batch[1]},\n",
    "                    options=run_options, run_metadata=run_metadata)\n",
    "                writer.add_summary(summary, step)\n",
    "                acc = sess.run(accuracy, feed_dict={x: batch[0], y_: batch[1]})\n",
    "                loss = sess.run(cross_entropy, feed_dict={x: batch[0], y_: batch[1]})\n",
    "                print('step: {} / loss: {:.6f} / test_acc: {:.5f}'.format(step, loss, acc))\n",
    "                step += 1\n",
    "\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "            writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 100 / loss: 2.922967 / acc: 0.03333\n",
      "step: 200 / loss: 2.051805 / acc: 0.46667\n",
      "step: 300 / loss: 0.879652 / acc: 0.86667\n",
      "step: 400 / loss: 0.198623 / acc: 1.00000\n",
      "step: 500 / loss: 0.046468 / acc: 1.00000\n",
      "step: 600 / loss: 0.028530 / acc: 1.00000\n",
      "step: 700 / loss: 0.016011 / acc: 1.00000\n",
      "step: 800 / loss: 0.009124 / acc: 1.00000\n",
      "step: 900 / loss: 0.179624 / acc: 1.00000\n",
      "step: 1000 / loss: 0.036639 / acc: 1.00000\n",
      "step: 1100 / loss: 0.013676 / acc: 1.00000\n",
      "step: 1200 / loss: 0.007700 / acc: 1.00000\n",
      "step: 1300 / loss: 0.005904 / acc: 1.00000\n",
      "step: 1400 / loss: 0.005123 / acc: 1.00000\n",
      "step: 1500 / loss: 0.003529 / acc: 1.00000\n",
      "step: 1600 / loss: 0.003411 / acc: 1.00000\n",
      "step: 1700 / loss: 0.002500 / acc: 1.00000\n",
      "step: 1800 / loss: 0.002675 / acc: 1.00000\n",
      "step: 1900 / loss: 0.001484 / acc: 1.00000\n",
      "step: 2000 / loss: 0.001909 / acc: 1.00000\n",
      "step: 2100 / loss: 0.001524 / acc: 1.00000\n",
      "step: 2200 / loss: 0.001440 / acc: 1.00000\n",
      "step: 2300 / loss: 0.001200 / acc: 1.00000\n",
      "step: 2400 / loss: 0.001163 / acc: 1.00000\n",
      "step: 2500 / loss: 0.001025 / acc: 1.00000\n",
      "step: 2600 / loss: 0.000974 / acc: 1.00000\n",
      "step: 2700 / loss: 0.000711 / acc: 1.00000\n",
      "step: 2800 / loss: 0.000661 / acc: 1.00000\n",
      "step: 2900 / loss: 0.000704 / acc: 1.00000\n",
      "step: 3000 / loss: 0.000572 / acc: 1.00000\n"
     ]
    }
   ],
   "source": [
    "m = main(\n",
    "    n_class = 22,\n",
    "    len_seq = 300,\n",
    "    dim_mfcc = 39,\n",
    "    batch_train = 30,\n",
    "    learning_rate = 0.001,\n",
    "    start_step = 1,\n",
    "    step_train = 3000,\n",
    "    path_tfr_train = '/media/ikesan009/B418B4D718B499B6/research/CENSREC/dataset/train-male.tfrecords',\n",
    "    dir_log = '/media/ikesan009/B418B4D718B499B6/research/CENSREC/log',\n",
    "    batch_test = 25,\n",
    "    test_step = 300,\n",
    "    step_test = 11,\n",
    "    path_tfr_test = '/media/ikesan009/B418B4D718B499B6/research/CENSREC/dataset/test-male.tfrecords'\n",
    "    )\n",
    "m.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
