{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "class main(object):\n",
    "    \n",
    "    PATH_DATASET = '/media/ikesan009/B418B4D718B499B6/research/CENSREC/dataset'\n",
    "    MFCC_DIM = 39\n",
    "    BATCH_SIZE = 30\n",
    "    LEN_SEQ = 300\n",
    "    N_CLASS = 22\n",
    "    \n",
    "    #TFRecordsからデータセット取り出し\n",
    "    def input(self, path_tfr, MFCC_DIM, BATCH_SIZE):\n",
    "        file_name_queue = tf.train.string_input_producer([path_tfr])\n",
    "        reader = tf.TFRecordReader()\n",
    "        _, serialized_example = reader.read(file_name_queue)\n",
    "        features = tf.parse_single_example(\n",
    "            serialized_example,\n",
    "            features={\n",
    "                'label': tf.FixedLenFeature([], tf.int64),\n",
    "                'data': tf.FixedLenFeature([], tf.string),\n",
    "            })\n",
    "        \n",
    "        datas = tf.decode_raw(features['data'], tf.float32)\n",
    "        labels = tf.cast(features['label'], tf.int32)\n",
    "        \n",
    "        datas = tf.reshape(datas, [self.LEN_SEQ, MFCC_DIM])\n",
    "        labels = tf.reshape(labels, [1])\n",
    "\n",
    "        datas, labels = tf.train.shuffle_batch(\n",
    "            [datas, labels],\n",
    "            batch_size=BATCH_SIZE,capacity=1000+MFCC_DIM*BATCH_SIZE,min_after_dequeue=1000\n",
    "        )\n",
    "            \n",
    "        return datas, labels\n",
    "\n",
    "    #RNN\n",
    "    def RNN(self, x):\n",
    "        x = tf.unstack(x, self.LEN_SEQ, 1)\n",
    "\n",
    "        # LSTMセルを定義する\n",
    "        lstm_cell = rnn.BasicLSTMCell(256, forget_bias=1.0)\n",
    "\n",
    "        # モデルの定義。各タイムステップの出力値と状態が返される\n",
    "        outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "        # 変数定義\n",
    "        weight = tf.Variable(tf.random_normal([256, self.N_CLASS]))\n",
    "        bias = tf.Variable(tf.random_normal([self.N_CLASS]))\n",
    "\n",
    "        return tf.matmul(outputs[-1], weight) + bias\n",
    "    \n",
    "    #トレーニング部\n",
    "    def train(self):\n",
    "        x = tf.placeholder(tf.float32, shape=[None, self.LEN_SEQ, self.MFCC_DIM])\n",
    "        y_ = tf.placeholder(tf.float32, shape=[None, self.N_CLASS])\n",
    "\n",
    "        preds = self.RNN(x)\n",
    "\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=y_))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "        \n",
    "        correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "        data, label = self.input(os.path.join(self.PATH_DATASET, 'train-male.tfrecords'), self.MFCC_DIM, self.BATCH_SIZE)\n",
    "        label = tf.one_hot(label, depth = self.N_CLASS, dtype = tf.float32)\n",
    "        \n",
    "        n_training_iters = 100000 * self.BATCH_SIZE\n",
    "        init_op = [tf.global_variables_initializer(), tf.local_variables_initializer()]\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_op)\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "            step = 1\n",
    "            while step * self.BATCH_SIZE < n_training_iters:\n",
    "                batch = sess.run([data, label])\n",
    "                batch[1] = batch[1].reshape([-1, self.N_CLASS])\n",
    "                sess.run(optimizer, feed_dict={x: batch[0], y_:batch[1]})\n",
    "                if step % 10 == 0:\n",
    "                    acc = sess.run(accuracy, feed_dict={x: batch[0], y_: batch[1]})\n",
    "                    loss = sess.run(cost, feed_dict={x: batch[0], y_: batch[1]})\n",
    "                    print('step: {} / loss: {:.6f} / acc: {:.5f}'.format(step, loss, acc))\n",
    "                step += 1\n",
    "\n",
    "            \"\"\"\n",
    "            # テスト\n",
    "            test_len = 128\n",
    "            test_batch = self.input(os.path.join(self.PATH_DATASET, 'test-male.tfrecords'), self.MFCC_DIM, test_len)\n",
    "            test_data, test_label = sess.run(test_batch)\n",
    "            test_acc = sess.run(accuracy, feed_dict={x: test_data, y: test_label})\n",
    "            print(\"Test Accuracy: {}\".format(test_acc))\n",
    "            \"\"\"\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-9d8567c597ad>:62: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "step: 10 / loss: 3.286863 / acc: 0.00000\n",
      "step: 20 / loss: 3.223075 / acc: 0.03333\n",
      "step: 30 / loss: 3.019622 / acc: 0.06667\n",
      "step: 40 / loss: 3.189710 / acc: 0.00000\n",
      "step: 50 / loss: 3.162052 / acc: 0.06667\n",
      "step: 60 / loss: 3.216613 / acc: 0.03333\n",
      "step: 70 / loss: 3.128908 / acc: 0.03333\n",
      "step: 80 / loss: 3.132331 / acc: 0.10000\n",
      "step: 90 / loss: 3.120088 / acc: 0.00000\n",
      "step: 100 / loss: 3.110112 / acc: 0.00000\n",
      "step: 110 / loss: 3.144465 / acc: 0.06667\n",
      "step: 120 / loss: 3.154385 / acc: 0.06667\n",
      "step: 130 / loss: 3.143729 / acc: 0.06667\n",
      "step: 140 / loss: 3.071159 / acc: 0.06667\n",
      "step: 150 / loss: 3.100992 / acc: 0.03333\n",
      "step: 160 / loss: 3.147342 / acc: 0.06667\n",
      "step: 170 / loss: 3.092034 / acc: 0.03333\n",
      "step: 180 / loss: 3.143621 / acc: 0.03333\n",
      "step: 190 / loss: 3.069518 / acc: 0.03333\n",
      "step: 200 / loss: 3.075239 / acc: 0.03333\n",
      "step: 210 / loss: 3.123729 / acc: 0.03333\n",
      "step: 220 / loss: 3.104248 / acc: 0.03333\n",
      "step: 230 / loss: 3.141355 / acc: 0.00000\n",
      "step: 240 / loss: 3.107514 / acc: 0.00000\n",
      "step: 250 / loss: 3.020368 / acc: 0.06667\n",
      "step: 260 / loss: 3.100591 / acc: 0.00000\n",
      "step: 270 / loss: 3.064380 / acc: 0.03333\n",
      "step: 280 / loss: 3.155360 / acc: 0.00000\n",
      "step: 290 / loss: 3.141284 / acc: 0.03333\n",
      "step: 300 / loss: 3.080092 / acc: 0.06667\n",
      "step: 310 / loss: 3.098187 / acc: 0.06667\n",
      "step: 320 / loss: 3.103044 / acc: 0.03333\n",
      "step: 330 / loss: 3.088060 / acc: 0.06667\n",
      "step: 340 / loss: 3.191758 / acc: 0.03333\n",
      "step: 350 / loss: 3.115958 / acc: 0.03333\n",
      "step: 360 / loss: 3.128685 / acc: 0.06667\n",
      "step: 370 / loss: 3.125190 / acc: 0.06667\n",
      "step: 380 / loss: 3.092462 / acc: 0.03333\n",
      "step: 390 / loss: 3.147048 / acc: 0.00000\n",
      "step: 400 / loss: 3.140329 / acc: 0.06667\n",
      "step: 410 / loss: 3.142441 / acc: 0.00000\n",
      "step: 420 / loss: 3.101959 / acc: 0.06667\n",
      "step: 430 / loss: 3.050229 / acc: 0.13333\n",
      "step: 440 / loss: 3.141891 / acc: 0.03333\n",
      "step: 450 / loss: 3.158620 / acc: 0.03333\n",
      "step: 460 / loss: 3.095016 / acc: 0.03333\n",
      "step: 470 / loss: 3.108440 / acc: 0.03333\n",
      "step: 480 / loss: 3.092027 / acc: 0.03333\n",
      "step: 490 / loss: 3.090828 / acc: 0.06667\n",
      "step: 500 / loss: 3.066041 / acc: 0.03333\n",
      "step: 510 / loss: 3.122669 / acc: 0.00000\n",
      "step: 520 / loss: 3.176369 / acc: 0.00000\n",
      "step: 530 / loss: 3.095242 / acc: 0.06667\n",
      "step: 540 / loss: 3.086020 / acc: 0.06667\n",
      "step: 550 / loss: 3.055237 / acc: 0.13333\n",
      "step: 560 / loss: 3.127133 / acc: 0.06667\n",
      "step: 570 / loss: 3.065212 / acc: 0.13333\n",
      "step: 580 / loss: 3.134645 / acc: 0.00000\n",
      "step: 590 / loss: 3.169310 / acc: 0.03333\n",
      "step: 600 / loss: 3.095925 / acc: 0.10000\n",
      "step: 610 / loss: 3.108927 / acc: 0.06667\n",
      "step: 620 / loss: 3.029523 / acc: 0.00000\n",
      "step: 630 / loss: 3.102872 / acc: 0.03333\n",
      "step: 640 / loss: 3.111706 / acc: 0.00000\n",
      "step: 650 / loss: 3.035771 / acc: 0.06667\n",
      "step: 660 / loss: 3.099191 / acc: 0.00000\n",
      "step: 670 / loss: 3.138984 / acc: 0.10000\n",
      "step: 680 / loss: 3.107536 / acc: 0.00000\n",
      "step: 690 / loss: 3.114348 / acc: 0.10000\n",
      "step: 700 / loss: 3.101493 / acc: 0.06667\n",
      "step: 710 / loss: 3.047984 / acc: 0.10000\n",
      "step: 720 / loss: 3.135788 / acc: 0.03333\n",
      "step: 730 / loss: 3.111562 / acc: 0.03333\n",
      "step: 740 / loss: 3.107702 / acc: 0.03333\n",
      "step: 750 / loss: 3.171222 / acc: 0.03333\n",
      "step: 760 / loss: 3.097817 / acc: 0.06667\n",
      "step: 770 / loss: 3.127092 / acc: 0.00000\n",
      "step: 780 / loss: 3.072558 / acc: 0.06667\n",
      "step: 790 / loss: 3.136307 / acc: 0.06667\n",
      "step: 800 / loss: 3.125699 / acc: 0.06667\n",
      "step: 810 / loss: 3.146102 / acc: 0.03333\n",
      "step: 820 / loss: 3.111148 / acc: 0.06667\n",
      "step: 830 / loss: 3.090291 / acc: 0.06667\n",
      "step: 840 / loss: 3.093540 / acc: 0.13333\n",
      "step: 850 / loss: 3.073719 / acc: 0.03333\n",
      "step: 860 / loss: 3.122607 / acc: 0.03333\n",
      "step: 870 / loss: 3.061016 / acc: 0.03333\n",
      "step: 880 / loss: 3.134584 / acc: 0.03333\n",
      "step: 890 / loss: 3.089746 / acc: 0.10000\n",
      "step: 900 / loss: 3.070146 / acc: 0.06667\n",
      "step: 910 / loss: 3.156613 / acc: 0.03333\n",
      "step: 920 / loss: 3.147756 / acc: 0.06667\n",
      "step: 930 / loss: 3.125247 / acc: 0.06667\n",
      "step: 940 / loss: 3.146746 / acc: 0.06667\n",
      "step: 950 / loss: 3.075846 / acc: 0.06667\n",
      "step: 960 / loss: 3.109328 / acc: 0.00000\n",
      "step: 970 / loss: 3.129986 / acc: 0.03333\n",
      "step: 980 / loss: 3.077008 / acc: 0.00000\n",
      "step: 990 / loss: 3.093380 / acc: 0.03333\n",
      "step: 1000 / loss: 3.123777 / acc: 0.00000\n",
      "step: 1010 / loss: 3.085442 / acc: 0.06667\n",
      "step: 1020 / loss: 3.097132 / acc: 0.03333\n",
      "step: 1030 / loss: 3.089515 / acc: 0.03333\n",
      "step: 1040 / loss: 3.096384 / acc: 0.10000\n",
      "step: 1050 / loss: 3.076968 / acc: 0.06667\n",
      "step: 1060 / loss: 3.083179 / acc: 0.03333\n",
      "step: 1070 / loss: 3.099852 / acc: 0.06667\n",
      "step: 1080 / loss: 3.100276 / acc: 0.03333\n",
      "step: 1090 / loss: 3.082104 / acc: 0.00000\n",
      "step: 1100 / loss: 3.079427 / acc: 0.03333\n",
      "step: 1110 / loss: 3.083981 / acc: 0.03333\n",
      "step: 1120 / loss: 3.082665 / acc: 0.10000\n",
      "step: 1130 / loss: 3.093166 / acc: 0.06667\n",
      "step: 1140 / loss: 3.082049 / acc: 0.03333\n",
      "step: 1150 / loss: 3.111040 / acc: 0.03333\n",
      "step: 1160 / loss: 3.121423 / acc: 0.03333\n",
      "step: 1170 / loss: 3.080637 / acc: 0.06667\n",
      "step: 1180 / loss: 3.027173 / acc: 0.10000\n",
      "step: 1190 / loss: 3.046817 / acc: 0.06667\n",
      "step: 1200 / loss: 3.083874 / acc: 0.10000\n",
      "step: 1210 / loss: 3.167843 / acc: 0.00000\n",
      "step: 1220 / loss: 3.134885 / acc: 0.00000\n",
      "step: 1230 / loss: 3.086309 / acc: 0.03333\n",
      "step: 1240 / loss: 3.108490 / acc: 0.10000\n",
      "step: 1250 / loss: 3.091751 / acc: 0.06667\n",
      "step: 1260 / loss: 3.110139 / acc: 0.03333\n",
      "step: 1270 / loss: 3.111403 / acc: 0.03333\n",
      "step: 1280 / loss: 3.141131 / acc: 0.03333\n",
      "step: 1290 / loss: 3.092309 / acc: 0.00000\n",
      "step: 1300 / loss: 3.101197 / acc: 0.03333\n",
      "step: 1310 / loss: 3.122188 / acc: 0.06667\n",
      "step: 1320 / loss: 3.131778 / acc: 0.03333\n",
      "step: 1330 / loss: 3.161093 / acc: 0.00000\n",
      "step: 1340 / loss: 3.086504 / acc: 0.03333\n",
      "step: 1350 / loss: 3.114816 / acc: 0.03333\n",
      "step: 1360 / loss: 3.105321 / acc: 0.10000\n",
      "step: 1370 / loss: 3.083342 / acc: 0.13333\n",
      "step: 1380 / loss: 3.071358 / acc: 0.03333\n",
      "step: 1390 / loss: 3.136498 / acc: 0.03333\n",
      "step: 1400 / loss: 3.106545 / acc: 0.03333\n",
      "step: 1410 / loss: 3.039117 / acc: 0.10000\n",
      "step: 1420 / loss: 3.123007 / acc: 0.00000\n",
      "step: 1430 / loss: 3.148814 / acc: 0.00000\n",
      "step: 1440 / loss: 3.136363 / acc: 0.03333\n",
      "step: 1450 / loss: 3.139483 / acc: 0.00000\n",
      "step: 1460 / loss: 3.133686 / acc: 0.00000\n",
      "step: 1470 / loss: 3.083601 / acc: 0.03333\n",
      "step: 1480 / loss: 3.090929 / acc: 0.06667\n",
      "step: 1490 / loss: 3.106250 / acc: 0.06667\n",
      "step: 1500 / loss: 3.112230 / acc: 0.03333\n",
      "step: 1510 / loss: 3.122267 / acc: 0.10000\n",
      "step: 1520 / loss: 3.115743 / acc: 0.00000\n",
      "step: 1530 / loss: 3.069118 / acc: 0.06667\n",
      "step: 1540 / loss: 3.111027 / acc: 0.03333\n",
      "step: 1550 / loss: 3.067731 / acc: 0.06667\n",
      "step: 1560 / loss: 3.086131 / acc: 0.00000\n",
      "step: 1570 / loss: 3.100256 / acc: 0.03333\n",
      "step: 1580 / loss: 3.105008 / acc: 0.06667\n",
      "step: 1590 / loss: 3.132596 / acc: 0.06667\n",
      "step: 1600 / loss: 3.100893 / acc: 0.00000\n",
      "step: 1610 / loss: 3.089461 / acc: 0.10000\n",
      "step: 1620 / loss: 3.092110 / acc: 0.03333\n",
      "step: 1630 / loss: 3.118814 / acc: 0.06667\n",
      "step: 1640 / loss: 3.092742 / acc: 0.10000\n",
      "step: 1650 / loss: 3.069789 / acc: 0.03333\n",
      "step: 1660 / loss: 3.110253 / acc: 0.03333\n",
      "step: 1670 / loss: 3.091415 / acc: 0.00000\n",
      "step: 1680 / loss: 3.115565 / acc: 0.10000\n",
      "step: 1690 / loss: 3.103059 / acc: 0.03333\n",
      "step: 1700 / loss: 3.072987 / acc: 0.03333\n",
      "step: 1710 / loss: 3.099109 / acc: 0.03333\n",
      "step: 1720 / loss: 3.112802 / acc: 0.10000\n",
      "step: 1730 / loss: 3.057952 / acc: 0.00000\n",
      "step: 1740 / loss: 3.092984 / acc: 0.03333\n",
      "step: 1750 / loss: 3.115219 / acc: 0.03333\n",
      "step: 1760 / loss: 3.117884 / acc: 0.06667\n",
      "step: 1770 / loss: 3.112229 / acc: 0.06667\n",
      "step: 1780 / loss: 3.152578 / acc: 0.03333\n",
      "step: 1790 / loss: 3.086511 / acc: 0.03333\n",
      "step: 1800 / loss: 3.146292 / acc: 0.06667\n",
      "step: 1810 / loss: 3.121365 / acc: 0.00000\n",
      "step: 1820 / loss: 3.070099 / acc: 0.13333\n",
      "step: 1830 / loss: 3.073262 / acc: 0.06667\n",
      "step: 1840 / loss: 3.111601 / acc: 0.10000\n",
      "step: 1850 / loss: 3.129868 / acc: 0.06667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1860 / loss: 3.099900 / acc: 0.00000\n",
      "step: 1870 / loss: 3.087710 / acc: 0.00000\n",
      "step: 1880 / loss: 3.106209 / acc: 0.03333\n",
      "step: 1890 / loss: 3.066225 / acc: 0.03333\n",
      "step: 1900 / loss: 3.113682 / acc: 0.00000\n",
      "step: 1910 / loss: 3.081240 / acc: 0.03333\n",
      "step: 1920 / loss: 3.152207 / acc: 0.00000\n",
      "step: 1930 / loss: 3.097428 / acc: 0.03333\n",
      "step: 1940 / loss: 3.131402 / acc: 0.03333\n",
      "step: 1950 / loss: 3.088255 / acc: 0.00000\n",
      "step: 1960 / loss: 3.067945 / acc: 0.06667\n",
      "step: 1970 / loss: 3.096052 / acc: 0.03333\n",
      "step: 1980 / loss: 3.060540 / acc: 0.10000\n",
      "step: 1990 / loss: 3.045642 / acc: 0.06667\n",
      "step: 2000 / loss: 3.096565 / acc: 0.03333\n",
      "step: 2010 / loss: 3.084502 / acc: 0.03333\n",
      "step: 2020 / loss: 3.117146 / acc: 0.10000\n",
      "step: 2030 / loss: 3.107150 / acc: 0.03333\n",
      "step: 2040 / loss: 3.057824 / acc: 0.03333\n",
      "step: 2050 / loss: 3.109582 / acc: 0.03333\n",
      "step: 2060 / loss: 3.097394 / acc: 0.03333\n",
      "step: 2070 / loss: 3.105355 / acc: 0.03333\n",
      "step: 2080 / loss: 3.086555 / acc: 0.00000\n",
      "step: 2090 / loss: 3.106357 / acc: 0.00000\n",
      "step: 2100 / loss: 3.129280 / acc: 0.06667\n",
      "step: 2110 / loss: 3.161685 / acc: 0.00000\n",
      "step: 2120 / loss: 3.060946 / acc: 0.16667\n",
      "step: 2130 / loss: 3.096450 / acc: 0.03333\n",
      "step: 2140 / loss: 3.110755 / acc: 0.03333\n",
      "step: 2150 / loss: 3.093474 / acc: 0.03333\n",
      "step: 2160 / loss: 3.130086 / acc: 0.00000\n",
      "step: 2170 / loss: 3.082549 / acc: 0.10000\n",
      "step: 2180 / loss: 3.075373 / acc: 0.06667\n",
      "step: 2190 / loss: 3.137468 / acc: 0.06667\n",
      "step: 2200 / loss: 3.116641 / acc: 0.00000\n",
      "step: 2210 / loss: 3.105419 / acc: 0.00000\n",
      "step: 2220 / loss: 3.109078 / acc: 0.03333\n",
      "step: 2230 / loss: 3.087905 / acc: 0.03333\n",
      "step: 2240 / loss: 3.117779 / acc: 0.00000\n",
      "step: 2250 / loss: 3.101351 / acc: 0.06667\n",
      "step: 2260 / loss: 3.121187 / acc: 0.06667\n",
      "step: 2270 / loss: 3.095481 / acc: 0.03333\n",
      "step: 2280 / loss: 3.079326 / acc: 0.06667\n",
      "step: 2290 / loss: 3.110571 / acc: 0.06667\n",
      "step: 2300 / loss: 3.051805 / acc: 0.06667\n",
      "step: 2310 / loss: 3.082236 / acc: 0.10000\n",
      "step: 2320 / loss: 3.081748 / acc: 0.10000\n",
      "step: 2330 / loss: 3.106455 / acc: 0.06667\n",
      "step: 2340 / loss: 3.101834 / acc: 0.06667\n",
      "step: 2350 / loss: 3.093597 / acc: 0.10000\n",
      "step: 2360 / loss: 3.055232 / acc: 0.10000\n",
      "step: 2370 / loss: 3.109934 / acc: 0.00000\n",
      "step: 2380 / loss: 3.115761 / acc: 0.00000\n",
      "step: 2390 / loss: 3.099080 / acc: 0.03333\n",
      "step: 2400 / loss: 3.165351 / acc: 0.00000\n",
      "step: 2410 / loss: 3.099409 / acc: 0.03333\n",
      "step: 2420 / loss: 3.081306 / acc: 0.03333\n",
      "step: 2430 / loss: 3.094133 / acc: 0.06667\n",
      "step: 2440 / loss: 3.128120 / acc: 0.00000\n",
      "step: 2450 / loss: 3.083076 / acc: 0.03333\n",
      "step: 2460 / loss: 3.184076 / acc: 0.06667\n",
      "step: 2470 / loss: 3.108687 / acc: 0.03333\n",
      "step: 2480 / loss: 3.084315 / acc: 0.00000\n",
      "step: 2490 / loss: 3.073619 / acc: 0.06667\n",
      "step: 2500 / loss: 3.110324 / acc: 0.00000\n",
      "step: 2510 / loss: 3.095661 / acc: 0.03333\n",
      "step: 2520 / loss: 3.095993 / acc: 0.06667\n",
      "step: 2530 / loss: 3.100624 / acc: 0.03333\n",
      "step: 2540 / loss: 3.133895 / acc: 0.00000\n",
      "step: 2550 / loss: 3.079937 / acc: 0.03333\n",
      "step: 2560 / loss: 3.108780 / acc: 0.00000\n",
      "step: 2570 / loss: 3.077925 / acc: 0.03333\n",
      "step: 2580 / loss: 3.118150 / acc: 0.00000\n",
      "step: 2590 / loss: 3.053125 / acc: 0.13333\n",
      "step: 2600 / loss: 3.156285 / acc: 0.03333\n",
      "step: 2610 / loss: 3.098361 / acc: 0.00000\n",
      "step: 2620 / loss: 3.106019 / acc: 0.06667\n",
      "step: 2630 / loss: 3.107945 / acc: 0.06667\n",
      "step: 2640 / loss: 3.120441 / acc: 0.00000\n",
      "step: 2650 / loss: 3.092237 / acc: 0.06667\n",
      "step: 2660 / loss: 3.138596 / acc: 0.00000\n",
      "step: 2670 / loss: 3.101235 / acc: 0.06667\n",
      "step: 2680 / loss: 3.088516 / acc: 0.06667\n",
      "step: 2690 / loss: 3.087821 / acc: 0.10000\n",
      "step: 2700 / loss: 3.051805 / acc: 0.03333\n",
      "step: 2710 / loss: 3.107708 / acc: 0.03333\n",
      "step: 2720 / loss: 3.091575 / acc: 0.03333\n",
      "step: 2730 / loss: 3.113461 / acc: 0.00000\n",
      "step: 2740 / loss: 3.122090 / acc: 0.06667\n",
      "step: 2750 / loss: 3.079023 / acc: 0.06667\n",
      "step: 2760 / loss: 3.060958 / acc: 0.06667\n",
      "step: 2770 / loss: 3.087221 / acc: 0.03333\n",
      "step: 2780 / loss: 3.084871 / acc: 0.03333\n",
      "step: 2790 / loss: 3.162312 / acc: 0.03333\n",
      "step: 2800 / loss: 3.141952 / acc: 0.00000\n",
      "step: 2810 / loss: 3.058170 / acc: 0.10000\n",
      "step: 2820 / loss: 3.128701 / acc: 0.06667\n",
      "step: 2830 / loss: 3.091142 / acc: 0.06667\n",
      "step: 2840 / loss: 3.056988 / acc: 0.06667\n",
      "step: 2850 / loss: 3.153140 / acc: 0.03333\n",
      "step: 2860 / loss: 3.058982 / acc: 0.16667\n",
      "step: 2870 / loss: 3.077818 / acc: 0.03333\n",
      "step: 2880 / loss: 3.054219 / acc: 0.00000\n",
      "step: 2890 / loss: 3.154010 / acc: 0.03333\n",
      "step: 2900 / loss: 3.120557 / acc: 0.06667\n",
      "step: 2910 / loss: 3.099314 / acc: 0.06667\n",
      "step: 2920 / loss: 3.101075 / acc: 0.03333\n",
      "step: 2930 / loss: 3.099093 / acc: 0.00000\n",
      "step: 2940 / loss: 3.092162 / acc: 0.10000\n",
      "step: 2950 / loss: 3.091052 / acc: 0.03333\n",
      "step: 2960 / loss: 3.096635 / acc: 0.06667\n",
      "step: 2970 / loss: 3.066906 / acc: 0.03333\n",
      "step: 2980 / loss: 3.062589 / acc: 0.06667\n",
      "step: 2990 / loss: 3.065116 / acc: 0.00000\n",
      "step: 3000 / loss: 3.162519 / acc: 0.00000\n",
      "step: 3010 / loss: 3.100693 / acc: 0.06667\n",
      "step: 3020 / loss: 3.063910 / acc: 0.13333\n",
      "step: 3030 / loss: 3.100312 / acc: 0.13333\n",
      "step: 3040 / loss: 3.115876 / acc: 0.00000\n",
      "step: 3050 / loss: 3.091088 / acc: 0.06667\n",
      "step: 3060 / loss: 3.100120 / acc: 0.03333\n",
      "step: 3070 / loss: 3.070806 / acc: 0.03333\n",
      "step: 3080 / loss: 3.122854 / acc: 0.00000\n",
      "step: 3090 / loss: 3.102917 / acc: 0.00000\n",
      "step: 3100 / loss: 3.082995 / acc: 0.06667\n",
      "step: 3110 / loss: 3.117123 / acc: 0.00000\n",
      "step: 3120 / loss: 3.055927 / acc: 0.06667\n",
      "step: 3130 / loss: 3.103350 / acc: 0.03333\n",
      "step: 3140 / loss: 3.102579 / acc: 0.03333\n",
      "step: 3150 / loss: 3.122803 / acc: 0.03333\n",
      "step: 3160 / loss: 3.096466 / acc: 0.10000\n",
      "step: 3170 / loss: 3.100036 / acc: 0.06667\n",
      "step: 3180 / loss: 3.077802 / acc: 0.03333\n",
      "step: 3190 / loss: 3.120531 / acc: 0.00000\n",
      "step: 3200 / loss: 3.119832 / acc: 0.03333\n",
      "step: 3210 / loss: 3.111181 / acc: 0.10000\n",
      "step: 3220 / loss: 3.132497 / acc: 0.00000\n",
      "step: 3230 / loss: 3.119439 / acc: 0.06667\n",
      "step: 3240 / loss: 3.141362 / acc: 0.00000\n",
      "step: 3250 / loss: 3.088307 / acc: 0.06667\n",
      "step: 3260 / loss: 3.106977 / acc: 0.00000\n",
      "step: 3270 / loss: 3.090990 / acc: 0.03333\n",
      "step: 3280 / loss: 3.071175 / acc: 0.03333\n",
      "step: 3290 / loss: 3.077831 / acc: 0.06667\n",
      "step: 3300 / loss: 3.092506 / acc: 0.03333\n",
      "step: 3310 / loss: 3.115278 / acc: 0.03333\n",
      "step: 3320 / loss: 3.129471 / acc: 0.00000\n",
      "step: 3330 / loss: 3.132520 / acc: 0.00000\n",
      "step: 3340 / loss: 3.124351 / acc: 0.03333\n",
      "step: 3350 / loss: 3.102496 / acc: 0.06667\n",
      "step: 3360 / loss: 3.081060 / acc: 0.03333\n",
      "step: 3370 / loss: 3.087854 / acc: 0.00000\n",
      "step: 3380 / loss: 3.111894 / acc: 0.03333\n",
      "step: 3390 / loss: 3.082141 / acc: 0.03333\n",
      "step: 3400 / loss: 3.098203 / acc: 0.10000\n",
      "step: 3410 / loss: 3.098882 / acc: 0.03333\n",
      "step: 3420 / loss: 3.071725 / acc: 0.16667\n",
      "step: 3430 / loss: 3.102228 / acc: 0.10000\n",
      "step: 3440 / loss: 3.092663 / acc: 0.03333\n",
      "step: 3450 / loss: 3.137031 / acc: 0.10000\n",
      "step: 3460 / loss: 3.100090 / acc: 0.10000\n",
      "step: 3470 / loss: 3.115547 / acc: 0.03333\n",
      "step: 3480 / loss: 3.104318 / acc: 0.00000\n",
      "step: 3490 / loss: 3.118289 / acc: 0.03333\n",
      "step: 3500 / loss: 3.105808 / acc: 0.10000\n",
      "step: 3510 / loss: 3.112452 / acc: 0.03333\n",
      "step: 3520 / loss: 3.092141 / acc: 0.06667\n",
      "step: 3530 / loss: 3.108094 / acc: 0.03333\n",
      "step: 3540 / loss: 3.092374 / acc: 0.00000\n",
      "step: 3550 / loss: 3.105556 / acc: 0.00000\n",
      "step: 3560 / loss: 3.076601 / acc: 0.03333\n",
      "step: 3570 / loss: 3.089838 / acc: 0.00000\n",
      "step: 3580 / loss: 3.116129 / acc: 0.03333\n",
      "step: 3590 / loss: 3.112733 / acc: 0.00000\n",
      "step: 3600 / loss: 3.131117 / acc: 0.03333\n",
      "step: 3610 / loss: 3.103509 / acc: 0.06667\n",
      "step: 3620 / loss: 3.152789 / acc: 0.03333\n",
      "step: 3630 / loss: 3.106449 / acc: 0.03333\n",
      "step: 3640 / loss: 3.090667 / acc: 0.06667\n",
      "step: 3650 / loss: 3.106439 / acc: 0.00000\n",
      "step: 3660 / loss: 3.079659 / acc: 0.10000\n",
      "step: 3670 / loss: 3.095384 / acc: 0.03333\n",
      "step: 3680 / loss: 3.086494 / acc: 0.03333\n",
      "step: 3690 / loss: 3.056870 / acc: 0.06667\n",
      "step: 3700 / loss: 3.037357 / acc: 0.10000\n",
      "step: 3710 / loss: 3.094812 / acc: 0.06667\n",
      "step: 3720 / loss: 3.090144 / acc: 0.00000\n",
      "step: 3730 / loss: 3.095220 / acc: 0.06667\n",
      "step: 3740 / loss: 3.133183 / acc: 0.00000\n",
      "step: 3750 / loss: 3.106957 / acc: 0.06667\n",
      "step: 3760 / loss: 3.092862 / acc: 0.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3770 / loss: 3.109689 / acc: 0.00000\n",
      "step: 3780 / loss: 3.096031 / acc: 0.06667\n",
      "step: 3790 / loss: 3.112941 / acc: 0.06667\n",
      "step: 3800 / loss: 3.102005 / acc: 0.03333\n",
      "step: 3810 / loss: 3.085505 / acc: 0.00000\n",
      "step: 3820 / loss: 3.082733 / acc: 0.06667\n",
      "step: 3830 / loss: 3.092904 / acc: 0.03333\n",
      "step: 3840 / loss: 3.078431 / acc: 0.10000\n",
      "step: 3850 / loss: 3.099363 / acc: 0.03333\n",
      "step: 3860 / loss: 3.079284 / acc: 0.06667\n",
      "step: 3870 / loss: 3.119755 / acc: 0.03333\n",
      "step: 3880 / loss: 3.105454 / acc: 0.10000\n",
      "step: 3890 / loss: 3.106450 / acc: 0.06667\n",
      "step: 3900 / loss: 3.072355 / acc: 0.03333\n",
      "step: 3910 / loss: 3.096549 / acc: 0.03333\n",
      "step: 3920 / loss: 3.113513 / acc: 0.10000\n",
      "step: 3930 / loss: 3.087064 / acc: 0.03333\n",
      "step: 3940 / loss: 3.091971 / acc: 0.03333\n",
      "step: 3950 / loss: 3.135488 / acc: 0.00000\n",
      "step: 3960 / loss: 3.107649 / acc: 0.03333\n",
      "step: 3970 / loss: 3.118599 / acc: 0.06667\n",
      "step: 3980 / loss: 3.070377 / acc: 0.06667\n",
      "step: 3990 / loss: 3.095540 / acc: 0.06667\n",
      "step: 4000 / loss: 3.131381 / acc: 0.03333\n",
      "step: 4010 / loss: 3.119290 / acc: 0.03333\n",
      "step: 4020 / loss: 3.072784 / acc: 0.03333\n",
      "step: 4030 / loss: 3.097143 / acc: 0.00000\n",
      "step: 4040 / loss: 3.135022 / acc: 0.00000\n",
      "step: 4050 / loss: 3.124612 / acc: 0.03333\n",
      "step: 4060 / loss: 3.097530 / acc: 0.00000\n",
      "step: 4070 / loss: 3.101373 / acc: 0.03333\n",
      "step: 4080 / loss: 3.097478 / acc: 0.03333\n",
      "step: 4090 / loss: 3.089915 / acc: 0.03333\n",
      "step: 4100 / loss: 3.072565 / acc: 0.00000\n",
      "step: 4110 / loss: 3.098479 / acc: 0.06667\n",
      "step: 4120 / loss: 3.121082 / acc: 0.00000\n",
      "step: 4130 / loss: 3.068779 / acc: 0.10000\n",
      "step: 4140 / loss: 3.141613 / acc: 0.06667\n",
      "step: 4150 / loss: 3.099504 / acc: 0.06667\n",
      "step: 4160 / loss: 3.090244 / acc: 0.03333\n",
      "step: 4170 / loss: 3.127624 / acc: 0.03333\n",
      "step: 4180 / loss: 3.140424 / acc: 0.06667\n",
      "step: 4190 / loss: 3.126428 / acc: 0.03333\n",
      "step: 4200 / loss: 3.075179 / acc: 0.03333\n",
      "step: 4210 / loss: 3.105381 / acc: 0.03333\n",
      "step: 4220 / loss: 3.091266 / acc: 0.03333\n",
      "step: 4230 / loss: 3.115637 / acc: 0.03333\n",
      "step: 4240 / loss: 3.123707 / acc: 0.00000\n",
      "step: 4250 / loss: 3.137057 / acc: 0.00000\n",
      "step: 4260 / loss: 3.096218 / acc: 0.06667\n",
      "step: 4270 / loss: 3.117206 / acc: 0.03333\n",
      "step: 4280 / loss: 3.091725 / acc: 0.10000\n",
      "step: 4290 / loss: 3.122665 / acc: 0.06667\n",
      "step: 4300 / loss: 3.058418 / acc: 0.03333\n",
      "step: 4310 / loss: 3.084345 / acc: 0.06667\n",
      "step: 4320 / loss: 3.065094 / acc: 0.10000\n",
      "step: 4330 / loss: 3.078809 / acc: 0.06667\n",
      "step: 4340 / loss: 3.077411 / acc: 0.00000\n",
      "step: 4350 / loss: 3.081066 / acc: 0.06667\n",
      "step: 4360 / loss: 3.050164 / acc: 0.10000\n",
      "step: 4370 / loss: 3.087457 / acc: 0.13333\n",
      "step: 4380 / loss: 3.070463 / acc: 0.06667\n",
      "step: 4390 / loss: 3.117467 / acc: 0.06667\n",
      "step: 4400 / loss: 3.111836 / acc: 0.03333\n",
      "step: 4410 / loss: 3.123196 / acc: 0.00000\n",
      "step: 4420 / loss: 3.103005 / acc: 0.06667\n",
      "step: 4430 / loss: 3.120339 / acc: 0.00000\n",
      "step: 4440 / loss: 3.076770 / acc: 0.06667\n",
      "step: 4450 / loss: 3.094038 / acc: 0.06667\n",
      "step: 4460 / loss: 3.162533 / acc: 0.03333\n",
      "step: 4470 / loss: 3.097508 / acc: 0.06667\n",
      "step: 4480 / loss: 3.093630 / acc: 0.06667\n",
      "step: 4490 / loss: 3.108121 / acc: 0.00000\n",
      "step: 4500 / loss: 3.110121 / acc: 0.03333\n",
      "step: 4510 / loss: 3.090924 / acc: 0.03333\n",
      "step: 4520 / loss: 3.115240 / acc: 0.00000\n",
      "step: 4530 / loss: 3.089888 / acc: 0.00000\n",
      "step: 4540 / loss: 3.098487 / acc: 0.06667\n",
      "step: 4550 / loss: 3.085669 / acc: 0.00000\n",
      "step: 4560 / loss: 3.100122 / acc: 0.10000\n",
      "step: 4570 / loss: 3.061803 / acc: 0.06667\n",
      "step: 4580 / loss: 3.116840 / acc: 0.03333\n",
      "step: 4590 / loss: 3.092095 / acc: 0.13333\n",
      "step: 4600 / loss: 3.102016 / acc: 0.03333\n",
      "step: 4610 / loss: 3.105579 / acc: 0.00000\n",
      "step: 4620 / loss: 3.068187 / acc: 0.06667\n",
      "step: 4630 / loss: 3.090153 / acc: 0.03333\n",
      "step: 4640 / loss: 3.115963 / acc: 0.03333\n",
      "step: 4650 / loss: 3.099355 / acc: 0.00000\n",
      "step: 4660 / loss: 3.106029 / acc: 0.06667\n",
      "step: 4670 / loss: 3.091026 / acc: 0.03333\n",
      "step: 4680 / loss: 3.137416 / acc: 0.06667\n",
      "step: 4690 / loss: 3.116664 / acc: 0.00000\n",
      "step: 4700 / loss: 3.095944 / acc: 0.00000\n",
      "step: 4710 / loss: 3.060153 / acc: 0.06667\n",
      "step: 4720 / loss: 3.047070 / acc: 0.06667\n",
      "step: 4730 / loss: 3.117872 / acc: 0.06667\n",
      "step: 4740 / loss: 3.102238 / acc: 0.00000\n",
      "step: 4750 / loss: 3.099483 / acc: 0.00000\n",
      "step: 4760 / loss: 3.083599 / acc: 0.00000\n",
      "step: 4770 / loss: 3.064081 / acc: 0.13333\n",
      "step: 4780 / loss: 3.108216 / acc: 0.03333\n",
      "step: 4790 / loss: 3.105332 / acc: 0.06667\n",
      "step: 4800 / loss: 3.078325 / acc: 0.10000\n",
      "step: 4810 / loss: 3.087279 / acc: 0.06667\n",
      "step: 4820 / loss: 3.096048 / acc: 0.00000\n",
      "step: 4830 / loss: 3.093415 / acc: 0.06667\n",
      "step: 4840 / loss: 3.077121 / acc: 0.06667\n",
      "step: 4850 / loss: 3.081429 / acc: 0.10000\n",
      "step: 4860 / loss: 3.074488 / acc: 0.06667\n",
      "step: 4870 / loss: 3.125242 / acc: 0.03333\n",
      "step: 4880 / loss: 3.128116 / acc: 0.06667\n",
      "step: 4890 / loss: 3.105659 / acc: 0.03333\n",
      "step: 4900 / loss: 3.074136 / acc: 0.10000\n",
      "step: 4910 / loss: 3.098490 / acc: 0.03333\n",
      "step: 4920 / loss: 3.132226 / acc: 0.06667\n",
      "step: 4930 / loss: 3.086820 / acc: 0.03333\n",
      "step: 4940 / loss: 3.090660 / acc: 0.10000\n",
      "step: 4950 / loss: 3.087074 / acc: 0.06667\n",
      "step: 4960 / loss: 3.081218 / acc: 0.00000\n",
      "step: 4970 / loss: 3.086534 / acc: 0.10000\n",
      "step: 4980 / loss: 3.099642 / acc: 0.10000\n",
      "step: 4990 / loss: 3.090040 / acc: 0.06667\n",
      "step: 5000 / loss: 3.076043 / acc: 0.00000\n",
      "step: 5010 / loss: 3.086057 / acc: 0.03333\n",
      "step: 5020 / loss: 3.083359 / acc: 0.10000\n",
      "step: 5030 / loss: 3.092708 / acc: 0.00000\n",
      "step: 5040 / loss: 3.106335 / acc: 0.03333\n",
      "step: 5050 / loss: 3.075886 / acc: 0.06667\n",
      "step: 5060 / loss: 3.082128 / acc: 0.06667\n",
      "step: 5070 / loss: 3.075347 / acc: 0.10000\n",
      "step: 5080 / loss: 3.091116 / acc: 0.03333\n",
      "step: 5090 / loss: 3.076365 / acc: 0.06667\n",
      "step: 5100 / loss: 3.086725 / acc: 0.13333\n",
      "step: 5110 / loss: 3.083663 / acc: 0.10000\n",
      "step: 5120 / loss: 3.098967 / acc: 0.00000\n",
      "step: 5130 / loss: 3.095300 / acc: 0.00000\n",
      "step: 5140 / loss: 3.113477 / acc: 0.03333\n",
      "step: 5150 / loss: 3.074859 / acc: 0.03333\n",
      "step: 5160 / loss: 3.085643 / acc: 0.06667\n",
      "step: 5170 / loss: 3.081063 / acc: 0.10000\n",
      "step: 5180 / loss: 3.102660 / acc: 0.03333\n",
      "step: 5190 / loss: 3.059536 / acc: 0.13333\n",
      "step: 5200 / loss: 3.091581 / acc: 0.00000\n",
      "step: 5210 / loss: 3.110435 / acc: 0.00000\n",
      "step: 5220 / loss: 3.098280 / acc: 0.00000\n",
      "step: 5230 / loss: 3.122778 / acc: 0.00000\n",
      "step: 5240 / loss: 3.121341 / acc: 0.00000\n",
      "step: 5250 / loss: 3.102329 / acc: 0.03333\n",
      "step: 5260 / loss: 3.084358 / acc: 0.00000\n",
      "step: 5270 / loss: 3.121332 / acc: 0.06667\n",
      "step: 5280 / loss: 3.121827 / acc: 0.06667\n",
      "step: 5290 / loss: 3.066057 / acc: 0.06667\n",
      "step: 5300 / loss: 3.094245 / acc: 0.00000\n",
      "step: 5310 / loss: 3.081323 / acc: 0.06667\n",
      "step: 5320 / loss: 3.094337 / acc: 0.03333\n",
      "step: 5330 / loss: 3.101936 / acc: 0.10000\n",
      "step: 5340 / loss: 3.101354 / acc: 0.03333\n",
      "step: 5350 / loss: 3.098889 / acc: 0.03333\n",
      "step: 5360 / loss: 3.117324 / acc: 0.00000\n",
      "step: 5370 / loss: 3.081572 / acc: 0.06667\n",
      "step: 5380 / loss: 3.093159 / acc: 0.03333\n",
      "step: 5390 / loss: 3.131156 / acc: 0.03333\n",
      "step: 5400 / loss: 3.100825 / acc: 0.06667\n",
      "step: 5410 / loss: 3.092800 / acc: 0.00000\n",
      "step: 5420 / loss: 3.063510 / acc: 0.10000\n",
      "step: 5430 / loss: 3.107877 / acc: 0.00000\n",
      "step: 5440 / loss: 3.080506 / acc: 0.03333\n",
      "step: 5450 / loss: 3.123434 / acc: 0.03333\n",
      "step: 5460 / loss: 3.073713 / acc: 0.06667\n",
      "step: 5470 / loss: 3.079545 / acc: 0.03333\n",
      "step: 5480 / loss: 3.076420 / acc: 0.06667\n",
      "step: 5490 / loss: 3.123116 / acc: 0.00000\n",
      "step: 5500 / loss: 3.101839 / acc: 0.00000\n",
      "step: 5510 / loss: 3.113828 / acc: 0.00000\n",
      "step: 5520 / loss: 3.058206 / acc: 0.13333\n",
      "step: 5530 / loss: 3.060060 / acc: 0.10000\n",
      "step: 5540 / loss: 3.111109 / acc: 0.03333\n",
      "step: 5550 / loss: 3.081465 / acc: 0.10000\n",
      "step: 5560 / loss: 3.107338 / acc: 0.06667\n",
      "step: 5570 / loss: 3.078131 / acc: 0.03333\n",
      "step: 5580 / loss: 3.097132 / acc: 0.03333\n",
      "step: 5590 / loss: 3.095866 / acc: 0.03333\n",
      "step: 5600 / loss: 3.089382 / acc: 0.03333\n",
      "step: 5610 / loss: 3.090199 / acc: 0.00000\n",
      "step: 5620 / loss: 3.078560 / acc: 0.06667\n",
      "step: 5630 / loss: 3.107619 / acc: 0.00000\n",
      "step: 5640 / loss: 3.091588 / acc: 0.00000\n",
      "step: 5650 / loss: 3.098163 / acc: 0.03333\n",
      "step: 5660 / loss: 3.100034 / acc: 0.03333\n",
      "step: 5670 / loss: 3.087746 / acc: 0.06667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5680 / loss: 3.096476 / acc: 0.00000\n",
      "step: 5690 / loss: 3.128875 / acc: 0.00000\n",
      "step: 5700 / loss: 3.074969 / acc: 0.06667\n",
      "step: 5710 / loss: 3.121425 / acc: 0.03333\n",
      "step: 5720 / loss: 3.081519 / acc: 0.06667\n",
      "step: 5730 / loss: 3.093925 / acc: 0.06667\n",
      "step: 5740 / loss: 3.108444 / acc: 0.06667\n",
      "step: 5750 / loss: 3.084746 / acc: 0.03333\n",
      "step: 5760 / loss: 3.117469 / acc: 0.00000\n",
      "step: 5770 / loss: 3.095059 / acc: 0.10000\n",
      "step: 5780 / loss: 3.111808 / acc: 0.03333\n",
      "step: 5790 / loss: 3.070290 / acc: 0.03333\n",
      "step: 5800 / loss: 3.114095 / acc: 0.03333\n",
      "step: 5810 / loss: 3.113350 / acc: 0.03333\n",
      "step: 5820 / loss: 3.107429 / acc: 0.00000\n",
      "step: 5830 / loss: 3.052614 / acc: 0.10000\n",
      "step: 5840 / loss: 3.103553 / acc: 0.03333\n",
      "step: 5850 / loss: 3.110769 / acc: 0.00000\n",
      "step: 5860 / loss: 3.100787 / acc: 0.03333\n",
      "step: 5870 / loss: 3.086168 / acc: 0.00000\n",
      "step: 5880 / loss: 3.112515 / acc: 0.03333\n",
      "step: 5890 / loss: 3.113173 / acc: 0.00000\n",
      "step: 5900 / loss: 3.066643 / acc: 0.03333\n",
      "step: 5910 / loss: 3.092166 / acc: 0.00000\n",
      "step: 5920 / loss: 3.077984 / acc: 0.10000\n",
      "step: 5930 / loss: 3.090529 / acc: 0.06667\n",
      "step: 5940 / loss: 3.095025 / acc: 0.03333\n",
      "step: 5950 / loss: 3.096339 / acc: 0.03333\n",
      "step: 5960 / loss: 3.104541 / acc: 0.03333\n",
      "step: 5970 / loss: 3.094437 / acc: 0.06667\n",
      "step: 5980 / loss: 3.129415 / acc: 0.03333\n",
      "step: 5990 / loss: 3.099540 / acc: 0.10000\n",
      "step: 6000 / loss: 3.112694 / acc: 0.03333\n",
      "step: 6010 / loss: 3.068128 / acc: 0.03333\n",
      "step: 6020 / loss: 3.106819 / acc: 0.00000\n",
      "step: 6030 / loss: 3.090564 / acc: 0.06667\n",
      "step: 6040 / loss: 3.063467 / acc: 0.13333\n",
      "step: 6050 / loss: 3.067496 / acc: 0.06667\n",
      "step: 6060 / loss: 3.105027 / acc: 0.03333\n",
      "step: 6070 / loss: 3.096740 / acc: 0.03333\n",
      "step: 6080 / loss: 3.094341 / acc: 0.00000\n",
      "step: 6090 / loss: 3.097217 / acc: 0.10000\n",
      "step: 6100 / loss: 3.095037 / acc: 0.03333\n",
      "step: 6110 / loss: 3.097390 / acc: 0.03333\n",
      "step: 6120 / loss: 3.097946 / acc: 0.03333\n",
      "step: 6130 / loss: 3.116203 / acc: 0.06667\n",
      "step: 6140 / loss: 3.120371 / acc: 0.03333\n",
      "step: 6150 / loss: 3.088875 / acc: 0.03333\n",
      "step: 6160 / loss: 3.098922 / acc: 0.06667\n",
      "step: 6170 / loss: 3.083511 / acc: 0.06667\n",
      "step: 6180 / loss: 3.111308 / acc: 0.00000\n",
      "step: 6190 / loss: 3.084864 / acc: 0.06667\n",
      "step: 6200 / loss: 3.086213 / acc: 0.00000\n",
      "step: 6210 / loss: 3.084470 / acc: 0.10000\n",
      "step: 6220 / loss: 3.095246 / acc: 0.06667\n",
      "step: 6230 / loss: 3.082864 / acc: 0.06667\n",
      "step: 6240 / loss: 3.082431 / acc: 0.06667\n",
      "step: 6250 / loss: 3.072147 / acc: 0.16667\n",
      "step: 6260 / loss: 3.105212 / acc: 0.06667\n",
      "step: 6270 / loss: 3.103814 / acc: 0.06667\n",
      "step: 6280 / loss: 3.095267 / acc: 0.03333\n",
      "step: 6290 / loss: 3.091899 / acc: 0.10000\n",
      "step: 6300 / loss: 3.098816 / acc: 0.03333\n",
      "step: 6310 / loss: 3.078074 / acc: 0.06667\n",
      "step: 6320 / loss: 3.078863 / acc: 0.10000\n",
      "step: 6330 / loss: 3.074151 / acc: 0.03333\n",
      "step: 6340 / loss: 3.111617 / acc: 0.00000\n",
      "step: 6350 / loss: 3.097482 / acc: 0.03333\n",
      "step: 6360 / loss: 3.095073 / acc: 0.10000\n",
      "step: 6370 / loss: 3.131514 / acc: 0.03333\n",
      "step: 6380 / loss: 3.096990 / acc: 0.06667\n",
      "step: 6390 / loss: 3.090346 / acc: 0.03333\n",
      "step: 6400 / loss: 3.092445 / acc: 0.03333\n",
      "step: 6410 / loss: 3.108230 / acc: 0.03333\n",
      "step: 6420 / loss: 3.099052 / acc: 0.06667\n",
      "step: 6430 / loss: 3.076016 / acc: 0.06667\n",
      "step: 6440 / loss: 3.093936 / acc: 0.03333\n",
      "step: 6450 / loss: 3.112015 / acc: 0.06667\n",
      "step: 6460 / loss: 3.101404 / acc: 0.03333\n",
      "step: 6470 / loss: 3.121973 / acc: 0.00000\n",
      "step: 6480 / loss: 3.134712 / acc: 0.00000\n",
      "step: 6490 / loss: 3.092876 / acc: 0.06667\n",
      "step: 6500 / loss: 3.103446 / acc: 0.00000\n",
      "step: 6510 / loss: 3.078145 / acc: 0.06667\n",
      "step: 6520 / loss: 3.104956 / acc: 0.03333\n",
      "step: 6530 / loss: 3.095414 / acc: 0.06667\n",
      "step: 6540 / loss: 3.073664 / acc: 0.10000\n",
      "step: 6550 / loss: 3.105396 / acc: 0.10000\n",
      "step: 6560 / loss: 3.122926 / acc: 0.06667\n",
      "step: 6570 / loss: 3.092315 / acc: 0.10000\n",
      "step: 6580 / loss: 3.105689 / acc: 0.10000\n",
      "step: 6590 / loss: 3.106129 / acc: 0.06667\n",
      "step: 6600 / loss: 3.109915 / acc: 0.03333\n",
      "step: 6610 / loss: 3.081692 / acc: 0.03333\n",
      "step: 6620 / loss: 3.070840 / acc: 0.00000\n",
      "step: 6630 / loss: 3.126344 / acc: 0.00000\n",
      "step: 6640 / loss: 3.110152 / acc: 0.03333\n",
      "step: 6650 / loss: 3.103383 / acc: 0.06667\n",
      "step: 6660 / loss: 3.096279 / acc: 0.00000\n",
      "step: 6670 / loss: 3.102270 / acc: 0.00000\n",
      "step: 6680 / loss: 3.102511 / acc: 0.00000\n",
      "step: 6690 / loss: 3.096915 / acc: 0.00000\n",
      "step: 6700 / loss: 3.097156 / acc: 0.06667\n",
      "step: 6710 / loss: 3.097332 / acc: 0.00000\n",
      "step: 6720 / loss: 3.105604 / acc: 0.06667\n",
      "step: 6730 / loss: 3.083187 / acc: 0.00000\n",
      "step: 6740 / loss: 3.098114 / acc: 0.13333\n",
      "step: 6750 / loss: 3.111636 / acc: 0.00000\n",
      "step: 6760 / loss: 3.100607 / acc: 0.03333\n",
      "step: 6770 / loss: 3.088511 / acc: 0.13333\n",
      "step: 6780 / loss: 3.114691 / acc: 0.06667\n",
      "step: 6790 / loss: 3.119041 / acc: 0.00000\n",
      "step: 6800 / loss: 3.117666 / acc: 0.03333\n",
      "step: 6810 / loss: 3.083434 / acc: 0.13333\n",
      "step: 6820 / loss: 3.024401 / acc: 0.16667\n",
      "step: 6830 / loss: 3.112247 / acc: 0.00000\n",
      "step: 6840 / loss: 3.105494 / acc: 0.00000\n",
      "step: 6850 / loss: 3.082101 / acc: 0.06667\n",
      "step: 6860 / loss: 3.102440 / acc: 0.03333\n",
      "step: 6870 / loss: 3.070783 / acc: 0.03333\n",
      "step: 6880 / loss: 3.077743 / acc: 0.06667\n",
      "step: 6890 / loss: 3.109223 / acc: 0.06667\n",
      "step: 6900 / loss: 3.110223 / acc: 0.10000\n",
      "step: 6910 / loss: 3.095015 / acc: 0.03333\n",
      "step: 6920 / loss: 3.060527 / acc: 0.13333\n",
      "step: 6930 / loss: 3.113366 / acc: 0.06667\n",
      "step: 6940 / loss: 3.105515 / acc: 0.03333\n",
      "step: 6950 / loss: 3.106644 / acc: 0.00000\n",
      "step: 6960 / loss: 3.095512 / acc: 0.03333\n",
      "step: 6970 / loss: 3.086883 / acc: 0.13333\n",
      "step: 6980 / loss: 3.096631 / acc: 0.10000\n",
      "step: 6990 / loss: 3.104692 / acc: 0.00000\n",
      "step: 7000 / loss: 3.105724 / acc: 0.06667\n",
      "step: 7010 / loss: 3.084482 / acc: 0.00000\n",
      "step: 7020 / loss: 3.089766 / acc: 0.10000\n",
      "step: 7030 / loss: 3.093717 / acc: 0.00000\n",
      "step: 7040 / loss: 3.085992 / acc: 0.06667\n",
      "step: 7050 / loss: 3.089488 / acc: 0.03333\n",
      "step: 7060 / loss: 3.107391 / acc: 0.00000\n",
      "step: 7070 / loss: 3.084373 / acc: 0.06667\n",
      "step: 7080 / loss: 3.109856 / acc: 0.13333\n",
      "step: 7090 / loss: 3.104444 / acc: 0.06667\n",
      "step: 7100 / loss: 3.079771 / acc: 0.10000\n",
      "step: 7110 / loss: 3.100753 / acc: 0.00000\n",
      "step: 7120 / loss: 3.088097 / acc: 0.03333\n",
      "step: 7130 / loss: 3.065585 / acc: 0.16667\n",
      "step: 7140 / loss: 3.066196 / acc: 0.10000\n",
      "step: 7150 / loss: 3.085371 / acc: 0.03333\n",
      "step: 7160 / loss: 3.109988 / acc: 0.00000\n",
      "step: 7170 / loss: 3.089433 / acc: 0.03333\n",
      "step: 7180 / loss: 3.108610 / acc: 0.06667\n",
      "step: 7190 / loss: 3.095120 / acc: 0.06667\n",
      "step: 7200 / loss: 3.090851 / acc: 0.03333\n",
      "step: 7210 / loss: 3.090250 / acc: 0.03333\n",
      "step: 7220 / loss: 3.107022 / acc: 0.10000\n",
      "step: 7230 / loss: 3.068260 / acc: 0.06667\n",
      "step: 7240 / loss: 3.085723 / acc: 0.06667\n",
      "step: 7250 / loss: 3.118638 / acc: 0.03333\n",
      "step: 7260 / loss: 3.072819 / acc: 0.03333\n",
      "step: 7270 / loss: 3.070358 / acc: 0.10000\n",
      "step: 7280 / loss: 3.094196 / acc: 0.00000\n",
      "step: 7290 / loss: 3.124281 / acc: 0.00000\n",
      "step: 7300 / loss: 3.110956 / acc: 0.03333\n",
      "step: 7310 / loss: 3.117508 / acc: 0.03333\n",
      "step: 7320 / loss: 3.101477 / acc: 0.06667\n",
      "step: 7330 / loss: 3.110315 / acc: 0.00000\n",
      "step: 7340 / loss: 3.116713 / acc: 0.03333\n",
      "step: 7350 / loss: 3.074514 / acc: 0.00000\n",
      "step: 7360 / loss: 3.076977 / acc: 0.10000\n",
      "step: 7370 / loss: 3.093502 / acc: 0.10000\n",
      "step: 7380 / loss: 3.087417 / acc: 0.06667\n",
      "step: 7390 / loss: 3.088927 / acc: 0.10000\n",
      "step: 7400 / loss: 3.111367 / acc: 0.03333\n",
      "step: 7410 / loss: 3.088950 / acc: 0.10000\n",
      "step: 7420 / loss: 3.076648 / acc: 0.06667\n",
      "step: 7430 / loss: 3.086495 / acc: 0.06667\n",
      "step: 7440 / loss: 3.080242 / acc: 0.10000\n",
      "step: 7450 / loss: 3.103887 / acc: 0.03333\n",
      "step: 7460 / loss: 3.088926 / acc: 0.03333\n",
      "step: 7470 / loss: 3.092655 / acc: 0.00000\n",
      "step: 7480 / loss: 3.118936 / acc: 0.00000\n",
      "step: 7490 / loss: 3.095598 / acc: 0.00000\n",
      "step: 7500 / loss: 3.113010 / acc: 0.03333\n",
      "step: 7510 / loss: 3.091624 / acc: 0.03333\n",
      "step: 7520 / loss: 3.121529 / acc: 0.03333\n",
      "step: 7530 / loss: 3.100497 / acc: 0.00000\n",
      "step: 7540 / loss: 3.077003 / acc: 0.03333\n",
      "step: 7550 / loss: 3.129414 / acc: 0.00000\n",
      "step: 7560 / loss: 3.098772 / acc: 0.00000\n",
      "step: 7570 / loss: 3.084547 / acc: 0.03333\n",
      "step: 7580 / loss: 3.102433 / acc: 0.03333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7590 / loss: 3.092086 / acc: 0.06667\n",
      "step: 7600 / loss: 3.092795 / acc: 0.10000\n",
      "step: 7610 / loss: 3.093738 / acc: 0.10000\n",
      "step: 7620 / loss: 3.083038 / acc: 0.06667\n",
      "step: 7630 / loss: 3.085395 / acc: 0.10000\n",
      "step: 7640 / loss: 3.077412 / acc: 0.06667\n",
      "step: 7650 / loss: 3.087431 / acc: 0.13333\n",
      "step: 7660 / loss: 3.064246 / acc: 0.03333\n",
      "step: 7670 / loss: 3.088764 / acc: 0.03333\n",
      "step: 7680 / loss: 3.098687 / acc: 0.10000\n",
      "step: 7690 / loss: 3.078195 / acc: 0.06667\n",
      "step: 7700 / loss: 3.105163 / acc: 0.00000\n",
      "step: 7710 / loss: 3.075496 / acc: 0.06667\n",
      "step: 7720 / loss: 3.075969 / acc: 0.10000\n",
      "step: 7730 / loss: 3.106746 / acc: 0.03333\n",
      "step: 7740 / loss: 3.115789 / acc: 0.03333\n",
      "step: 7750 / loss: 3.101948 / acc: 0.00000\n",
      "step: 7760 / loss: 3.086252 / acc: 0.03333\n",
      "step: 7770 / loss: 3.100387 / acc: 0.00000\n",
      "step: 7780 / loss: 3.091138 / acc: 0.03333\n",
      "step: 7790 / loss: 3.087053 / acc: 0.06667\n",
      "step: 7800 / loss: 3.106921 / acc: 0.00000\n",
      "step: 7810 / loss: 3.108994 / acc: 0.00000\n",
      "step: 7820 / loss: 3.100199 / acc: 0.03333\n",
      "step: 7830 / loss: 3.102799 / acc: 0.00000\n",
      "step: 7840 / loss: 3.104928 / acc: 0.00000\n",
      "step: 7850 / loss: 3.090833 / acc: 0.10000\n",
      "step: 7860 / loss: 3.114511 / acc: 0.00000\n",
      "step: 7870 / loss: 3.079182 / acc: 0.10000\n",
      "step: 7880 / loss: 3.098930 / acc: 0.03333\n",
      "step: 7890 / loss: 3.094825 / acc: 0.10000\n",
      "step: 7900 / loss: 3.119637 / acc: 0.00000\n",
      "step: 7910 / loss: 3.084555 / acc: 0.00000\n",
      "step: 7920 / loss: 3.100590 / acc: 0.03333\n",
      "step: 7930 / loss: 3.094774 / acc: 0.00000\n",
      "step: 7940 / loss: 3.093925 / acc: 0.06667\n",
      "step: 7950 / loss: 3.118218 / acc: 0.00000\n",
      "step: 7960 / loss: 3.101052 / acc: 0.00000\n",
      "step: 7970 / loss: 3.095998 / acc: 0.03333\n",
      "step: 7980 / loss: 3.084769 / acc: 0.00000\n",
      "step: 7990 / loss: 3.096976 / acc: 0.06667\n",
      "step: 8000 / loss: 3.095736 / acc: 0.06667\n",
      "step: 8010 / loss: 3.115874 / acc: 0.03333\n",
      "step: 8020 / loss: 3.081703 / acc: 0.10000\n",
      "step: 8030 / loss: 3.091938 / acc: 0.00000\n",
      "step: 8040 / loss: 3.065129 / acc: 0.16667\n",
      "step: 8050 / loss: 3.116092 / acc: 0.00000\n",
      "step: 8060 / loss: 3.110545 / acc: 0.03333\n",
      "step: 8070 / loss: 3.089292 / acc: 0.00000\n",
      "step: 8080 / loss: 3.092861 / acc: 0.06667\n",
      "step: 8090 / loss: 3.117650 / acc: 0.03333\n",
      "step: 8100 / loss: 3.111984 / acc: 0.06667\n",
      "step: 8110 / loss: 3.069391 / acc: 0.03333\n",
      "step: 8120 / loss: 3.128177 / acc: 0.00000\n",
      "step: 8130 / loss: 3.104278 / acc: 0.00000\n",
      "step: 8140 / loss: 3.071326 / acc: 0.10000\n",
      "step: 8150 / loss: 3.113513 / acc: 0.03333\n",
      "step: 8160 / loss: 3.072743 / acc: 0.06667\n",
      "step: 8170 / loss: 3.082799 / acc: 0.10000\n",
      "step: 8180 / loss: 3.107989 / acc: 0.03333\n",
      "step: 8190 / loss: 3.095791 / acc: 0.06667\n",
      "step: 8200 / loss: 3.117513 / acc: 0.03333\n",
      "step: 8210 / loss: 3.101423 / acc: 0.03333\n",
      "step: 8220 / loss: 3.101323 / acc: 0.06667\n",
      "step: 8230 / loss: 3.120383 / acc: 0.00000\n",
      "step: 8240 / loss: 3.072015 / acc: 0.00000\n",
      "step: 8250 / loss: 3.099518 / acc: 0.10000\n",
      "step: 8260 / loss: 3.106951 / acc: 0.06667\n",
      "step: 8270 / loss: 3.120388 / acc: 0.00000\n",
      "step: 8280 / loss: 3.078678 / acc: 0.03333\n",
      "step: 8290 / loss: 3.075799 / acc: 0.10000\n",
      "step: 8300 / loss: 3.075842 / acc: 0.03333\n",
      "step: 8310 / loss: 3.097074 / acc: 0.06667\n",
      "step: 8320 / loss: 3.065279 / acc: 0.06667\n",
      "step: 8330 / loss: 3.103390 / acc: 0.03333\n",
      "step: 8340 / loss: 3.120666 / acc: 0.00000\n",
      "step: 8350 / loss: 3.086575 / acc: 0.06667\n",
      "step: 8360 / loss: 3.107104 / acc: 0.03333\n",
      "step: 8370 / loss: 3.112175 / acc: 0.00000\n",
      "step: 8380 / loss: 3.105582 / acc: 0.06667\n",
      "step: 8390 / loss: 3.098763 / acc: 0.03333\n",
      "step: 8400 / loss: 3.108060 / acc: 0.00000\n",
      "step: 8410 / loss: 3.100885 / acc: 0.00000\n",
      "step: 8420 / loss: 3.100660 / acc: 0.00000\n",
      "step: 8430 / loss: 3.096462 / acc: 0.03333\n",
      "step: 8440 / loss: 3.083416 / acc: 0.03333\n",
      "step: 8450 / loss: 3.089158 / acc: 0.00000\n",
      "step: 8460 / loss: 3.089706 / acc: 0.06667\n",
      "step: 8470 / loss: 3.091195 / acc: 0.06667\n",
      "step: 8480 / loss: 3.097138 / acc: 0.00000\n",
      "step: 8490 / loss: 3.110251 / acc: 0.00000\n",
      "step: 8500 / loss: 3.103257 / acc: 0.06667\n",
      "step: 8510 / loss: 3.104547 / acc: 0.10000\n",
      "step: 8520 / loss: 3.065144 / acc: 0.10000\n",
      "step: 8530 / loss: 3.095715 / acc: 0.16667\n",
      "step: 8540 / loss: 3.100883 / acc: 0.06667\n",
      "step: 8550 / loss: 3.078928 / acc: 0.06667\n",
      "step: 8560 / loss: 3.094220 / acc: 0.03333\n",
      "step: 8570 / loss: 3.092359 / acc: 0.06667\n",
      "step: 8580 / loss: 3.101276 / acc: 0.03333\n",
      "step: 8590 / loss: 3.087193 / acc: 0.03333\n",
      "step: 8600 / loss: 3.107651 / acc: 0.00000\n",
      "step: 8610 / loss: 3.111167 / acc: 0.06667\n",
      "step: 8620 / loss: 3.135889 / acc: 0.06667\n",
      "step: 8630 / loss: 3.080426 / acc: 0.06667\n",
      "step: 8640 / loss: 3.081216 / acc: 0.00000\n",
      "step: 8650 / loss: 3.082264 / acc: 0.10000\n",
      "step: 8660 / loss: 3.109691 / acc: 0.03333\n",
      "step: 8670 / loss: 3.075604 / acc: 0.06667\n",
      "step: 8680 / loss: 3.097439 / acc: 0.03333\n",
      "step: 8690 / loss: 3.086002 / acc: 0.06667\n",
      "step: 8700 / loss: 3.108023 / acc: 0.03333\n",
      "step: 8710 / loss: 3.097746 / acc: 0.03333\n",
      "step: 8720 / loss: 3.114693 / acc: 0.00000\n",
      "step: 8730 / loss: 3.087309 / acc: 0.00000\n",
      "step: 8740 / loss: 3.091938 / acc: 0.00000\n",
      "step: 8750 / loss: 3.092076 / acc: 0.03333\n",
      "step: 8760 / loss: 3.110585 / acc: 0.00000\n",
      "step: 8770 / loss: 3.094593 / acc: 0.06667\n",
      "step: 8780 / loss: 3.104684 / acc: 0.00000\n",
      "step: 8790 / loss: 3.109456 / acc: 0.00000\n",
      "step: 8800 / loss: 3.089382 / acc: 0.03333\n",
      "step: 8810 / loss: 3.100605 / acc: 0.03333\n",
      "step: 8820 / loss: 3.127395 / acc: 0.03333\n",
      "step: 8830 / loss: 3.093185 / acc: 0.00000\n",
      "step: 8840 / loss: 3.089548 / acc: 0.03333\n",
      "step: 8850 / loss: 3.088329 / acc: 0.06667\n",
      "step: 8860 / loss: 3.104173 / acc: 0.06667\n",
      "step: 8870 / loss: 3.099013 / acc: 0.00000\n",
      "step: 8880 / loss: 3.071777 / acc: 0.06667\n",
      "step: 8890 / loss: 3.113590 / acc: 0.00000\n",
      "step: 8900 / loss: 3.082131 / acc: 0.10000\n",
      "step: 8910 / loss: 3.107741 / acc: 0.00000\n",
      "step: 8920 / loss: 3.124226 / acc: 0.00000\n",
      "step: 8930 / loss: 3.094869 / acc: 0.06667\n",
      "step: 8940 / loss: 3.083955 / acc: 0.10000\n",
      "step: 8950 / loss: 3.084728 / acc: 0.00000\n",
      "step: 8960 / loss: 3.095860 / acc: 0.10000\n",
      "step: 8970 / loss: 3.083734 / acc: 0.00000\n",
      "step: 8980 / loss: 3.085129 / acc: 0.03333\n",
      "step: 8990 / loss: 3.073047 / acc: 0.03333\n",
      "step: 9000 / loss: 3.102834 / acc: 0.06667\n",
      "step: 9010 / loss: 3.106932 / acc: 0.03333\n",
      "step: 9020 / loss: 3.089002 / acc: 0.03333\n",
      "step: 9030 / loss: 3.070895 / acc: 0.03333\n",
      "step: 9040 / loss: 3.088169 / acc: 0.00000\n",
      "step: 9050 / loss: 3.083204 / acc: 0.10000\n",
      "step: 9060 / loss: 3.092430 / acc: 0.10000\n",
      "step: 9070 / loss: 3.076709 / acc: 0.10000\n",
      "step: 9080 / loss: 3.112816 / acc: 0.03333\n",
      "step: 9090 / loss: 3.092895 / acc: 0.03333\n",
      "step: 9100 / loss: 3.093240 / acc: 0.10000\n",
      "step: 9110 / loss: 3.095374 / acc: 0.03333\n",
      "step: 9120 / loss: 3.094701 / acc: 0.03333\n",
      "step: 9130 / loss: 3.081539 / acc: 0.03333\n",
      "step: 9140 / loss: 3.110480 / acc: 0.03333\n",
      "step: 9150 / loss: 3.090360 / acc: 0.03333\n",
      "step: 9160 / loss: 3.092527 / acc: 0.10000\n",
      "step: 9170 / loss: 3.089907 / acc: 0.13333\n",
      "step: 9180 / loss: 3.093984 / acc: 0.03333\n",
      "step: 9190 / loss: 3.085101 / acc: 0.03333\n",
      "step: 9200 / loss: 3.088681 / acc: 0.00000\n",
      "step: 9210 / loss: 3.091065 / acc: 0.13333\n",
      "step: 9220 / loss: 3.088406 / acc: 0.03333\n",
      "step: 9230 / loss: 3.081131 / acc: 0.10000\n",
      "step: 9240 / loss: 3.089985 / acc: 0.00000\n",
      "step: 9250 / loss: 3.094965 / acc: 0.00000\n",
      "step: 9260 / loss: 3.111466 / acc: 0.10000\n",
      "step: 9270 / loss: 3.107704 / acc: 0.03333\n",
      "step: 9280 / loss: 3.091565 / acc: 0.03333\n",
      "step: 9290 / loss: 3.085179 / acc: 0.10000\n",
      "step: 9300 / loss: 3.094424 / acc: 0.06667\n",
      "step: 9310 / loss: 3.118914 / acc: 0.00000\n",
      "step: 9320 / loss: 3.088100 / acc: 0.06667\n",
      "step: 9330 / loss: 3.090571 / acc: 0.10000\n",
      "step: 9340 / loss: 3.064497 / acc: 0.06667\n",
      "step: 9350 / loss: 3.084266 / acc: 0.03333\n",
      "step: 9360 / loss: 3.112156 / acc: 0.10000\n",
      "step: 9370 / loss: 3.116882 / acc: 0.03333\n",
      "step: 9380 / loss: 3.083135 / acc: 0.06667\n",
      "step: 9390 / loss: 3.105380 / acc: 0.00000\n",
      "step: 9400 / loss: 3.117955 / acc: 0.03333\n",
      "step: 9410 / loss: 3.116046 / acc: 0.00000\n",
      "step: 9420 / loss: 3.111059 / acc: 0.00000\n",
      "step: 9430 / loss: 3.103188 / acc: 0.03333\n",
      "step: 9440 / loss: 3.098252 / acc: 0.03333\n",
      "step: 9450 / loss: 3.082055 / acc: 0.03333\n",
      "step: 9460 / loss: 3.083178 / acc: 0.10000\n",
      "step: 9470 / loss: 3.094275 / acc: 0.06667\n",
      "step: 9480 / loss: 3.095910 / acc: 0.03333\n",
      "step: 9490 / loss: 3.087627 / acc: 0.03333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9500 / loss: 3.085369 / acc: 0.06667\n",
      "step: 9510 / loss: 3.074300 / acc: 0.06667\n",
      "step: 9520 / loss: 3.114491 / acc: 0.03333\n",
      "step: 9530 / loss: 3.088955 / acc: 0.03333\n",
      "step: 9540 / loss: 3.099231 / acc: 0.00000\n",
      "step: 9550 / loss: 3.096547 / acc: 0.10000\n",
      "step: 9560 / loss: 3.112840 / acc: 0.03333\n",
      "step: 9570 / loss: 3.110056 / acc: 0.03333\n",
      "step: 9580 / loss: 3.092347 / acc: 0.00000\n",
      "step: 9590 / loss: 3.072464 / acc: 0.13333\n",
      "step: 9600 / loss: 3.132296 / acc: 0.00000\n",
      "step: 9610 / loss: 3.094776 / acc: 0.06667\n",
      "step: 9620 / loss: 3.114424 / acc: 0.00000\n",
      "step: 9630 / loss: 3.088796 / acc: 0.03333\n",
      "step: 9640 / loss: 3.076033 / acc: 0.06667\n",
      "step: 9650 / loss: 3.088674 / acc: 0.06667\n",
      "step: 9660 / loss: 3.095582 / acc: 0.06667\n",
      "step: 9670 / loss: 3.129875 / acc: 0.00000\n",
      "step: 9680 / loss: 3.093312 / acc: 0.03333\n",
      "step: 9690 / loss: 3.093468 / acc: 0.10000\n",
      "step: 9700 / loss: 3.066283 / acc: 0.06667\n",
      "step: 9710 / loss: 3.097320 / acc: 0.06667\n",
      "step: 9720 / loss: 3.074974 / acc: 0.03333\n",
      "step: 9730 / loss: 3.092004 / acc: 0.06667\n",
      "step: 9740 / loss: 3.066146 / acc: 0.13333\n",
      "step: 9750 / loss: 3.087455 / acc: 0.10000\n",
      "step: 9760 / loss: 3.092580 / acc: 0.03333\n",
      "step: 9770 / loss: 3.079333 / acc: 0.10000\n",
      "step: 9780 / loss: 3.099861 / acc: 0.00000\n",
      "step: 9790 / loss: 3.075899 / acc: 0.06667\n",
      "step: 9800 / loss: 3.085025 / acc: 0.06667\n",
      "step: 9810 / loss: 3.102432 / acc: 0.03333\n",
      "step: 9820 / loss: 3.083606 / acc: 0.00000\n",
      "step: 9830 / loss: 3.087222 / acc: 0.06667\n",
      "step: 9840 / loss: 3.089607 / acc: 0.06667\n",
      "step: 9850 / loss: 3.075201 / acc: 0.03333\n",
      "step: 9860 / loss: 3.067351 / acc: 0.16667\n",
      "step: 9870 / loss: 3.109923 / acc: 0.03333\n",
      "step: 9880 / loss: 3.089277 / acc: 0.06667\n",
      "step: 9890 / loss: 3.094107 / acc: 0.06667\n",
      "step: 9900 / loss: 3.089760 / acc: 0.06667\n",
      "step: 9910 / loss: 3.075281 / acc: 0.06667\n",
      "step: 9920 / loss: 3.103054 / acc: 0.03333\n",
      "step: 9930 / loss: 3.135446 / acc: 0.03333\n",
      "step: 9940 / loss: 3.067072 / acc: 0.06667\n",
      "step: 9950 / loss: 3.101371 / acc: 0.03333\n",
      "step: 9960 / loss: 3.068997 / acc: 0.10000\n",
      "step: 9970 / loss: 3.080533 / acc: 0.16667\n",
      "step: 9980 / loss: 3.089458 / acc: 0.03333\n",
      "step: 9990 / loss: 3.087150 / acc: 0.13333\n",
      "step: 10000 / loss: 3.071927 / acc: 0.03333\n",
      "step: 10010 / loss: 3.095675 / acc: 0.03333\n",
      "step: 10020 / loss: 3.094208 / acc: 0.03333\n",
      "step: 10030 / loss: 3.094761 / acc: 0.00000\n",
      "step: 10040 / loss: 3.096499 / acc: 0.00000\n",
      "step: 10050 / loss: 3.095908 / acc: 0.03333\n",
      "step: 10060 / loss: 3.090253 / acc: 0.06667\n",
      "step: 10070 / loss: 3.097015 / acc: 0.03333\n",
      "step: 10080 / loss: 3.093118 / acc: 0.03333\n",
      "step: 10090 / loss: 3.096756 / acc: 0.06667\n",
      "step: 10100 / loss: 3.092881 / acc: 0.06667\n",
      "step: 10110 / loss: 3.100273 / acc: 0.03333\n",
      "step: 10120 / loss: 3.110845 / acc: 0.03333\n",
      "step: 10130 / loss: 3.109898 / acc: 0.03333\n",
      "step: 10140 / loss: 3.099449 / acc: 0.03333\n",
      "step: 10150 / loss: 3.098337 / acc: 0.06667\n",
      "step: 10160 / loss: 3.104833 / acc: 0.03333\n",
      "step: 10170 / loss: 3.080415 / acc: 0.03333\n",
      "step: 10180 / loss: 3.083949 / acc: 0.13333\n",
      "step: 10190 / loss: 3.099664 / acc: 0.03333\n",
      "step: 10200 / loss: 3.086741 / acc: 0.00000\n",
      "step: 10210 / loss: 3.059300 / acc: 0.10000\n",
      "step: 10220 / loss: 3.088322 / acc: 0.06667\n",
      "step: 10230 / loss: 3.077732 / acc: 0.00000\n",
      "step: 10240 / loss: 3.069874 / acc: 0.06667\n",
      "step: 10250 / loss: 3.081318 / acc: 0.03333\n",
      "step: 10260 / loss: 3.096953 / acc: 0.03333\n",
      "step: 10270 / loss: 3.092724 / acc: 0.06667\n",
      "step: 10280 / loss: 3.088139 / acc: 0.06667\n",
      "step: 10290 / loss: 3.073463 / acc: 0.03333\n",
      "step: 10300 / loss: 3.104859 / acc: 0.03333\n",
      "step: 10310 / loss: 3.113949 / acc: 0.00000\n",
      "step: 10320 / loss: 3.071571 / acc: 0.13333\n",
      "step: 10330 / loss: 3.098563 / acc: 0.06667\n",
      "step: 10340 / loss: 3.060411 / acc: 0.13333\n",
      "step: 10350 / loss: 3.094149 / acc: 0.10000\n",
      "step: 10360 / loss: 3.100420 / acc: 0.06667\n",
      "step: 10370 / loss: 3.091473 / acc: 0.06667\n",
      "step: 10380 / loss: 3.066239 / acc: 0.16667\n",
      "step: 10390 / loss: 3.086188 / acc: 0.13333\n",
      "step: 10400 / loss: 3.100780 / acc: 0.03333\n",
      "step: 10410 / loss: 3.087309 / acc: 0.06667\n",
      "step: 10420 / loss: 3.097149 / acc: 0.10000\n",
      "step: 10430 / loss: 3.099406 / acc: 0.00000\n",
      "step: 10440 / loss: 3.110128 / acc: 0.06667\n",
      "step: 10450 / loss: 3.112850 / acc: 0.06667\n",
      "step: 10460 / loss: 3.080623 / acc: 0.03333\n",
      "step: 10470 / loss: 3.090587 / acc: 0.00000\n",
      "step: 10480 / loss: 3.083955 / acc: 0.03333\n",
      "step: 10490 / loss: 3.080437 / acc: 0.06667\n",
      "step: 10500 / loss: 3.102663 / acc: 0.03333\n",
      "step: 10510 / loss: 3.100349 / acc: 0.06667\n",
      "step: 10520 / loss: 3.080049 / acc: 0.06667\n",
      "step: 10530 / loss: 3.111973 / acc: 0.06667\n",
      "step: 10540 / loss: 3.095797 / acc: 0.06667\n",
      "step: 10550 / loss: 3.079072 / acc: 0.16667\n",
      "step: 10560 / loss: 3.092768 / acc: 0.00000\n",
      "step: 10570 / loss: 3.103225 / acc: 0.13333\n",
      "step: 10580 / loss: 3.085304 / acc: 0.06667\n",
      "step: 10590 / loss: 3.092297 / acc: 0.00000\n",
      "step: 10600 / loss: 3.102425 / acc: 0.06667\n",
      "step: 10610 / loss: 3.078069 / acc: 0.06667\n",
      "step: 10620 / loss: 3.088383 / acc: 0.10000\n",
      "step: 10630 / loss: 3.088096 / acc: 0.00000\n",
      "step: 10640 / loss: 3.089110 / acc: 0.06667\n",
      "step: 10650 / loss: 3.097750 / acc: 0.00000\n",
      "step: 10660 / loss: 3.120213 / acc: 0.06667\n",
      "step: 10670 / loss: 3.101846 / acc: 0.03333\n",
      "step: 10680 / loss: 3.122046 / acc: 0.00000\n",
      "step: 10690 / loss: 3.111575 / acc: 0.06667\n",
      "step: 10700 / loss: 3.090455 / acc: 0.00000\n",
      "step: 10710 / loss: 3.084000 / acc: 0.06667\n",
      "step: 10720 / loss: 3.090463 / acc: 0.00000\n",
      "step: 10730 / loss: 3.087124 / acc: 0.06667\n",
      "step: 10740 / loss: 3.073386 / acc: 0.00000\n",
      "step: 10750 / loss: 3.096880 / acc: 0.03333\n",
      "step: 10760 / loss: 3.098513 / acc: 0.03333\n",
      "step: 10770 / loss: 3.084617 / acc: 0.03333\n",
      "step: 10780 / loss: 3.082297 / acc: 0.03333\n",
      "step: 10790 / loss: 3.076600 / acc: 0.00000\n",
      "step: 10800 / loss: 3.131379 / acc: 0.00000\n",
      "step: 10810 / loss: 3.124904 / acc: 0.03333\n",
      "step: 10820 / loss: 3.075793 / acc: 0.03333\n",
      "step: 10830 / loss: 3.105395 / acc: 0.13333\n",
      "step: 10840 / loss: 3.098846 / acc: 0.03333\n",
      "step: 10850 / loss: 3.113076 / acc: 0.00000\n",
      "step: 10860 / loss: 3.087123 / acc: 0.06667\n",
      "step: 10870 / loss: 3.088541 / acc: 0.03333\n",
      "step: 10880 / loss: 3.101635 / acc: 0.03333\n",
      "step: 10890 / loss: 3.082438 / acc: 0.06667\n",
      "step: 10900 / loss: 3.096743 / acc: 0.00000\n",
      "step: 10910 / loss: 3.096099 / acc: 0.03333\n",
      "step: 10920 / loss: 3.084012 / acc: 0.10000\n",
      "step: 10930 / loss: 3.072961 / acc: 0.03333\n",
      "step: 10940 / loss: 3.098089 / acc: 0.00000\n",
      "step: 10950 / loss: 3.103229 / acc: 0.03333\n",
      "step: 10960 / loss: 3.085504 / acc: 0.03333\n",
      "step: 10970 / loss: 3.074345 / acc: 0.10000\n",
      "step: 10980 / loss: 3.106676 / acc: 0.00000\n",
      "step: 10990 / loss: 3.102302 / acc: 0.00000\n",
      "step: 11000 / loss: 3.088802 / acc: 0.06667\n",
      "step: 11010 / loss: 3.089843 / acc: 0.10000\n",
      "step: 11020 / loss: 3.096877 / acc: 0.03333\n",
      "step: 11030 / loss: 3.074155 / acc: 0.16667\n",
      "step: 11040 / loss: 3.074579 / acc: 0.06667\n",
      "step: 11050 / loss: 3.086470 / acc: 0.06667\n",
      "step: 11060 / loss: 3.086666 / acc: 0.00000\n",
      "step: 11070 / loss: 3.079291 / acc: 0.00000\n",
      "step: 11080 / loss: 3.088742 / acc: 0.06667\n",
      "step: 11090 / loss: 3.084579 / acc: 0.06667\n",
      "step: 11100 / loss: 3.086722 / acc: 0.06667\n",
      "step: 11110 / loss: 3.111229 / acc: 0.03333\n",
      "step: 11120 / loss: 3.098864 / acc: 0.10000\n",
      "step: 11130 / loss: 3.094148 / acc: 0.03333\n",
      "step: 11140 / loss: 3.077252 / acc: 0.03333\n",
      "step: 11150 / loss: 3.090630 / acc: 0.03333\n",
      "step: 11160 / loss: 3.096340 / acc: 0.06667\n",
      "step: 11170 / loss: 3.068193 / acc: 0.06667\n",
      "step: 11180 / loss: 3.100768 / acc: 0.06667\n",
      "step: 11190 / loss: 3.110503 / acc: 0.03333\n",
      "step: 11200 / loss: 3.087068 / acc: 0.03333\n",
      "step: 11210 / loss: 3.114174 / acc: 0.06667\n",
      "step: 11220 / loss: 3.090016 / acc: 0.03333\n",
      "step: 11230 / loss: 3.099329 / acc: 0.00000\n",
      "step: 11240 / loss: 3.096420 / acc: 0.06667\n",
      "step: 11250 / loss: 3.082459 / acc: 0.00000\n",
      "step: 11260 / loss: 3.105448 / acc: 0.00000\n",
      "step: 11270 / loss: 3.103515 / acc: 0.06667\n",
      "step: 11280 / loss: 3.103898 / acc: 0.00000\n",
      "step: 11290 / loss: 3.101356 / acc: 0.10000\n",
      "step: 11300 / loss: 3.105381 / acc: 0.06667\n",
      "step: 11310 / loss: 3.091396 / acc: 0.06667\n",
      "step: 11320 / loss: 3.103346 / acc: 0.03333\n",
      "step: 11330 / loss: 3.089765 / acc: 0.00000\n",
      "step: 11340 / loss: 3.103934 / acc: 0.06667\n",
      "step: 11350 / loss: 3.093665 / acc: 0.06667\n",
      "step: 11360 / loss: 3.110329 / acc: 0.00000\n",
      "step: 11370 / loss: 3.081291 / acc: 0.06667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 11380 / loss: 3.080207 / acc: 0.10000\n",
      "step: 11390 / loss: 3.082789 / acc: 0.06667\n",
      "step: 11400 / loss: 3.085320 / acc: 0.06667\n",
      "step: 11410 / loss: 3.088834 / acc: 0.00000\n",
      "step: 11420 / loss: 3.096457 / acc: 0.00000\n",
      "step: 11430 / loss: 3.094344 / acc: 0.00000\n",
      "step: 11440 / loss: 3.101653 / acc: 0.03333\n",
      "step: 11450 / loss: 3.102130 / acc: 0.00000\n",
      "step: 11460 / loss: 3.090020 / acc: 0.06667\n",
      "step: 11470 / loss: 3.095418 / acc: 0.03333\n",
      "step: 11480 / loss: 3.095122 / acc: 0.03333\n",
      "step: 11490 / loss: 3.096250 / acc: 0.06667\n",
      "step: 11500 / loss: 3.089528 / acc: 0.03333\n",
      "step: 11510 / loss: 3.094740 / acc: 0.06667\n",
      "step: 11520 / loss: 3.077349 / acc: 0.03333\n",
      "step: 11530 / loss: 3.082740 / acc: 0.03333\n",
      "step: 11540 / loss: 3.084250 / acc: 0.06667\n",
      "step: 11550 / loss: 3.115345 / acc: 0.00000\n",
      "step: 11560 / loss: 3.101192 / acc: 0.00000\n",
      "step: 11570 / loss: 3.076767 / acc: 0.13333\n",
      "step: 11580 / loss: 3.099450 / acc: 0.10000\n",
      "step: 11590 / loss: 3.103934 / acc: 0.00000\n",
      "step: 11600 / loss: 3.082011 / acc: 0.06667\n",
      "step: 11610 / loss: 3.093007 / acc: 0.03333\n",
      "step: 11620 / loss: 3.081896 / acc: 0.03333\n",
      "step: 11630 / loss: 3.105936 / acc: 0.06667\n",
      "step: 11640 / loss: 3.088749 / acc: 0.00000\n",
      "step: 11650 / loss: 3.092135 / acc: 0.03333\n",
      "step: 11660 / loss: 3.098865 / acc: 0.03333\n",
      "step: 11670 / loss: 3.098199 / acc: 0.06667\n",
      "step: 11680 / loss: 3.092275 / acc: 0.03333\n",
      "step: 11690 / loss: 3.068691 / acc: 0.10000\n",
      "step: 11700 / loss: 3.114881 / acc: 0.00000\n",
      "step: 11710 / loss: 3.089846 / acc: 0.03333\n",
      "step: 11720 / loss: 3.091817 / acc: 0.03333\n",
      "step: 11730 / loss: 3.084888 / acc: 0.03333\n",
      "step: 11740 / loss: 3.101772 / acc: 0.00000\n",
      "step: 11750 / loss: 3.105522 / acc: 0.00000\n",
      "step: 11760 / loss: 3.078895 / acc: 0.03333\n",
      "step: 11770 / loss: 3.102509 / acc: 0.03333\n",
      "step: 11780 / loss: 3.092879 / acc: 0.03333\n",
      "step: 11790 / loss: 3.085502 / acc: 0.03333\n",
      "step: 11800 / loss: 3.093911 / acc: 0.06667\n",
      "step: 11810 / loss: 3.074777 / acc: 0.10000\n",
      "step: 11820 / loss: 3.102038 / acc: 0.00000\n",
      "step: 11830 / loss: 3.090172 / acc: 0.00000\n",
      "step: 11840 / loss: 3.082187 / acc: 0.06667\n",
      "step: 11850 / loss: 3.104287 / acc: 0.00000\n",
      "step: 11860 / loss: 3.088526 / acc: 0.13333\n",
      "step: 11870 / loss: 3.088171 / acc: 0.06667\n",
      "step: 11880 / loss: 3.098791 / acc: 0.06667\n",
      "step: 11890 / loss: 3.100546 / acc: 0.06667\n",
      "step: 11900 / loss: 3.077841 / acc: 0.06667\n",
      "step: 11910 / loss: 3.093443 / acc: 0.03333\n",
      "step: 11920 / loss: 3.079364 / acc: 0.10000\n",
      "step: 11930 / loss: 3.087687 / acc: 0.00000\n",
      "step: 11940 / loss: 3.098862 / acc: 0.03333\n",
      "step: 11950 / loss: 3.110356 / acc: 0.03333\n",
      "step: 11960 / loss: 3.101069 / acc: 0.10000\n",
      "step: 11970 / loss: 3.084470 / acc: 0.03333\n",
      "step: 11980 / loss: 3.081031 / acc: 0.03333\n",
      "step: 11990 / loss: 3.081462 / acc: 0.13333\n",
      "step: 12000 / loss: 3.106804 / acc: 0.00000\n",
      "step: 12010 / loss: 3.102690 / acc: 0.00000\n",
      "step: 12020 / loss: 3.094630 / acc: 0.00000\n",
      "step: 12030 / loss: 3.087103 / acc: 0.03333\n",
      "step: 12040 / loss: 3.125391 / acc: 0.03333\n",
      "step: 12050 / loss: 3.090460 / acc: 0.06667\n",
      "step: 12060 / loss: 3.085989 / acc: 0.10000\n",
      "step: 12070 / loss: 3.075486 / acc: 0.06667\n",
      "step: 12080 / loss: 3.097730 / acc: 0.06667\n",
      "step: 12090 / loss: 3.106451 / acc: 0.10000\n",
      "step: 12100 / loss: 3.089026 / acc: 0.06667\n",
      "step: 12110 / loss: 3.096938 / acc: 0.00000\n",
      "step: 12120 / loss: 3.094967 / acc: 0.00000\n",
      "step: 12130 / loss: 3.090240 / acc: 0.03333\n",
      "step: 12140 / loss: 3.106347 / acc: 0.00000\n",
      "step: 12150 / loss: 3.088917 / acc: 0.00000\n",
      "step: 12160 / loss: 3.095424 / acc: 0.06667\n",
      "step: 12170 / loss: 3.099131 / acc: 0.00000\n",
      "step: 12180 / loss: 3.082653 / acc: 0.03333\n",
      "step: 12190 / loss: 3.119592 / acc: 0.10000\n",
      "step: 12200 / loss: 3.082322 / acc: 0.06667\n",
      "step: 12210 / loss: 3.120088 / acc: 0.00000\n",
      "step: 12220 / loss: 3.088318 / acc: 0.03333\n",
      "step: 12230 / loss: 3.082019 / acc: 0.10000\n",
      "step: 12240 / loss: 3.095879 / acc: 0.03333\n",
      "step: 12250 / loss: 3.103440 / acc: 0.00000\n",
      "step: 12260 / loss: 3.115620 / acc: 0.03333\n",
      "step: 12270 / loss: 3.099272 / acc: 0.00000\n",
      "step: 12280 / loss: 3.092470 / acc: 0.03333\n",
      "step: 12290 / loss: 3.089403 / acc: 0.06667\n",
      "step: 12300 / loss: 3.087964 / acc: 0.00000\n",
      "step: 12310 / loss: 3.096187 / acc: 0.03333\n",
      "step: 12320 / loss: 3.099778 / acc: 0.00000\n",
      "step: 12330 / loss: 3.091683 / acc: 0.00000\n",
      "step: 12340 / loss: 3.099818 / acc: 0.03333\n",
      "step: 12350 / loss: 3.101521 / acc: 0.00000\n",
      "step: 12360 / loss: 3.091062 / acc: 0.00000\n",
      "step: 12370 / loss: 3.097247 / acc: 0.06667\n",
      "step: 12380 / loss: 3.098276 / acc: 0.06667\n",
      "step: 12390 / loss: 3.101200 / acc: 0.03333\n",
      "step: 12400 / loss: 3.079717 / acc: 0.06667\n",
      "step: 12410 / loss: 3.110783 / acc: 0.03333\n",
      "step: 12420 / loss: 3.088756 / acc: 0.10000\n",
      "step: 12430 / loss: 3.101985 / acc: 0.00000\n",
      "step: 12440 / loss: 3.093977 / acc: 0.00000\n",
      "step: 12450 / loss: 3.090311 / acc: 0.06667\n",
      "step: 12460 / loss: 3.093179 / acc: 0.03333\n",
      "step: 12470 / loss: 3.096581 / acc: 0.06667\n",
      "step: 12480 / loss: 3.104239 / acc: 0.03333\n",
      "step: 12490 / loss: 3.092770 / acc: 0.03333\n",
      "step: 12500 / loss: 3.098772 / acc: 0.00000\n",
      "step: 12510 / loss: 3.089047 / acc: 0.06667\n",
      "step: 12520 / loss: 3.095726 / acc: 0.00000\n",
      "step: 12530 / loss: 3.099676 / acc: 0.06667\n",
      "step: 12540 / loss: 3.104141 / acc: 0.03333\n",
      "step: 12550 / loss: 3.083457 / acc: 0.03333\n",
      "step: 12560 / loss: 3.085849 / acc: 0.03333\n",
      "step: 12570 / loss: 3.108564 / acc: 0.03333\n",
      "step: 12580 / loss: 3.077650 / acc: 0.10000\n",
      "step: 12590 / loss: 3.095171 / acc: 0.00000\n",
      "step: 12600 / loss: 3.082685 / acc: 0.06667\n",
      "step: 12610 / loss: 3.096339 / acc: 0.06667\n",
      "step: 12620 / loss: 3.102653 / acc: 0.03333\n",
      "step: 12630 / loss: 3.087608 / acc: 0.03333\n",
      "step: 12640 / loss: 3.099867 / acc: 0.06667\n",
      "step: 12650 / loss: 3.089959 / acc: 0.03333\n",
      "step: 12660 / loss: 3.081527 / acc: 0.06667\n",
      "step: 12670 / loss: 3.079924 / acc: 0.03333\n",
      "step: 12680 / loss: 3.113359 / acc: 0.03333\n",
      "step: 12690 / loss: 3.086226 / acc: 0.06667\n",
      "step: 12700 / loss: 3.101033 / acc: 0.03333\n",
      "step: 12710 / loss: 3.091407 / acc: 0.00000\n",
      "step: 12720 / loss: 3.101471 / acc: 0.00000\n",
      "step: 12730 / loss: 3.093426 / acc: 0.03333\n",
      "step: 12740 / loss: 3.088562 / acc: 0.10000\n",
      "step: 12750 / loss: 3.090642 / acc: 0.06667\n",
      "step: 12760 / loss: 3.085942 / acc: 0.00000\n",
      "step: 12770 / loss: 3.102191 / acc: 0.06667\n",
      "step: 12780 / loss: 3.082817 / acc: 0.03333\n",
      "step: 12790 / loss: 3.095105 / acc: 0.00000\n",
      "step: 12800 / loss: 3.102024 / acc: 0.03333\n",
      "step: 12810 / loss: 3.087888 / acc: 0.00000\n",
      "step: 12820 / loss: 3.087349 / acc: 0.03333\n",
      "step: 12830 / loss: 3.100211 / acc: 0.03333\n",
      "step: 12840 / loss: 3.097561 / acc: 0.10000\n",
      "step: 12850 / loss: 3.071567 / acc: 0.06667\n",
      "step: 12860 / loss: 3.115129 / acc: 0.00000\n",
      "step: 12870 / loss: 3.096413 / acc: 0.00000\n",
      "step: 12880 / loss: 3.088215 / acc: 0.06667\n",
      "step: 12890 / loss: 3.087198 / acc: 0.06667\n",
      "step: 12900 / loss: 3.098171 / acc: 0.10000\n",
      "step: 12910 / loss: 3.105996 / acc: 0.00000\n",
      "step: 12920 / loss: 3.070765 / acc: 0.06667\n",
      "step: 12930 / loss: 3.084222 / acc: 0.06667\n",
      "step: 12940 / loss: 3.099528 / acc: 0.06667\n",
      "step: 12950 / loss: 3.100917 / acc: 0.00000\n",
      "step: 12960 / loss: 3.098735 / acc: 0.03333\n",
      "step: 12970 / loss: 3.095553 / acc: 0.06667\n",
      "step: 12980 / loss: 3.101186 / acc: 0.00000\n",
      "step: 12990 / loss: 3.084779 / acc: 0.06667\n",
      "step: 13000 / loss: 3.098395 / acc: 0.00000\n",
      "step: 13010 / loss: 3.096725 / acc: 0.03333\n",
      "step: 13020 / loss: 3.092318 / acc: 0.03333\n",
      "step: 13030 / loss: 3.085722 / acc: 0.10000\n",
      "step: 13040 / loss: 3.088654 / acc: 0.10000\n",
      "step: 13050 / loss: 3.098371 / acc: 0.00000\n",
      "step: 13060 / loss: 3.095256 / acc: 0.06667\n",
      "step: 13070 / loss: 3.084403 / acc: 0.03333\n",
      "step: 13080 / loss: 3.107661 / acc: 0.00000\n",
      "step: 13090 / loss: 3.068347 / acc: 0.06667\n",
      "step: 13100 / loss: 3.092711 / acc: 0.03333\n",
      "step: 13110 / loss: 3.105204 / acc: 0.00000\n",
      "step: 13120 / loss: 3.076394 / acc: 0.06667\n",
      "step: 13130 / loss: 3.090490 / acc: 0.00000\n",
      "step: 13140 / loss: 3.100604 / acc: 0.06667\n",
      "step: 13150 / loss: 3.118255 / acc: 0.03333\n",
      "step: 13160 / loss: 3.113906 / acc: 0.00000\n",
      "step: 13170 / loss: 3.104142 / acc: 0.03333\n",
      "step: 13180 / loss: 3.104538 / acc: 0.00000\n",
      "step: 13190 / loss: 3.091673 / acc: 0.03333\n",
      "step: 13200 / loss: 3.081775 / acc: 0.13333\n",
      "step: 13210 / loss: 3.096995 / acc: 0.00000\n",
      "step: 13220 / loss: 3.105608 / acc: 0.03333\n",
      "step: 13230 / loss: 3.109367 / acc: 0.00000\n",
      "step: 13240 / loss: 3.082016 / acc: 0.06667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 13250 / loss: 3.092993 / acc: 0.06667\n",
      "step: 13260 / loss: 3.102723 / acc: 0.00000\n",
      "step: 13270 / loss: 3.089023 / acc: 0.10000\n",
      "step: 13280 / loss: 3.113542 / acc: 0.10000\n",
      "step: 13290 / loss: 3.108872 / acc: 0.06667\n",
      "step: 13300 / loss: 3.091376 / acc: 0.03333\n",
      "step: 13310 / loss: 3.075096 / acc: 0.10000\n",
      "step: 13320 / loss: 3.096873 / acc: 0.00000\n",
      "step: 13330 / loss: 3.099856 / acc: 0.03333\n",
      "step: 13340 / loss: 3.115619 / acc: 0.00000\n",
      "step: 13350 / loss: 3.082057 / acc: 0.03333\n",
      "step: 13360 / loss: 3.089550 / acc: 0.10000\n",
      "step: 13370 / loss: 3.077174 / acc: 0.10000\n",
      "step: 13380 / loss: 3.102086 / acc: 0.06667\n",
      "step: 13390 / loss: 3.093714 / acc: 0.06667\n",
      "step: 13400 / loss: 3.101462 / acc: 0.03333\n",
      "step: 13410 / loss: 3.114119 / acc: 0.00000\n",
      "step: 13420 / loss: 3.096224 / acc: 0.03333\n",
      "step: 13430 / loss: 3.094760 / acc: 0.03333\n",
      "step: 13440 / loss: 3.080221 / acc: 0.03333\n",
      "step: 13450 / loss: 3.103395 / acc: 0.03333\n",
      "step: 13460 / loss: 3.100088 / acc: 0.06667\n",
      "step: 13470 / loss: 3.097196 / acc: 0.00000\n",
      "step: 13480 / loss: 3.095782 / acc: 0.00000\n",
      "step: 13490 / loss: 3.091667 / acc: 0.03333\n",
      "step: 13500 / loss: 3.089723 / acc: 0.10000\n",
      "step: 13510 / loss: 3.104656 / acc: 0.00000\n",
      "step: 13520 / loss: 3.090771 / acc: 0.00000\n",
      "step: 13530 / loss: 3.105828 / acc: 0.03333\n",
      "step: 13540 / loss: 3.086582 / acc: 0.03333\n",
      "step: 13550 / loss: 3.094298 / acc: 0.06667\n",
      "step: 13560 / loss: 3.083642 / acc: 0.03333\n",
      "step: 13570 / loss: 3.099526 / acc: 0.03333\n",
      "step: 13580 / loss: 3.087439 / acc: 0.13333\n",
      "step: 13590 / loss: 3.097553 / acc: 0.03333\n",
      "step: 13600 / loss: 3.090310 / acc: 0.10000\n",
      "step: 13610 / loss: 3.101704 / acc: 0.03333\n",
      "step: 13620 / loss: 3.098574 / acc: 0.03333\n",
      "step: 13630 / loss: 3.088253 / acc: 0.00000\n",
      "step: 13640 / loss: 3.071851 / acc: 0.10000\n",
      "step: 13650 / loss: 3.082658 / acc: 0.06667\n",
      "step: 13660 / loss: 3.084932 / acc: 0.00000\n",
      "step: 13670 / loss: 3.097288 / acc: 0.06667\n",
      "step: 13680 / loss: 3.090669 / acc: 0.03333\n",
      "step: 13690 / loss: 3.108013 / acc: 0.00000\n",
      "step: 13700 / loss: 3.104957 / acc: 0.03333\n",
      "step: 13710 / loss: 3.077966 / acc: 0.06667\n",
      "step: 13720 / loss: 3.087762 / acc: 0.03333\n",
      "step: 13730 / loss: 3.107337 / acc: 0.03333\n",
      "step: 13740 / loss: 3.080273 / acc: 0.03333\n",
      "step: 13750 / loss: 3.071326 / acc: 0.06667\n",
      "step: 13760 / loss: 3.102637 / acc: 0.03333\n",
      "step: 13770 / loss: 3.094840 / acc: 0.00000\n",
      "step: 13780 / loss: 3.102981 / acc: 0.03333\n",
      "step: 13790 / loss: 3.084970 / acc: 0.06667\n",
      "step: 13800 / loss: 3.095110 / acc: 0.03333\n",
      "step: 13810 / loss: 3.078702 / acc: 0.03333\n",
      "step: 13820 / loss: 3.086216 / acc: 0.10000\n",
      "step: 13830 / loss: 3.087702 / acc: 0.13333\n",
      "step: 13840 / loss: 3.093941 / acc: 0.00000\n",
      "step: 13850 / loss: 3.097183 / acc: 0.03333\n",
      "step: 13860 / loss: 3.101383 / acc: 0.03333\n",
      "step: 13870 / loss: 3.093418 / acc: 0.03333\n",
      "step: 13880 / loss: 3.076183 / acc: 0.03333\n",
      "step: 13890 / loss: 3.097607 / acc: 0.10000\n",
      "step: 13900 / loss: 3.099440 / acc: 0.00000\n",
      "step: 13910 / loss: 3.096052 / acc: 0.00000\n",
      "step: 13920 / loss: 3.073351 / acc: 0.03333\n",
      "step: 13930 / loss: 3.085871 / acc: 0.06667\n",
      "step: 13940 / loss: 3.101197 / acc: 0.00000\n",
      "step: 13950 / loss: 3.103751 / acc: 0.03333\n",
      "step: 13960 / loss: 3.092633 / acc: 0.03333\n",
      "step: 13970 / loss: 3.092345 / acc: 0.06667\n",
      "step: 13980 / loss: 3.083569 / acc: 0.06667\n",
      "step: 13990 / loss: 3.101294 / acc: 0.00000\n",
      "step: 14000 / loss: 3.100367 / acc: 0.03333\n",
      "step: 14010 / loss: 3.095697 / acc: 0.03333\n",
      "step: 14020 / loss: 3.095227 / acc: 0.00000\n",
      "step: 14030 / loss: 3.096799 / acc: 0.00000\n",
      "step: 14040 / loss: 3.099941 / acc: 0.00000\n",
      "step: 14050 / loss: 3.095291 / acc: 0.06667\n",
      "step: 14060 / loss: 3.090590 / acc: 0.03333\n",
      "step: 14070 / loss: 3.087872 / acc: 0.03333\n",
      "step: 14080 / loss: 3.100204 / acc: 0.00000\n",
      "step: 14090 / loss: 3.101628 / acc: 0.00000\n",
      "step: 14100 / loss: 3.093899 / acc: 0.03333\n",
      "step: 14110 / loss: 3.092155 / acc: 0.10000\n",
      "step: 14120 / loss: 3.084387 / acc: 0.03333\n",
      "step: 14130 / loss: 3.099271 / acc: 0.10000\n",
      "step: 14140 / loss: 3.090680 / acc: 0.06667\n",
      "step: 14150 / loss: 3.118262 / acc: 0.00000\n",
      "step: 14160 / loss: 3.091031 / acc: 0.03333\n",
      "step: 14170 / loss: 3.092641 / acc: 0.00000\n",
      "step: 14180 / loss: 3.096424 / acc: 0.03333\n",
      "step: 14190 / loss: 3.099491 / acc: 0.00000\n",
      "step: 14200 / loss: 3.081801 / acc: 0.10000\n",
      "step: 14210 / loss: 3.088953 / acc: 0.00000\n",
      "step: 14220 / loss: 3.093198 / acc: 0.03333\n",
      "step: 14230 / loss: 3.083896 / acc: 0.00000\n",
      "step: 14240 / loss: 3.105868 / acc: 0.03333\n",
      "step: 14250 / loss: 3.097552 / acc: 0.03333\n",
      "step: 14260 / loss: 3.097190 / acc: 0.06667\n",
      "step: 14270 / loss: 3.094340 / acc: 0.03333\n",
      "step: 14280 / loss: 3.099921 / acc: 0.00000\n",
      "step: 14290 / loss: 3.092580 / acc: 0.03333\n",
      "step: 14300 / loss: 3.103358 / acc: 0.03333\n",
      "step: 14310 / loss: 3.076873 / acc: 0.10000\n",
      "step: 14320 / loss: 3.102574 / acc: 0.00000\n",
      "step: 14330 / loss: 3.113153 / acc: 0.03333\n",
      "step: 14340 / loss: 3.089165 / acc: 0.06667\n",
      "step: 14350 / loss: 3.090843 / acc: 0.03333\n",
      "step: 14360 / loss: 3.091274 / acc: 0.03333\n",
      "step: 14370 / loss: 3.094390 / acc: 0.03333\n",
      "step: 14380 / loss: 3.059934 / acc: 0.06667\n",
      "step: 14390 / loss: 3.090808 / acc: 0.03333\n",
      "step: 14400 / loss: 3.101979 / acc: 0.06667\n",
      "step: 14410 / loss: 3.092181 / acc: 0.03333\n",
      "step: 14420 / loss: 3.092853 / acc: 0.03333\n",
      "step: 14430 / loss: 3.110180 / acc: 0.10000\n",
      "step: 14440 / loss: 3.077586 / acc: 0.10000\n",
      "step: 14450 / loss: 3.091660 / acc: 0.03333\n",
      "step: 14460 / loss: 3.086933 / acc: 0.06667\n",
      "step: 14470 / loss: 3.100209 / acc: 0.03333\n",
      "step: 14480 / loss: 3.090976 / acc: 0.06667\n",
      "step: 14490 / loss: 3.085788 / acc: 0.06667\n",
      "step: 14500 / loss: 3.086336 / acc: 0.06667\n",
      "step: 14510 / loss: 3.089552 / acc: 0.03333\n",
      "step: 14520 / loss: 3.091900 / acc: 0.03333\n",
      "step: 14530 / loss: 3.072264 / acc: 0.10000\n",
      "step: 14540 / loss: 3.114809 / acc: 0.03333\n",
      "step: 14550 / loss: 3.093093 / acc: 0.03333\n",
      "step: 14560 / loss: 3.086381 / acc: 0.03333\n",
      "step: 14570 / loss: 3.091508 / acc: 0.03333\n",
      "step: 14580 / loss: 3.124740 / acc: 0.03333\n",
      "step: 14590 / loss: 3.093681 / acc: 0.03333\n",
      "step: 14600 / loss: 3.071044 / acc: 0.03333\n",
      "step: 14610 / loss: 3.086904 / acc: 0.06667\n",
      "step: 14620 / loss: 3.074379 / acc: 0.16667\n",
      "step: 14630 / loss: 3.082746 / acc: 0.06667\n",
      "step: 14640 / loss: 3.085073 / acc: 0.03333\n",
      "step: 14650 / loss: 3.087209 / acc: 0.06667\n",
      "step: 14660 / loss: 3.097219 / acc: 0.03333\n",
      "step: 14670 / loss: 3.099017 / acc: 0.00000\n",
      "step: 14680 / loss: 3.098330 / acc: 0.00000\n",
      "step: 14690 / loss: 3.101362 / acc: 0.00000\n",
      "step: 14700 / loss: 3.081632 / acc: 0.10000\n",
      "step: 14710 / loss: 3.076360 / acc: 0.06667\n",
      "step: 14720 / loss: 3.097123 / acc: 0.00000\n",
      "step: 14730 / loss: 3.108245 / acc: 0.03333\n",
      "step: 14740 / loss: 3.095017 / acc: 0.00000\n",
      "step: 14750 / loss: 3.105872 / acc: 0.00000\n",
      "step: 14760 / loss: 3.089547 / acc: 0.06667\n",
      "step: 14770 / loss: 3.086163 / acc: 0.06667\n",
      "step: 14780 / loss: 3.087592 / acc: 0.06667\n",
      "step: 14790 / loss: 3.103978 / acc: 0.03333\n",
      "step: 14800 / loss: 3.095193 / acc: 0.06667\n",
      "step: 14810 / loss: 3.094177 / acc: 0.03333\n",
      "step: 14820 / loss: 3.091270 / acc: 0.06667\n",
      "step: 14830 / loss: 3.104633 / acc: 0.03333\n",
      "step: 14840 / loss: 3.086875 / acc: 0.06667\n",
      "step: 14850 / loss: 3.094732 / acc: 0.10000\n",
      "step: 14860 / loss: 3.083493 / acc: 0.00000\n",
      "step: 14870 / loss: 3.097328 / acc: 0.10000\n",
      "step: 14880 / loss: 3.094002 / acc: 0.06667\n",
      "step: 14890 / loss: 3.103223 / acc: 0.03333\n",
      "step: 14900 / loss: 3.089358 / acc: 0.03333\n",
      "step: 14910 / loss: 3.085075 / acc: 0.10000\n",
      "step: 14920 / loss: 3.085915 / acc: 0.03333\n",
      "step: 14930 / loss: 3.099240 / acc: 0.03333\n",
      "step: 14940 / loss: 3.104169 / acc: 0.00000\n",
      "step: 14950 / loss: 3.081914 / acc: 0.00000\n",
      "step: 14960 / loss: 3.086040 / acc: 0.06667\n",
      "step: 14970 / loss: 3.088907 / acc: 0.03333\n",
      "step: 14980 / loss: 3.102269 / acc: 0.03333\n",
      "step: 14990 / loss: 3.079204 / acc: 0.03333\n",
      "step: 15000 / loss: 3.098660 / acc: 0.10000\n",
      "step: 15010 / loss: 3.099251 / acc: 0.06667\n",
      "step: 15020 / loss: 3.089071 / acc: 0.03333\n",
      "step: 15030 / loss: 3.104373 / acc: 0.03333\n",
      "step: 15040 / loss: 3.097211 / acc: 0.03333\n",
      "step: 15050 / loss: 3.076779 / acc: 0.06667\n",
      "step: 15060 / loss: 3.141185 / acc: 0.03333\n",
      "step: 15070 / loss: 3.094924 / acc: 0.13333\n",
      "step: 15080 / loss: 3.105829 / acc: 0.03333\n",
      "step: 15090 / loss: 3.097901 / acc: 0.00000\n",
      "step: 15100 / loss: 3.087778 / acc: 0.10000\n",
      "step: 15110 / loss: 3.090266 / acc: 0.06667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 15120 / loss: 3.093601 / acc: 0.03333\n",
      "step: 15130 / loss: 3.096334 / acc: 0.00000\n",
      "step: 15140 / loss: 3.105103 / acc: 0.03333\n",
      "step: 15150 / loss: 3.103517 / acc: 0.03333\n",
      "step: 15160 / loss: 3.097507 / acc: 0.03333\n",
      "step: 15170 / loss: 3.099000 / acc: 0.00000\n",
      "step: 15180 / loss: 3.089931 / acc: 0.06667\n",
      "step: 15190 / loss: 3.105331 / acc: 0.03333\n",
      "step: 15200 / loss: 3.107861 / acc: 0.00000\n",
      "step: 15210 / loss: 3.084070 / acc: 0.03333\n",
      "step: 15220 / loss: 3.101166 / acc: 0.00000\n",
      "step: 15230 / loss: 3.106493 / acc: 0.00000\n",
      "step: 15240 / loss: 3.094161 / acc: 0.00000\n",
      "step: 15250 / loss: 3.098807 / acc: 0.00000\n",
      "step: 15260 / loss: 3.095789 / acc: 0.03333\n",
      "step: 15270 / loss: 3.087719 / acc: 0.00000\n",
      "step: 15280 / loss: 3.097425 / acc: 0.03333\n",
      "step: 15290 / loss: 3.103113 / acc: 0.06667\n",
      "step: 15300 / loss: 3.094218 / acc: 0.03333\n",
      "step: 15310 / loss: 3.082078 / acc: 0.03333\n",
      "step: 15320 / loss: 3.108173 / acc: 0.06667\n",
      "step: 15330 / loss: 3.101888 / acc: 0.03333\n",
      "step: 15340 / loss: 3.085177 / acc: 0.06667\n",
      "step: 15350 / loss: 3.089876 / acc: 0.10000\n",
      "step: 15360 / loss: 3.090686 / acc: 0.03333\n",
      "step: 15370 / loss: 3.089800 / acc: 0.06667\n",
      "step: 15380 / loss: 3.096907 / acc: 0.00000\n",
      "step: 15390 / loss: 3.088084 / acc: 0.03333\n",
      "step: 15400 / loss: 3.092940 / acc: 0.00000\n",
      "step: 15410 / loss: 3.091396 / acc: 0.06667\n",
      "step: 15420 / loss: 3.085475 / acc: 0.06667\n",
      "step: 15430 / loss: 3.079865 / acc: 0.10000\n",
      "step: 15440 / loss: 3.097197 / acc: 0.00000\n",
      "step: 15450 / loss: 3.108468 / acc: 0.06667\n",
      "step: 15460 / loss: 3.084040 / acc: 0.06667\n",
      "step: 15470 / loss: 3.089198 / acc: 0.06667\n",
      "step: 15480 / loss: 3.088031 / acc: 0.03333\n",
      "step: 15490 / loss: 3.094148 / acc: 0.00000\n",
      "step: 15500 / loss: 3.109241 / acc: 0.03333\n",
      "step: 15510 / loss: 3.095207 / acc: 0.06667\n",
      "step: 15520 / loss: 3.089583 / acc: 0.10000\n",
      "step: 15530 / loss: 3.092204 / acc: 0.00000\n",
      "step: 15540 / loss: 3.092327 / acc: 0.03333\n",
      "step: 15550 / loss: 3.096853 / acc: 0.03333\n",
      "step: 15560 / loss: 3.092815 / acc: 0.03333\n",
      "step: 15570 / loss: 3.093275 / acc: 0.10000\n",
      "step: 15580 / loss: 3.096294 / acc: 0.10000\n",
      "step: 15590 / loss: 3.091268 / acc: 0.06667\n",
      "step: 15600 / loss: 3.088342 / acc: 0.06667\n",
      "step: 15610 / loss: 3.091941 / acc: 0.00000\n",
      "step: 15620 / loss: 3.083884 / acc: 0.03333\n",
      "step: 15630 / loss: 3.091848 / acc: 0.03333\n",
      "step: 15640 / loss: 3.087340 / acc: 0.10000\n",
      "step: 15650 / loss: 3.085182 / acc: 0.03333\n",
      "step: 15660 / loss: 3.091160 / acc: 0.06667\n",
      "step: 15670 / loss: 3.091225 / acc: 0.03333\n",
      "step: 15680 / loss: 3.103607 / acc: 0.03333\n",
      "step: 15690 / loss: 3.092154 / acc: 0.06667\n",
      "step: 15700 / loss: 3.092674 / acc: 0.00000\n",
      "step: 15710 / loss: 3.091089 / acc: 0.03333\n",
      "step: 15720 / loss: 3.088531 / acc: 0.03333\n",
      "step: 15730 / loss: 3.081681 / acc: 0.03333\n",
      "step: 15740 / loss: 3.088344 / acc: 0.13333\n",
      "step: 15750 / loss: 3.089993 / acc: 0.06667\n",
      "step: 15760 / loss: 3.091148 / acc: 0.03333\n",
      "step: 15770 / loss: 3.084939 / acc: 0.03333\n",
      "step: 15780 / loss: 3.095790 / acc: 0.03333\n",
      "step: 15790 / loss: 3.084769 / acc: 0.16667\n",
      "step: 15800 / loss: 3.078246 / acc: 0.00000\n",
      "step: 15810 / loss: 3.074831 / acc: 0.10000\n",
      "step: 15820 / loss: 3.098135 / acc: 0.03333\n",
      "step: 15830 / loss: 3.105264 / acc: 0.00000\n",
      "step: 15840 / loss: 3.090916 / acc: 0.03333\n",
      "step: 15850 / loss: 3.099276 / acc: 0.03333\n",
      "step: 15860 / loss: 3.097720 / acc: 0.10000\n",
      "step: 15870 / loss: 3.101223 / acc: 0.03333\n",
      "step: 15880 / loss: 3.115332 / acc: 0.00000\n",
      "step: 15890 / loss: 3.092012 / acc: 0.00000\n",
      "step: 15900 / loss: 3.102150 / acc: 0.03333\n",
      "step: 15910 / loss: 3.089802 / acc: 0.10000\n",
      "step: 15920 / loss: 3.088651 / acc: 0.03333\n",
      "step: 15930 / loss: 3.089592 / acc: 0.10000\n",
      "step: 15940 / loss: 3.105869 / acc: 0.00000\n",
      "step: 15950 / loss: 3.072350 / acc: 0.06667\n",
      "step: 15960 / loss: 3.100060 / acc: 0.03333\n",
      "step: 15970 / loss: 3.090159 / acc: 0.10000\n",
      "step: 15980 / loss: 3.092468 / acc: 0.03333\n",
      "step: 15990 / loss: 3.095657 / acc: 0.03333\n",
      "step: 16000 / loss: 3.096653 / acc: 0.03333\n",
      "step: 16010 / loss: 3.078384 / acc: 0.10000\n",
      "step: 16020 / loss: 3.098880 / acc: 0.00000\n",
      "step: 16030 / loss: 3.098230 / acc: 0.06667\n",
      "step: 16040 / loss: 3.080875 / acc: 0.03333\n",
      "step: 16050 / loss: 3.103623 / acc: 0.00000\n",
      "step: 16060 / loss: 3.083009 / acc: 0.10000\n",
      "step: 16070 / loss: 3.083037 / acc: 0.13333\n",
      "step: 16080 / loss: 3.073590 / acc: 0.06667\n",
      "step: 16090 / loss: 3.082170 / acc: 0.06667\n",
      "step: 16100 / loss: 3.094967 / acc: 0.03333\n",
      "step: 16110 / loss: 3.085164 / acc: 0.13333\n",
      "step: 16120 / loss: 3.079431 / acc: 0.03333\n",
      "step: 16130 / loss: 3.095153 / acc: 0.03333\n",
      "step: 16140 / loss: 3.111924 / acc: 0.03333\n",
      "step: 16150 / loss: 3.092252 / acc: 0.03333\n",
      "step: 16160 / loss: 3.105449 / acc: 0.06667\n",
      "step: 16170 / loss: 3.095637 / acc: 0.03333\n",
      "step: 16180 / loss: 3.084695 / acc: 0.06667\n",
      "step: 16190 / loss: 3.100225 / acc: 0.00000\n",
      "step: 16200 / loss: 3.095045 / acc: 0.03333\n",
      "step: 16210 / loss: 3.090304 / acc: 0.10000\n",
      "step: 16220 / loss: 3.083443 / acc: 0.10000\n",
      "step: 16230 / loss: 3.108042 / acc: 0.03333\n",
      "step: 16240 / loss: 3.093730 / acc: 0.00000\n",
      "step: 16250 / loss: 3.078612 / acc: 0.03333\n",
      "step: 16260 / loss: 3.096307 / acc: 0.03333\n",
      "step: 16270 / loss: 3.089128 / acc: 0.03333\n",
      "step: 16280 / loss: 3.069130 / acc: 0.03333\n",
      "step: 16290 / loss: 3.103874 / acc: 0.00000\n",
      "step: 16300 / loss: 3.102616 / acc: 0.00000\n",
      "step: 16310 / loss: 3.097095 / acc: 0.00000\n",
      "step: 16320 / loss: 3.092870 / acc: 0.06667\n",
      "step: 16330 / loss: 3.092159 / acc: 0.06667\n",
      "step: 16340 / loss: 3.097590 / acc: 0.03333\n",
      "step: 16350 / loss: 3.079133 / acc: 0.03333\n",
      "step: 16360 / loss: 3.102097 / acc: 0.00000\n",
      "step: 16370 / loss: 3.097557 / acc: 0.03333\n",
      "step: 16380 / loss: 3.101619 / acc: 0.03333\n",
      "step: 16390 / loss: 3.088567 / acc: 0.03333\n",
      "step: 16400 / loss: 3.090307 / acc: 0.03333\n",
      "step: 16410 / loss: 3.076680 / acc: 0.03333\n",
      "step: 16420 / loss: 3.100962 / acc: 0.03333\n",
      "step: 16430 / loss: 3.100303 / acc: 0.03333\n",
      "step: 16440 / loss: 3.095242 / acc: 0.03333\n",
      "step: 16450 / loss: 3.093491 / acc: 0.06667\n",
      "step: 16460 / loss: 3.106367 / acc: 0.03333\n",
      "step: 16470 / loss: 3.102813 / acc: 0.10000\n",
      "step: 16480 / loss: 3.099775 / acc: 0.10000\n",
      "step: 16490 / loss: 3.092603 / acc: 0.03333\n",
      "step: 16500 / loss: 3.090640 / acc: 0.06667\n",
      "step: 16510 / loss: 3.098754 / acc: 0.00000\n",
      "step: 16520 / loss: 3.084430 / acc: 0.03333\n",
      "step: 16530 / loss: 3.088511 / acc: 0.00000\n",
      "step: 16540 / loss: 3.096219 / acc: 0.00000\n",
      "step: 16550 / loss: 3.091775 / acc: 0.06667\n",
      "step: 16560 / loss: 3.114575 / acc: 0.03333\n",
      "step: 16570 / loss: 3.077064 / acc: 0.10000\n",
      "step: 16580 / loss: 3.099610 / acc: 0.00000\n",
      "step: 16590 / loss: 3.094353 / acc: 0.00000\n",
      "step: 16600 / loss: 3.096102 / acc: 0.03333\n",
      "step: 16610 / loss: 3.077685 / acc: 0.03333\n",
      "step: 16620 / loss: 3.080036 / acc: 0.13333\n",
      "step: 16630 / loss: 3.075920 / acc: 0.13333\n",
      "step: 16640 / loss: 3.088670 / acc: 0.06667\n",
      "step: 16650 / loss: 3.100548 / acc: 0.03333\n",
      "step: 16660 / loss: 3.083566 / acc: 0.03333\n",
      "step: 16670 / loss: 3.095656 / acc: 0.06667\n",
      "step: 16680 / loss: 3.091644 / acc: 0.06667\n",
      "step: 16690 / loss: 3.099569 / acc: 0.03333\n",
      "step: 16700 / loss: 3.085445 / acc: 0.06667\n",
      "step: 16710 / loss: 3.062108 / acc: 0.16667\n",
      "step: 16720 / loss: 3.098996 / acc: 0.03333\n",
      "step: 16730 / loss: 3.104883 / acc: 0.00000\n",
      "step: 16740 / loss: 3.094065 / acc: 0.03333\n",
      "step: 16750 / loss: 3.085409 / acc: 0.03333\n",
      "step: 16760 / loss: 3.093796 / acc: 0.03333\n",
      "step: 16770 / loss: 3.093169 / acc: 0.03333\n",
      "step: 16780 / loss: 3.112288 / acc: 0.00000\n",
      "step: 16790 / loss: 3.101849 / acc: 0.03333\n",
      "step: 16800 / loss: 3.086456 / acc: 0.03333\n",
      "step: 16810 / loss: 3.106800 / acc: 0.00000\n",
      "step: 16820 / loss: 3.101383 / acc: 0.00000\n",
      "step: 16830 / loss: 3.091512 / acc: 0.06667\n",
      "step: 16840 / loss: 3.093395 / acc: 0.00000\n",
      "step: 16850 / loss: 3.091671 / acc: 0.10000\n",
      "step: 16860 / loss: 3.093404 / acc: 0.00000\n",
      "step: 16870 / loss: 3.088813 / acc: 0.06667\n",
      "step: 16880 / loss: 3.098873 / acc: 0.03333\n",
      "step: 16890 / loss: 3.087117 / acc: 0.06667\n",
      "step: 16900 / loss: 3.083371 / acc: 0.06667\n",
      "step: 16910 / loss: 3.100291 / acc: 0.06667\n",
      "step: 16920 / loss: 3.090683 / acc: 0.03333\n",
      "step: 16930 / loss: 3.094446 / acc: 0.10000\n",
      "step: 16940 / loss: 3.099319 / acc: 0.10000\n",
      "step: 16950 / loss: 3.096578 / acc: 0.03333\n",
      "step: 16960 / loss: 3.086734 / acc: 0.06667\n",
      "step: 16970 / loss: 3.092284 / acc: 0.00000\n",
      "step: 16980 / loss: 3.093938 / acc: 0.06667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 16990 / loss: 3.102360 / acc: 0.03333\n",
      "step: 17000 / loss: 3.086747 / acc: 0.10000\n",
      "step: 17010 / loss: 3.083473 / acc: 0.00000\n",
      "step: 17020 / loss: 3.108591 / acc: 0.10000\n",
      "step: 17030 / loss: 3.085856 / acc: 0.13333\n",
      "step: 17040 / loss: 3.081771 / acc: 0.10000\n",
      "step: 17050 / loss: 3.089461 / acc: 0.13333\n",
      "step: 17060 / loss: 3.079721 / acc: 0.10000\n",
      "step: 17070 / loss: 3.094408 / acc: 0.03333\n",
      "step: 17080 / loss: 3.086109 / acc: 0.00000\n",
      "step: 17090 / loss: 3.089172 / acc: 0.06667\n",
      "step: 17100 / loss: 3.090713 / acc: 0.06667\n",
      "step: 17110 / loss: 3.092295 / acc: 0.06667\n",
      "step: 17120 / loss: 3.088406 / acc: 0.03333\n",
      "step: 17130 / loss: 3.099154 / acc: 0.03333\n",
      "step: 17140 / loss: 3.080337 / acc: 0.03333\n",
      "step: 17150 / loss: 3.094057 / acc: 0.00000\n",
      "step: 17160 / loss: 3.097020 / acc: 0.06667\n",
      "step: 17170 / loss: 3.090298 / acc: 0.00000\n",
      "step: 17180 / loss: 3.105983 / acc: 0.00000\n",
      "step: 17190 / loss: 3.090905 / acc: 0.00000\n",
      "step: 17200 / loss: 3.088511 / acc: 0.10000\n",
      "step: 17210 / loss: 3.102051 / acc: 0.00000\n",
      "step: 17220 / loss: 3.096184 / acc: 0.06667\n",
      "step: 17230 / loss: 3.098624 / acc: 0.06667\n",
      "step: 17240 / loss: 3.080141 / acc: 0.13333\n",
      "step: 17250 / loss: 3.083002 / acc: 0.03333\n",
      "step: 17260 / loss: 3.108763 / acc: 0.10000\n",
      "step: 17270 / loss: 3.096267 / acc: 0.06667\n",
      "step: 17280 / loss: 3.080277 / acc: 0.00000\n",
      "step: 17290 / loss: 3.089133 / acc: 0.03333\n",
      "step: 17300 / loss: 3.081324 / acc: 0.06667\n",
      "step: 17310 / loss: 3.104872 / acc: 0.00000\n",
      "step: 17320 / loss: 3.097518 / acc: 0.03333\n",
      "step: 17330 / loss: 3.090946 / acc: 0.06667\n",
      "step: 17340 / loss: 3.077882 / acc: 0.06667\n",
      "step: 17350 / loss: 3.092720 / acc: 0.06667\n",
      "step: 17360 / loss: 3.101784 / acc: 0.03333\n",
      "step: 17370 / loss: 3.094481 / acc: 0.03333\n",
      "step: 17380 / loss: 3.094703 / acc: 0.06667\n",
      "step: 17390 / loss: 3.092880 / acc: 0.03333\n",
      "step: 17400 / loss: 3.119268 / acc: 0.00000\n",
      "step: 17410 / loss: 3.077926 / acc: 0.03333\n",
      "step: 17420 / loss: 3.093812 / acc: 0.03333\n",
      "step: 17430 / loss: 3.114928 / acc: 0.00000\n",
      "step: 17440 / loss: 3.093324 / acc: 0.10000\n",
      "step: 17450 / loss: 3.097364 / acc: 0.03333\n",
      "step: 17460 / loss: 3.102502 / acc: 0.03333\n",
      "step: 17470 / loss: 3.083339 / acc: 0.10000\n",
      "step: 17480 / loss: 3.084943 / acc: 0.10000\n",
      "step: 17490 / loss: 3.094956 / acc: 0.03333\n",
      "step: 17500 / loss: 3.084973 / acc: 0.13333\n",
      "step: 17510 / loss: 3.098588 / acc: 0.03333\n",
      "step: 17520 / loss: 3.090345 / acc: 0.00000\n",
      "step: 17530 / loss: 3.092147 / acc: 0.00000\n",
      "step: 17540 / loss: 3.100342 / acc: 0.06667\n",
      "step: 17550 / loss: 3.097519 / acc: 0.06667\n",
      "step: 17560 / loss: 3.095169 / acc: 0.03333\n",
      "step: 17570 / loss: 3.095053 / acc: 0.00000\n",
      "step: 17580 / loss: 3.112698 / acc: 0.00000\n",
      "step: 17590 / loss: 3.095121 / acc: 0.03333\n",
      "step: 17600 / loss: 3.101987 / acc: 0.06667\n",
      "step: 17610 / loss: 3.114638 / acc: 0.03333\n",
      "step: 17620 / loss: 3.100219 / acc: 0.03333\n",
      "step: 17630 / loss: 3.082804 / acc: 0.03333\n",
      "step: 17640 / loss: 3.100859 / acc: 0.03333\n",
      "step: 17650 / loss: 3.094602 / acc: 0.00000\n",
      "step: 17660 / loss: 3.099228 / acc: 0.06667\n",
      "step: 17670 / loss: 3.092778 / acc: 0.03333\n",
      "step: 17680 / loss: 3.089065 / acc: 0.13333\n",
      "step: 17690 / loss: 3.087614 / acc: 0.10000\n",
      "step: 17700 / loss: 3.108533 / acc: 0.03333\n",
      "step: 17710 / loss: 3.074674 / acc: 0.13333\n",
      "step: 17720 / loss: 3.094900 / acc: 0.00000\n",
      "step: 17730 / loss: 3.096685 / acc: 0.10000\n",
      "step: 17740 / loss: 3.083544 / acc: 0.03333\n",
      "step: 17750 / loss: 3.079810 / acc: 0.00000\n",
      "step: 17760 / loss: 3.095827 / acc: 0.00000\n",
      "step: 17770 / loss: 3.092427 / acc: 0.06667\n",
      "step: 17780 / loss: 3.084346 / acc: 0.06667\n",
      "step: 17790 / loss: 3.094345 / acc: 0.00000\n",
      "step: 17800 / loss: 3.091928 / acc: 0.03333\n",
      "step: 17810 / loss: 3.084924 / acc: 0.03333\n",
      "step: 17820 / loss: 3.080387 / acc: 0.06667\n",
      "step: 17830 / loss: 3.103916 / acc: 0.06667\n",
      "step: 17840 / loss: 3.097484 / acc: 0.03333\n",
      "step: 17850 / loss: 3.089333 / acc: 0.03333\n",
      "step: 17860 / loss: 3.105883 / acc: 0.00000\n",
      "step: 17870 / loss: 3.085057 / acc: 0.03333\n",
      "step: 17880 / loss: 3.083631 / acc: 0.06667\n",
      "step: 17890 / loss: 3.079919 / acc: 0.00000\n",
      "step: 17900 / loss: 3.096733 / acc: 0.03333\n",
      "step: 17910 / loss: 3.082082 / acc: 0.03333\n",
      "step: 17920 / loss: 3.100505 / acc: 0.03333\n",
      "step: 17930 / loss: 3.097043 / acc: 0.13333\n",
      "step: 17940 / loss: 3.080794 / acc: 0.03333\n",
      "step: 17950 / loss: 3.104663 / acc: 0.00000\n",
      "step: 17960 / loss: 3.083749 / acc: 0.03333\n",
      "step: 17970 / loss: 3.099861 / acc: 0.03333\n",
      "step: 17980 / loss: 3.100188 / acc: 0.00000\n",
      "step: 17990 / loss: 3.090986 / acc: 0.06667\n",
      "step: 18000 / loss: 3.098168 / acc: 0.03333\n",
      "step: 18010 / loss: 3.090611 / acc: 0.03333\n",
      "step: 18020 / loss: 3.104104 / acc: 0.10000\n",
      "step: 18030 / loss: 3.081227 / acc: 0.06667\n",
      "step: 18040 / loss: 3.103190 / acc: 0.03333\n",
      "step: 18050 / loss: 3.104395 / acc: 0.06667\n",
      "step: 18060 / loss: 3.098762 / acc: 0.00000\n",
      "step: 18070 / loss: 3.091050 / acc: 0.03333\n",
      "step: 18080 / loss: 3.084764 / acc: 0.03333\n",
      "step: 18090 / loss: 3.084687 / acc: 0.13333\n",
      "step: 18100 / loss: 3.104655 / acc: 0.10000\n",
      "step: 18110 / loss: 3.093638 / acc: 0.06667\n",
      "step: 18120 / loss: 3.092026 / acc: 0.00000\n",
      "step: 18130 / loss: 3.095087 / acc: 0.06667\n",
      "step: 18140 / loss: 3.096437 / acc: 0.03333\n",
      "step: 18150 / loss: 3.085921 / acc: 0.06667\n",
      "step: 18160 / loss: 3.097502 / acc: 0.06667\n",
      "step: 18170 / loss: 3.086538 / acc: 0.03333\n",
      "step: 18180 / loss: 3.101634 / acc: 0.03333\n",
      "step: 18190 / loss: 3.090303 / acc: 0.00000\n",
      "step: 18200 / loss: 3.101107 / acc: 0.00000\n",
      "step: 18210 / loss: 3.095593 / acc: 0.00000\n",
      "step: 18220 / loss: 3.094364 / acc: 0.06667\n",
      "step: 18230 / loss: 3.087351 / acc: 0.03333\n",
      "step: 18240 / loss: 3.092445 / acc: 0.03333\n",
      "step: 18250 / loss: 3.107409 / acc: 0.03333\n",
      "step: 18260 / loss: 3.081007 / acc: 0.06667\n",
      "step: 18270 / loss: 3.092397 / acc: 0.03333\n",
      "step: 18280 / loss: 3.093719 / acc: 0.00000\n",
      "step: 18290 / loss: 3.086220 / acc: 0.03333\n",
      "step: 18300 / loss: 3.095002 / acc: 0.03333\n",
      "step: 18310 / loss: 3.088356 / acc: 0.03333\n",
      "step: 18320 / loss: 3.089381 / acc: 0.00000\n",
      "step: 18330 / loss: 3.083144 / acc: 0.03333\n",
      "step: 18340 / loss: 3.093103 / acc: 0.10000\n",
      "step: 18350 / loss: 3.090869 / acc: 0.00000\n",
      "step: 18360 / loss: 3.088068 / acc: 0.00000\n",
      "step: 18370 / loss: 3.105105 / acc: 0.00000\n",
      "step: 18380 / loss: 3.108700 / acc: 0.00000\n",
      "step: 18390 / loss: 3.096497 / acc: 0.00000\n",
      "step: 18400 / loss: 3.100591 / acc: 0.06667\n",
      "step: 18410 / loss: 3.089859 / acc: 0.06667\n",
      "step: 18420 / loss: 3.095403 / acc: 0.00000\n",
      "step: 18430 / loss: 3.106573 / acc: 0.03333\n",
      "step: 18440 / loss: 3.084527 / acc: 0.03333\n",
      "step: 18450 / loss: 3.076549 / acc: 0.03333\n",
      "step: 18460 / loss: 3.099144 / acc: 0.03333\n",
      "step: 18470 / loss: 3.087034 / acc: 0.03333\n",
      "step: 18480 / loss: 3.084883 / acc: 0.10000\n",
      "step: 18490 / loss: 3.104916 / acc: 0.03333\n",
      "step: 18500 / loss: 3.088074 / acc: 0.03333\n",
      "step: 18510 / loss: 3.082385 / acc: 0.06667\n",
      "step: 18520 / loss: 3.099891 / acc: 0.00000\n",
      "step: 18530 / loss: 3.101326 / acc: 0.00000\n",
      "step: 18540 / loss: 3.095737 / acc: 0.06667\n",
      "step: 18550 / loss: 3.104453 / acc: 0.00000\n",
      "step: 18560 / loss: 3.093125 / acc: 0.03333\n",
      "step: 18570 / loss: 3.096878 / acc: 0.03333\n",
      "step: 18580 / loss: 3.092096 / acc: 0.03333\n",
      "step: 18590 / loss: 3.112069 / acc: 0.00000\n",
      "step: 18600 / loss: 3.098044 / acc: 0.00000\n",
      "step: 18610 / loss: 3.094810 / acc: 0.00000\n",
      "step: 18620 / loss: 3.090799 / acc: 0.06667\n",
      "step: 18630 / loss: 3.096939 / acc: 0.03333\n",
      "step: 18640 / loss: 3.084898 / acc: 0.06667\n",
      "step: 18650 / loss: 3.099753 / acc: 0.03333\n",
      "step: 18660 / loss: 3.093040 / acc: 0.00000\n",
      "step: 18670 / loss: 3.102267 / acc: 0.00000\n",
      "step: 18680 / loss: 3.089948 / acc: 0.03333\n",
      "step: 18690 / loss: 3.090677 / acc: 0.06667\n",
      "step: 18700 / loss: 3.087960 / acc: 0.06667\n",
      "step: 18710 / loss: 3.100998 / acc: 0.03333\n",
      "step: 18720 / loss: 3.108463 / acc: 0.03333\n",
      "step: 18730 / loss: 3.082317 / acc: 0.10000\n",
      "step: 18740 / loss: 3.103195 / acc: 0.06667\n",
      "step: 18750 / loss: 3.089664 / acc: 0.06667\n",
      "step: 18760 / loss: 3.088936 / acc: 0.06667\n",
      "step: 18770 / loss: 3.095545 / acc: 0.03333\n",
      "step: 18780 / loss: 3.090286 / acc: 0.00000\n",
      "step: 18790 / loss: 3.088284 / acc: 0.03333\n",
      "step: 18800 / loss: 3.096611 / acc: 0.00000\n",
      "step: 18810 / loss: 3.105129 / acc: 0.06667\n",
      "step: 18820 / loss: 3.092277 / acc: 0.03333\n",
      "step: 18830 / loss: 3.105974 / acc: 0.03333\n",
      "step: 18840 / loss: 3.083327 / acc: 0.03333\n",
      "step: 18850 / loss: 3.097674 / acc: 0.03333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 18860 / loss: 3.095074 / acc: 0.03333\n",
      "step: 18870 / loss: 3.098558 / acc: 0.00000\n",
      "step: 18880 / loss: 3.086182 / acc: 0.06667\n",
      "step: 18890 / loss: 3.092979 / acc: 0.00000\n",
      "step: 18900 / loss: 3.092567 / acc: 0.03333\n",
      "step: 18910 / loss: 3.100576 / acc: 0.00000\n",
      "step: 18920 / loss: 3.086110 / acc: 0.03333\n",
      "step: 18930 / loss: 3.107366 / acc: 0.00000\n",
      "step: 18940 / loss: 3.091305 / acc: 0.06667\n",
      "step: 18950 / loss: 3.111449 / acc: 0.00000\n",
      "step: 18960 / loss: 3.096222 / acc: 0.03333\n",
      "step: 18970 / loss: 3.083896 / acc: 0.06667\n",
      "step: 18980 / loss: 3.088231 / acc: 0.03333\n",
      "step: 18990 / loss: 3.092388 / acc: 0.00000\n",
      "step: 19000 / loss: 3.087056 / acc: 0.13333\n",
      "step: 19010 / loss: 3.103899 / acc: 0.00000\n",
      "step: 19020 / loss: 3.097135 / acc: 0.03333\n",
      "step: 19030 / loss: 3.089196 / acc: 0.03333\n",
      "step: 19040 / loss: 3.088832 / acc: 0.06667\n",
      "step: 19050 / loss: 3.095633 / acc: 0.06667\n",
      "step: 19060 / loss: 3.091557 / acc: 0.00000\n",
      "step: 19070 / loss: 3.099100 / acc: 0.13333\n",
      "step: 19080 / loss: 3.082595 / acc: 0.03333\n",
      "step: 19090 / loss: 3.087587 / acc: 0.06667\n",
      "step: 19100 / loss: 3.104709 / acc: 0.03333\n",
      "step: 19110 / loss: 3.078667 / acc: 0.10000\n",
      "step: 19120 / loss: 3.101264 / acc: 0.03333\n",
      "step: 19130 / loss: 3.091936 / acc: 0.00000\n",
      "step: 19140 / loss: 3.095412 / acc: 0.00000\n",
      "step: 19150 / loss: 3.093209 / acc: 0.10000\n",
      "step: 19160 / loss: 3.090265 / acc: 0.13333\n",
      "step: 19170 / loss: 3.086009 / acc: 0.03333\n",
      "step: 19180 / loss: 3.088413 / acc: 0.03333\n",
      "step: 19190 / loss: 3.104124 / acc: 0.03333\n",
      "step: 19200 / loss: 3.109845 / acc: 0.03333\n",
      "step: 19210 / loss: 3.101309 / acc: 0.10000\n",
      "step: 19220 / loss: 3.094976 / acc: 0.00000\n",
      "step: 19230 / loss: 3.094491 / acc: 0.06667\n",
      "step: 19240 / loss: 3.100922 / acc: 0.00000\n",
      "step: 19250 / loss: 3.102212 / acc: 0.00000\n",
      "step: 19260 / loss: 3.092739 / acc: 0.03333\n",
      "step: 19270 / loss: 3.082723 / acc: 0.00000\n",
      "step: 19280 / loss: 3.091946 / acc: 0.06667\n",
      "step: 19290 / loss: 3.096470 / acc: 0.00000\n",
      "step: 19300 / loss: 3.096001 / acc: 0.03333\n",
      "step: 19310 / loss: 3.100221 / acc: 0.06667\n",
      "step: 19320 / loss: 3.087933 / acc: 0.10000\n",
      "step: 19330 / loss: 3.086234 / acc: 0.00000\n",
      "step: 19340 / loss: 3.096354 / acc: 0.00000\n",
      "step: 19350 / loss: 3.088072 / acc: 0.03333\n",
      "step: 19360 / loss: 3.079425 / acc: 0.03333\n",
      "step: 19370 / loss: 3.095867 / acc: 0.03333\n",
      "step: 19380 / loss: 3.093064 / acc: 0.06667\n",
      "step: 19390 / loss: 3.093120 / acc: 0.00000\n",
      "step: 19400 / loss: 3.087789 / acc: 0.03333\n",
      "step: 19410 / loss: 3.091718 / acc: 0.06667\n",
      "step: 19420 / loss: 3.091094 / acc: 0.00000\n",
      "step: 19430 / loss: 3.091634 / acc: 0.03333\n",
      "step: 19440 / loss: 3.092841 / acc: 0.06667\n",
      "step: 19450 / loss: 3.087971 / acc: 0.00000\n",
      "step: 19460 / loss: 3.090612 / acc: 0.06667\n",
      "step: 19470 / loss: 3.114721 / acc: 0.00000\n",
      "step: 19480 / loss: 3.087869 / acc: 0.03333\n",
      "step: 19490 / loss: 3.097591 / acc: 0.03333\n",
      "step: 19500 / loss: 3.093203 / acc: 0.00000\n",
      "step: 19510 / loss: 3.095777 / acc: 0.00000\n",
      "step: 19520 / loss: 3.083688 / acc: 0.03333\n",
      "step: 19530 / loss: 3.080497 / acc: 0.16667\n",
      "step: 19540 / loss: 3.086707 / acc: 0.03333\n",
      "step: 19550 / loss: 3.096755 / acc: 0.00000\n",
      "step: 19560 / loss: 3.110285 / acc: 0.00000\n",
      "step: 19570 / loss: 3.088842 / acc: 0.03333\n",
      "step: 19580 / loss: 3.074341 / acc: 0.06667\n",
      "step: 19590 / loss: 3.093087 / acc: 0.03333\n",
      "step: 19600 / loss: 3.096390 / acc: 0.06667\n",
      "step: 19610 / loss: 3.089435 / acc: 0.03333\n",
      "step: 19620 / loss: 3.091024 / acc: 0.03333\n",
      "step: 19630 / loss: 3.094399 / acc: 0.03333\n",
      "step: 19640 / loss: 3.092265 / acc: 0.03333\n",
      "step: 19650 / loss: 3.098948 / acc: 0.03333\n",
      "step: 19660 / loss: 3.083442 / acc: 0.13333\n",
      "step: 19670 / loss: 3.081619 / acc: 0.10000\n",
      "step: 19680 / loss: 3.076994 / acc: 0.03333\n",
      "step: 19690 / loss: 3.101505 / acc: 0.03333\n",
      "step: 19700 / loss: 3.108506 / acc: 0.00000\n",
      "step: 19710 / loss: 3.088014 / acc: 0.03333\n",
      "step: 19720 / loss: 3.093392 / acc: 0.06667\n",
      "step: 19730 / loss: 3.095137 / acc: 0.00000\n",
      "step: 19740 / loss: 3.096749 / acc: 0.06667\n",
      "step: 19750 / loss: 3.101825 / acc: 0.03333\n",
      "step: 19760 / loss: 3.106174 / acc: 0.00000\n",
      "step: 19770 / loss: 3.091298 / acc: 0.00000\n",
      "step: 19780 / loss: 3.101104 / acc: 0.00000\n",
      "step: 19790 / loss: 3.100524 / acc: 0.00000\n",
      "step: 19800 / loss: 3.089916 / acc: 0.00000\n",
      "step: 19810 / loss: 3.089326 / acc: 0.10000\n",
      "step: 19820 / loss: 3.095025 / acc: 0.06667\n",
      "step: 19830 / loss: 3.103570 / acc: 0.03333\n",
      "step: 19840 / loss: 3.097216 / acc: 0.03333\n",
      "step: 19850 / loss: 3.076429 / acc: 0.10000\n",
      "step: 19860 / loss: 3.097233 / acc: 0.00000\n",
      "step: 19870 / loss: 3.085624 / acc: 0.10000\n",
      "step: 19880 / loss: 3.094824 / acc: 0.06667\n",
      "step: 19890 / loss: 3.100806 / acc: 0.03333\n",
      "step: 19900 / loss: 3.087890 / acc: 0.03333\n",
      "step: 19910 / loss: 3.095897 / acc: 0.00000\n",
      "step: 19920 / loss: 3.088696 / acc: 0.06667\n",
      "step: 19930 / loss: 3.091453 / acc: 0.03333\n",
      "step: 19940 / loss: 3.078411 / acc: 0.03333\n",
      "step: 19950 / loss: 3.092812 / acc: 0.10000\n",
      "step: 19960 / loss: 3.105679 / acc: 0.03333\n",
      "step: 19970 / loss: 3.093185 / acc: 0.10000\n",
      "step: 19980 / loss: 3.100190 / acc: 0.03333\n",
      "step: 19990 / loss: 3.099780 / acc: 0.00000\n",
      "step: 20000 / loss: 3.130881 / acc: 0.00000\n",
      "step: 20010 / loss: 3.081935 / acc: 0.10000\n",
      "step: 20020 / loss: 3.088300 / acc: 0.00000\n",
      "step: 20030 / loss: 3.096042 / acc: 0.10000\n",
      "step: 20040 / loss: 3.095776 / acc: 0.03333\n",
      "step: 20050 / loss: 3.097488 / acc: 0.00000\n",
      "step: 20060 / loss: 3.092090 / acc: 0.03333\n",
      "step: 20070 / loss: 3.092665 / acc: 0.03333\n",
      "step: 20080 / loss: 3.079075 / acc: 0.06667\n",
      "step: 20090 / loss: 3.090752 / acc: 0.03333\n",
      "step: 20100 / loss: 3.093048 / acc: 0.10000\n",
      "step: 20110 / loss: 3.086326 / acc: 0.03333\n",
      "step: 20120 / loss: 3.096161 / acc: 0.03333\n",
      "step: 20130 / loss: 3.092806 / acc: 0.03333\n",
      "step: 20140 / loss: 3.081179 / acc: 0.10000\n",
      "step: 20150 / loss: 3.090002 / acc: 0.06667\n",
      "step: 20160 / loss: 3.063815 / acc: 0.10000\n",
      "step: 20170 / loss: 3.076957 / acc: 0.13333\n",
      "step: 20180 / loss: 3.100806 / acc: 0.03333\n",
      "step: 20190 / loss: 3.096115 / acc: 0.00000\n",
      "step: 20200 / loss: 3.085041 / acc: 0.06667\n",
      "step: 20210 / loss: 3.087879 / acc: 0.03333\n",
      "step: 20220 / loss: 3.088889 / acc: 0.03333\n",
      "step: 20230 / loss: 3.096990 / acc: 0.00000\n",
      "step: 20240 / loss: 3.097954 / acc: 0.03333\n",
      "step: 20250 / loss: 3.080520 / acc: 0.06667\n",
      "step: 20260 / loss: 3.086708 / acc: 0.06667\n",
      "step: 20270 / loss: 3.086492 / acc: 0.10000\n",
      "step: 20280 / loss: 3.094570 / acc: 0.00000\n",
      "step: 20290 / loss: 3.095024 / acc: 0.00000\n",
      "step: 20300 / loss: 3.098157 / acc: 0.03333\n",
      "step: 20310 / loss: 3.091911 / acc: 0.00000\n",
      "step: 20320 / loss: 3.096500 / acc: 0.06667\n",
      "step: 20330 / loss: 3.095399 / acc: 0.03333\n",
      "step: 20340 / loss: 3.100083 / acc: 0.06667\n",
      "step: 20350 / loss: 3.084532 / acc: 0.06667\n",
      "step: 20360 / loss: 3.093275 / acc: 0.06667\n",
      "step: 20370 / loss: 3.095806 / acc: 0.03333\n",
      "step: 20380 / loss: 3.086559 / acc: 0.00000\n",
      "step: 20390 / loss: 3.098889 / acc: 0.00000\n",
      "step: 20400 / loss: 3.092340 / acc: 0.03333\n",
      "step: 20410 / loss: 3.092835 / acc: 0.10000\n",
      "step: 20420 / loss: 3.086208 / acc: 0.06667\n",
      "step: 20430 / loss: 3.089197 / acc: 0.10000\n",
      "step: 20440 / loss: 3.095627 / acc: 0.00000\n",
      "step: 20450 / loss: 3.088483 / acc: 0.06667\n",
      "step: 20460 / loss: 3.109817 / acc: 0.03333\n",
      "step: 20470 / loss: 3.100366 / acc: 0.06667\n",
      "step: 20480 / loss: 3.095309 / acc: 0.03333\n",
      "step: 20490 / loss: 3.102501 / acc: 0.03333\n",
      "step: 20500 / loss: 3.096350 / acc: 0.03333\n",
      "step: 20510 / loss: 3.088439 / acc: 0.03333\n",
      "step: 20520 / loss: 3.099046 / acc: 0.06667\n",
      "step: 20530 / loss: 3.089089 / acc: 0.06667\n",
      "step: 20540 / loss: 3.099500 / acc: 0.06667\n",
      "step: 20550 / loss: 3.106584 / acc: 0.03333\n",
      "step: 20560 / loss: 3.099614 / acc: 0.06667\n",
      "step: 20570 / loss: 3.108108 / acc: 0.00000\n",
      "step: 20580 / loss: 3.090047 / acc: 0.03333\n",
      "step: 20590 / loss: 3.094346 / acc: 0.03333\n",
      "step: 20600 / loss: 3.096164 / acc: 0.06667\n",
      "step: 20610 / loss: 3.096406 / acc: 0.00000\n",
      "step: 20620 / loss: 3.085735 / acc: 0.06667\n",
      "step: 20630 / loss: 3.103312 / acc: 0.03333\n",
      "step: 20640 / loss: 3.081522 / acc: 0.10000\n",
      "step: 20650 / loss: 3.097795 / acc: 0.00000\n",
      "step: 20660 / loss: 3.089799 / acc: 0.03333\n",
      "step: 20670 / loss: 3.093230 / acc: 0.06667\n",
      "step: 20680 / loss: 3.095687 / acc: 0.03333\n",
      "step: 20690 / loss: 3.086801 / acc: 0.10000\n",
      "step: 20700 / loss: 3.091404 / acc: 0.06667\n",
      "step: 20710 / loss: 3.076894 / acc: 0.10000\n",
      "step: 20720 / loss: 3.090713 / acc: 0.03333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 20730 / loss: 3.096967 / acc: 0.03333\n",
      "step: 20740 / loss: 3.096504 / acc: 0.03333\n",
      "step: 20750 / loss: 3.103705 / acc: 0.00000\n",
      "step: 20760 / loss: 3.092510 / acc: 0.06667\n",
      "step: 20770 / loss: 3.091791 / acc: 0.06667\n",
      "step: 20780 / loss: 3.090487 / acc: 0.06667\n",
      "step: 20790 / loss: 3.095587 / acc: 0.00000\n",
      "step: 20800 / loss: 3.087951 / acc: 0.03333\n",
      "step: 20810 / loss: 3.093127 / acc: 0.10000\n",
      "step: 20820 / loss: 3.106276 / acc: 0.00000\n",
      "step: 20830 / loss: 3.075327 / acc: 0.10000\n",
      "step: 20840 / loss: 3.080910 / acc: 0.10000\n",
      "step: 20850 / loss: 3.083005 / acc: 0.03333\n",
      "step: 20860 / loss: 3.092277 / acc: 0.00000\n",
      "step: 20870 / loss: 3.096391 / acc: 0.03333\n",
      "step: 20880 / loss: 3.096567 / acc: 0.03333\n",
      "step: 20890 / loss: 3.096254 / acc: 0.06667\n",
      "step: 20900 / loss: 3.102368 / acc: 0.00000\n",
      "step: 20910 / loss: 3.092458 / acc: 0.03333\n",
      "step: 20920 / loss: 3.087882 / acc: 0.13333\n",
      "step: 20930 / loss: 3.089479 / acc: 0.03333\n",
      "step: 20940 / loss: 3.100032 / acc: 0.00000\n",
      "step: 20950 / loss: 3.103028 / acc: 0.03333\n",
      "step: 20960 / loss: 3.091930 / acc: 0.06667\n",
      "step: 20970 / loss: 3.087505 / acc: 0.06667\n",
      "step: 20980 / loss: 3.093976 / acc: 0.00000\n",
      "step: 20990 / loss: 3.087314 / acc: 0.10000\n",
      "step: 21000 / loss: 3.102859 / acc: 0.00000\n",
      "step: 21010 / loss: 3.094961 / acc: 0.03333\n",
      "step: 21020 / loss: 3.094317 / acc: 0.06667\n",
      "step: 21030 / loss: 3.087194 / acc: 0.06667\n",
      "step: 21040 / loss: 3.098580 / acc: 0.03333\n",
      "step: 21050 / loss: 3.088080 / acc: 0.00000\n",
      "step: 21060 / loss: 3.101938 / acc: 0.06667\n",
      "step: 21070 / loss: 3.097553 / acc: 0.03333\n",
      "step: 21080 / loss: 3.096555 / acc: 0.06667\n",
      "step: 21090 / loss: 3.089070 / acc: 0.06667\n",
      "step: 21100 / loss: 3.105732 / acc: 0.03333\n",
      "step: 21110 / loss: 3.094265 / acc: 0.03333\n",
      "step: 21120 / loss: 3.089923 / acc: 0.03333\n",
      "step: 21130 / loss: 3.086529 / acc: 0.03333\n",
      "step: 21140 / loss: 3.090915 / acc: 0.00000\n",
      "step: 21150 / loss: 3.096237 / acc: 0.03333\n",
      "step: 21160 / loss: 3.101871 / acc: 0.00000\n",
      "step: 21170 / loss: 3.093214 / acc: 0.06667\n",
      "step: 21180 / loss: 3.089631 / acc: 0.06667\n",
      "step: 21190 / loss: 3.086006 / acc: 0.10000\n",
      "step: 21200 / loss: 3.095981 / acc: 0.03333\n",
      "step: 21210 / loss: 3.085335 / acc: 0.03333\n",
      "step: 21220 / loss: 3.099922 / acc: 0.00000\n",
      "step: 21230 / loss: 3.108708 / acc: 0.00000\n",
      "step: 21240 / loss: 3.090470 / acc: 0.06667\n",
      "step: 21250 / loss: 3.091529 / acc: 0.06667\n",
      "step: 21260 / loss: 3.091202 / acc: 0.00000\n",
      "step: 21270 / loss: 3.092594 / acc: 0.03333\n",
      "step: 21280 / loss: 3.088503 / acc: 0.06667\n",
      "step: 21290 / loss: 3.096097 / acc: 0.00000\n",
      "step: 21300 / loss: 3.084046 / acc: 0.06667\n",
      "step: 21310 / loss: 3.099053 / acc: 0.03333\n",
      "step: 21320 / loss: 3.103518 / acc: 0.03333\n",
      "step: 21330 / loss: 3.083756 / acc: 0.06667\n",
      "step: 21340 / loss: 3.086700 / acc: 0.00000\n",
      "step: 21350 / loss: 3.107886 / acc: 0.00000\n",
      "step: 21360 / loss: 3.101364 / acc: 0.10000\n",
      "step: 21370 / loss: 3.105493 / acc: 0.03333\n",
      "step: 21380 / loss: 3.088467 / acc: 0.06667\n",
      "step: 21390 / loss: 3.084231 / acc: 0.13333\n",
      "step: 21400 / loss: 3.096916 / acc: 0.00000\n",
      "step: 21410 / loss: 3.096975 / acc: 0.06667\n",
      "step: 21420 / loss: 3.092401 / acc: 0.06667\n",
      "step: 21430 / loss: 3.101336 / acc: 0.03333\n",
      "step: 21440 / loss: 3.101329 / acc: 0.06667\n",
      "step: 21450 / loss: 3.085863 / acc: 0.03333\n",
      "step: 21460 / loss: 3.107098 / acc: 0.03333\n",
      "step: 21470 / loss: 3.090674 / acc: 0.06667\n",
      "step: 21480 / loss: 3.087533 / acc: 0.03333\n",
      "step: 21490 / loss: 3.095182 / acc: 0.06667\n",
      "step: 21500 / loss: 3.089821 / acc: 0.00000\n",
      "step: 21510 / loss: 3.088747 / acc: 0.06667\n",
      "step: 21520 / loss: 3.099740 / acc: 0.03333\n",
      "step: 21530 / loss: 3.105900 / acc: 0.03333\n",
      "step: 21540 / loss: 3.089403 / acc: 0.03333\n",
      "step: 21550 / loss: 3.089173 / acc: 0.06667\n",
      "step: 21560 / loss: 3.089522 / acc: 0.00000\n",
      "step: 21570 / loss: 3.090465 / acc: 0.06667\n",
      "step: 21580 / loss: 3.096165 / acc: 0.00000\n",
      "step: 21590 / loss: 3.098400 / acc: 0.03333\n",
      "step: 21600 / loss: 3.090024 / acc: 0.10000\n",
      "step: 21610 / loss: 3.089821 / acc: 0.00000\n",
      "step: 21620 / loss: 3.091722 / acc: 0.00000\n",
      "step: 21630 / loss: 3.086898 / acc: 0.06667\n",
      "step: 21640 / loss: 3.095966 / acc: 0.00000\n",
      "step: 21650 / loss: 3.090949 / acc: 0.03333\n",
      "step: 21660 / loss: 3.089041 / acc: 0.03333\n",
      "step: 21670 / loss: 3.093903 / acc: 0.03333\n",
      "step: 21680 / loss: 3.095591 / acc: 0.00000\n",
      "step: 21690 / loss: 3.101074 / acc: 0.00000\n",
      "step: 21700 / loss: 3.097894 / acc: 0.03333\n",
      "step: 21710 / loss: 3.096120 / acc: 0.03333\n",
      "step: 21720 / loss: 3.080636 / acc: 0.10000\n",
      "step: 21730 / loss: 3.094741 / acc: 0.13333\n",
      "step: 21740 / loss: 3.086066 / acc: 0.06667\n",
      "step: 21750 / loss: 3.098901 / acc: 0.03333\n",
      "step: 21760 / loss: 3.084262 / acc: 0.06667\n",
      "step: 21770 / loss: 3.098983 / acc: 0.03333\n",
      "step: 21780 / loss: 3.101367 / acc: 0.03333\n",
      "step: 21790 / loss: 3.104531 / acc: 0.00000\n",
      "step: 21800 / loss: 3.085422 / acc: 0.10000\n",
      "step: 21810 / loss: 3.090494 / acc: 0.00000\n",
      "step: 21820 / loss: 3.107311 / acc: 0.00000\n",
      "step: 21830 / loss: 3.089272 / acc: 0.00000\n",
      "step: 21840 / loss: 3.097424 / acc: 0.03333\n",
      "step: 21850 / loss: 3.096730 / acc: 0.00000\n",
      "step: 21860 / loss: 3.091087 / acc: 0.00000\n",
      "step: 21870 / loss: 3.095560 / acc: 0.06667\n",
      "step: 21880 / loss: 3.090156 / acc: 0.10000\n",
      "step: 21890 / loss: 3.100248 / acc: 0.00000\n",
      "step: 21900 / loss: 3.086333 / acc: 0.06667\n",
      "step: 21910 / loss: 3.077204 / acc: 0.03333\n",
      "step: 21920 / loss: 3.087033 / acc: 0.03333\n",
      "step: 21930 / loss: 3.095789 / acc: 0.06667\n",
      "step: 21940 / loss: 3.087050 / acc: 0.16667\n",
      "step: 21950 / loss: 3.118651 / acc: 0.00000\n",
      "step: 21960 / loss: 3.083884 / acc: 0.03333\n",
      "step: 21970 / loss: 3.087141 / acc: 0.03333\n",
      "step: 21980 / loss: 3.090132 / acc: 0.10000\n",
      "step: 21990 / loss: 3.092070 / acc: 0.10000\n",
      "step: 22000 / loss: 3.085912 / acc: 0.03333\n",
      "step: 22010 / loss: 3.077381 / acc: 0.03333\n",
      "step: 22020 / loss: 3.103977 / acc: 0.06667\n",
      "step: 22030 / loss: 3.087976 / acc: 0.06667\n",
      "step: 22040 / loss: 3.096836 / acc: 0.00000\n",
      "step: 22050 / loss: 3.075146 / acc: 0.10000\n",
      "step: 22060 / loss: 3.090279 / acc: 0.00000\n",
      "step: 22070 / loss: 3.096460 / acc: 0.03333\n",
      "step: 22080 / loss: 3.099244 / acc: 0.00000\n",
      "step: 22090 / loss: 3.107481 / acc: 0.06667\n",
      "step: 22100 / loss: 3.087663 / acc: 0.06667\n",
      "step: 22110 / loss: 3.086761 / acc: 0.00000\n",
      "step: 22120 / loss: 3.093050 / acc: 0.03333\n",
      "step: 22130 / loss: 3.097229 / acc: 0.00000\n",
      "step: 22140 / loss: 3.099676 / acc: 0.00000\n",
      "step: 22150 / loss: 3.106981 / acc: 0.03333\n",
      "step: 22160 / loss: 3.104461 / acc: 0.03333\n",
      "step: 22170 / loss: 3.102468 / acc: 0.00000\n",
      "step: 22180 / loss: 3.095287 / acc: 0.00000\n",
      "step: 22190 / loss: 3.089137 / acc: 0.00000\n",
      "step: 22200 / loss: 3.098102 / acc: 0.03333\n",
      "step: 22210 / loss: 3.102470 / acc: 0.03333\n",
      "step: 22220 / loss: 3.100762 / acc: 0.10000\n",
      "step: 22230 / loss: 3.094470 / acc: 0.06667\n",
      "step: 22240 / loss: 3.080259 / acc: 0.06667\n",
      "step: 22250 / loss: 3.096786 / acc: 0.06667\n",
      "step: 22260 / loss: 3.100546 / acc: 0.00000\n",
      "step: 22270 / loss: 3.084243 / acc: 0.10000\n",
      "step: 22280 / loss: 3.100946 / acc: 0.00000\n",
      "step: 22290 / loss: 3.090939 / acc: 0.06667\n",
      "step: 22300 / loss: 3.082596 / acc: 0.10000\n",
      "step: 22310 / loss: 3.103013 / acc: 0.03333\n",
      "step: 22320 / loss: 3.084736 / acc: 0.00000\n",
      "step: 22330 / loss: 3.086325 / acc: 0.06667\n",
      "step: 22340 / loss: 3.078343 / acc: 0.10000\n",
      "step: 22350 / loss: 3.083794 / acc: 0.00000\n",
      "step: 22360 / loss: 3.097819 / acc: 0.03333\n",
      "step: 22370 / loss: 3.091401 / acc: 0.10000\n",
      "step: 22380 / loss: 3.084327 / acc: 0.03333\n",
      "step: 22390 / loss: 3.092772 / acc: 0.06667\n",
      "step: 22400 / loss: 3.101918 / acc: 0.00000\n",
      "step: 22410 / loss: 3.096575 / acc: 0.00000\n",
      "step: 22420 / loss: 3.087148 / acc: 0.06667\n",
      "step: 22430 / loss: 3.093095 / acc: 0.10000\n",
      "step: 22440 / loss: 3.101541 / acc: 0.03333\n",
      "step: 22450 / loss: 3.101441 / acc: 0.03333\n",
      "step: 22460 / loss: 3.092697 / acc: 0.03333\n",
      "step: 22470 / loss: 3.092120 / acc: 0.06667\n",
      "step: 22480 / loss: 3.086263 / acc: 0.06667\n",
      "step: 22490 / loss: 3.097977 / acc: 0.00000\n",
      "step: 22500 / loss: 3.088819 / acc: 0.06667\n",
      "step: 22510 / loss: 3.090512 / acc: 0.10000\n",
      "step: 22520 / loss: 3.077992 / acc: 0.06667\n",
      "step: 22530 / loss: 3.100451 / acc: 0.00000\n",
      "step: 22540 / loss: 3.095523 / acc: 0.03333\n",
      "step: 22550 / loss: 3.101822 / acc: 0.00000\n",
      "step: 22560 / loss: 3.082943 / acc: 0.13333\n",
      "step: 22570 / loss: 3.096256 / acc: 0.06667\n",
      "step: 22580 / loss: 3.096573 / acc: 0.06667\n",
      "step: 22590 / loss: 3.101599 / acc: 0.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 22600 / loss: 3.093820 / acc: 0.06667\n",
      "step: 22610 / loss: 3.089662 / acc: 0.00000\n",
      "step: 22620 / loss: 3.083999 / acc: 0.06667\n",
      "step: 22630 / loss: 3.093971 / acc: 0.03333\n",
      "step: 22640 / loss: 3.094699 / acc: 0.06667\n",
      "step: 22650 / loss: 3.091839 / acc: 0.03333\n",
      "step: 22660 / loss: 3.101394 / acc: 0.00000\n",
      "step: 22670 / loss: 3.099607 / acc: 0.00000\n",
      "step: 22680 / loss: 3.089421 / acc: 0.06667\n",
      "step: 22690 / loss: 3.087256 / acc: 0.06667\n",
      "step: 22700 / loss: 3.087459 / acc: 0.06667\n",
      "step: 22710 / loss: 3.097270 / acc: 0.03333\n",
      "step: 22720 / loss: 3.098293 / acc: 0.06667\n",
      "step: 22730 / loss: 3.096084 / acc: 0.03333\n",
      "step: 22740 / loss: 3.092781 / acc: 0.00000\n",
      "step: 22750 / loss: 3.098173 / acc: 0.03333\n",
      "step: 22760 / loss: 3.083798 / acc: 0.03333\n",
      "step: 22770 / loss: 3.090818 / acc: 0.00000\n",
      "step: 22780 / loss: 3.107553 / acc: 0.06667\n",
      "step: 22790 / loss: 3.081717 / acc: 0.06667\n",
      "step: 22800 / loss: 3.086507 / acc: 0.06667\n",
      "step: 22810 / loss: 3.096800 / acc: 0.00000\n",
      "step: 22820 / loss: 3.083527 / acc: 0.06667\n",
      "step: 22830 / loss: 3.100098 / acc: 0.00000\n",
      "step: 22840 / loss: 3.084714 / acc: 0.03333\n",
      "step: 22850 / loss: 3.092235 / acc: 0.03333\n",
      "step: 22860 / loss: 3.107703 / acc: 0.03333\n",
      "step: 22870 / loss: 3.089091 / acc: 0.06667\n",
      "step: 22880 / loss: 3.090326 / acc: 0.06667\n",
      "step: 22890 / loss: 3.097905 / acc: 0.00000\n",
      "step: 22900 / loss: 3.087810 / acc: 0.16667\n",
      "step: 22910 / loss: 3.102562 / acc: 0.03333\n",
      "step: 22920 / loss: 3.085412 / acc: 0.10000\n",
      "step: 22930 / loss: 3.087334 / acc: 0.03333\n",
      "step: 22940 / loss: 3.097202 / acc: 0.00000\n",
      "step: 22950 / loss: 3.100519 / acc: 0.00000\n",
      "step: 22960 / loss: 3.085738 / acc: 0.06667\n",
      "step: 22970 / loss: 3.104963 / acc: 0.03333\n",
      "step: 22980 / loss: 3.093022 / acc: 0.00000\n",
      "step: 22990 / loss: 3.090502 / acc: 0.03333\n",
      "step: 23000 / loss: 3.096243 / acc: 0.06667\n",
      "step: 23010 / loss: 3.080773 / acc: 0.06667\n",
      "step: 23020 / loss: 3.094820 / acc: 0.06667\n",
      "step: 23030 / loss: 3.082860 / acc: 0.03333\n",
      "step: 23040 / loss: 3.092490 / acc: 0.03333\n",
      "step: 23050 / loss: 3.093955 / acc: 0.03333\n",
      "step: 23060 / loss: 3.106124 / acc: 0.03333\n",
      "step: 23070 / loss: 3.099644 / acc: 0.00000\n",
      "step: 23080 / loss: 3.091613 / acc: 0.00000\n",
      "step: 23090 / loss: 3.088838 / acc: 0.06667\n",
      "step: 23100 / loss: 3.093469 / acc: 0.06667\n",
      "step: 23110 / loss: 3.093209 / acc: 0.06667\n",
      "step: 23120 / loss: 3.092054 / acc: 0.03333\n",
      "step: 23130 / loss: 3.073421 / acc: 0.13333\n",
      "step: 23140 / loss: 3.102795 / acc: 0.00000\n",
      "step: 23150 / loss: 3.083549 / acc: 0.03333\n",
      "step: 23160 / loss: 3.095681 / acc: 0.00000\n",
      "step: 23170 / loss: 3.097339 / acc: 0.03333\n",
      "step: 23180 / loss: 3.086543 / acc: 0.00000\n",
      "step: 23190 / loss: 3.093928 / acc: 0.03333\n",
      "step: 23200 / loss: 3.087738 / acc: 0.00000\n",
      "step: 23210 / loss: 3.101406 / acc: 0.00000\n",
      "step: 23220 / loss: 3.099662 / acc: 0.03333\n",
      "step: 23230 / loss: 3.098272 / acc: 0.03333\n",
      "step: 23240 / loss: 3.091379 / acc: 0.00000\n",
      "step: 23250 / loss: 3.077489 / acc: 0.10000\n",
      "step: 23260 / loss: 3.081827 / acc: 0.03333\n",
      "step: 23270 / loss: 3.099235 / acc: 0.06667\n",
      "step: 23280 / loss: 3.096642 / acc: 0.00000\n",
      "step: 23290 / loss: 3.088174 / acc: 0.06667\n",
      "step: 23300 / loss: 3.098461 / acc: 0.00000\n",
      "step: 23310 / loss: 3.087942 / acc: 0.10000\n",
      "step: 23320 / loss: 3.095553 / acc: 0.03333\n",
      "step: 23330 / loss: 3.086415 / acc: 0.03333\n",
      "step: 23340 / loss: 3.088360 / acc: 0.00000\n",
      "step: 23350 / loss: 3.097472 / acc: 0.10000\n",
      "step: 23360 / loss: 3.100314 / acc: 0.03333\n",
      "step: 23370 / loss: 3.092301 / acc: 0.03333\n",
      "step: 23380 / loss: 3.091919 / acc: 0.00000\n",
      "step: 23390 / loss: 3.088887 / acc: 0.03333\n",
      "step: 23400 / loss: 3.095699 / acc: 0.06667\n",
      "step: 23410 / loss: 3.094242 / acc: 0.03333\n",
      "step: 23420 / loss: 3.096121 / acc: 0.03333\n",
      "step: 23430 / loss: 3.106686 / acc: 0.00000\n",
      "step: 23440 / loss: 3.111325 / acc: 0.00000\n",
      "step: 23450 / loss: 3.096071 / acc: 0.03333\n",
      "step: 23460 / loss: 3.089167 / acc: 0.03333\n",
      "step: 23470 / loss: 3.098681 / acc: 0.00000\n",
      "step: 23480 / loss: 3.095560 / acc: 0.00000\n",
      "step: 23490 / loss: 3.086722 / acc: 0.03333\n",
      "step: 23500 / loss: 3.101586 / acc: 0.00000\n",
      "step: 23510 / loss: 3.096390 / acc: 0.03333\n",
      "step: 23520 / loss: 3.098190 / acc: 0.10000\n",
      "step: 23530 / loss: 3.093169 / acc: 0.03333\n",
      "step: 23540 / loss: 3.092643 / acc: 0.03333\n",
      "step: 23550 / loss: 3.100079 / acc: 0.00000\n",
      "step: 23560 / loss: 3.093150 / acc: 0.10000\n",
      "step: 23570 / loss: 3.093192 / acc: 0.03333\n",
      "step: 23580 / loss: 3.102901 / acc: 0.00000\n",
      "step: 23590 / loss: 3.086759 / acc: 0.06667\n",
      "step: 23600 / loss: 3.081893 / acc: 0.16667\n",
      "step: 23610 / loss: 3.078949 / acc: 0.10000\n",
      "step: 23620 / loss: 3.094006 / acc: 0.03333\n",
      "step: 23630 / loss: 3.089931 / acc: 0.10000\n",
      "step: 23640 / loss: 3.096168 / acc: 0.03333\n",
      "step: 23650 / loss: 3.084172 / acc: 0.03333\n",
      "step: 23660 / loss: 3.081254 / acc: 0.03333\n",
      "step: 23670 / loss: 3.080802 / acc: 0.13333\n",
      "step: 23680 / loss: 3.106771 / acc: 0.00000\n",
      "step: 23690 / loss: 3.087973 / acc: 0.00000\n",
      "step: 23700 / loss: 3.097413 / acc: 0.03333\n",
      "step: 23710 / loss: 3.089043 / acc: 0.10000\n",
      "step: 23720 / loss: 3.097814 / acc: 0.03333\n",
      "step: 23730 / loss: 3.081891 / acc: 0.06667\n",
      "step: 23740 / loss: 3.091659 / acc: 0.06667\n",
      "step: 23750 / loss: 3.096320 / acc: 0.03333\n",
      "step: 23760 / loss: 3.094227 / acc: 0.00000\n",
      "step: 23770 / loss: 3.103234 / acc: 0.03333\n",
      "step: 23780 / loss: 3.091374 / acc: 0.03333\n",
      "step: 23790 / loss: 3.088647 / acc: 0.06667\n",
      "step: 23800 / loss: 3.104306 / acc: 0.00000\n",
      "step: 23810 / loss: 3.092033 / acc: 0.03333\n",
      "step: 23820 / loss: 3.090743 / acc: 0.03333\n",
      "step: 23830 / loss: 3.096519 / acc: 0.03333\n",
      "step: 23840 / loss: 3.087506 / acc: 0.03333\n",
      "step: 23850 / loss: 3.103832 / acc: 0.03333\n",
      "step: 23860 / loss: 3.096316 / acc: 0.03333\n",
      "step: 23870 / loss: 3.097425 / acc: 0.00000\n",
      "step: 23880 / loss: 3.085928 / acc: 0.03333\n",
      "step: 23890 / loss: 3.085885 / acc: 0.00000\n",
      "step: 23900 / loss: 3.092017 / acc: 0.06667\n",
      "step: 23910 / loss: 3.092112 / acc: 0.03333\n",
      "step: 23920 / loss: 3.092598 / acc: 0.06667\n",
      "step: 23930 / loss: 3.083986 / acc: 0.10000\n",
      "step: 23940 / loss: 3.096761 / acc: 0.00000\n",
      "step: 23950 / loss: 3.096864 / acc: 0.06667\n",
      "step: 23960 / loss: 3.081121 / acc: 0.06667\n",
      "step: 23970 / loss: 3.104256 / acc: 0.00000\n",
      "step: 23980 / loss: 3.089557 / acc: 0.00000\n",
      "step: 23990 / loss: 3.093755 / acc: 0.03333\n",
      "step: 24000 / loss: 3.101810 / acc: 0.03333\n",
      "step: 24010 / loss: 3.086534 / acc: 0.00000\n",
      "step: 24020 / loss: 3.087064 / acc: 0.03333\n",
      "step: 24030 / loss: 3.096501 / acc: 0.00000\n",
      "step: 24040 / loss: 3.086987 / acc: 0.03333\n",
      "step: 24050 / loss: 3.085753 / acc: 0.06667\n",
      "step: 24060 / loss: 3.088663 / acc: 0.06667\n",
      "step: 24070 / loss: 3.098363 / acc: 0.03333\n",
      "step: 24080 / loss: 3.104415 / acc: 0.00000\n",
      "step: 24090 / loss: 3.090805 / acc: 0.03333\n",
      "step: 24100 / loss: 3.078929 / acc: 0.00000\n",
      "step: 24110 / loss: 3.108388 / acc: 0.00000\n",
      "step: 24120 / loss: 3.099205 / acc: 0.03333\n",
      "step: 24130 / loss: 3.093332 / acc: 0.03333\n",
      "step: 24140 / loss: 3.082275 / acc: 0.06667\n",
      "step: 24150 / loss: 3.095782 / acc: 0.03333\n",
      "step: 24160 / loss: 3.100760 / acc: 0.03333\n",
      "step: 24170 / loss: 3.101391 / acc: 0.00000\n",
      "step: 24180 / loss: 3.086709 / acc: 0.06667\n",
      "step: 24190 / loss: 3.103286 / acc: 0.10000\n",
      "step: 24200 / loss: 3.085114 / acc: 0.10000\n",
      "step: 24210 / loss: 3.094104 / acc: 0.03333\n",
      "step: 24220 / loss: 3.094413 / acc: 0.03333\n",
      "step: 24230 / loss: 3.092975 / acc: 0.06667\n",
      "step: 24240 / loss: 3.089414 / acc: 0.10000\n",
      "step: 24250 / loss: 3.095136 / acc: 0.03333\n",
      "step: 24260 / loss: 3.090002 / acc: 0.03333\n",
      "step: 24270 / loss: 3.100787 / acc: 0.03333\n",
      "step: 24280 / loss: 3.095547 / acc: 0.00000\n",
      "step: 24290 / loss: 3.096754 / acc: 0.06667\n",
      "step: 24300 / loss: 3.092951 / acc: 0.06667\n",
      "step: 24310 / loss: 3.089763 / acc: 0.00000\n",
      "step: 24320 / loss: 3.094208 / acc: 0.06667\n",
      "step: 24330 / loss: 3.089999 / acc: 0.03333\n",
      "step: 24340 / loss: 3.093349 / acc: 0.03333\n",
      "step: 24350 / loss: 3.098932 / acc: 0.06667\n",
      "step: 24360 / loss: 3.094116 / acc: 0.10000\n",
      "step: 24370 / loss: 3.089140 / acc: 0.10000\n",
      "step: 24380 / loss: 3.076354 / acc: 0.06667\n",
      "step: 24390 / loss: 3.098032 / acc: 0.00000\n",
      "step: 24400 / loss: 3.102692 / acc: 0.03333\n",
      "step: 24410 / loss: 3.083029 / acc: 0.00000\n",
      "step: 24420 / loss: 3.103592 / acc: 0.00000\n",
      "step: 24430 / loss: 3.098073 / acc: 0.00000\n",
      "step: 24440 / loss: 3.092236 / acc: 0.03333\n",
      "step: 24450 / loss: 3.094759 / acc: 0.03333\n",
      "step: 24460 / loss: 3.089173 / acc: 0.10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 24470 / loss: 3.102418 / acc: 0.03333\n",
      "step: 24480 / loss: 3.095439 / acc: 0.06667\n",
      "step: 24490 / loss: 3.096694 / acc: 0.00000\n",
      "step: 24500 / loss: 3.088891 / acc: 0.13333\n",
      "step: 24510 / loss: 3.090844 / acc: 0.03333\n",
      "step: 24520 / loss: 3.100078 / acc: 0.03333\n",
      "step: 24530 / loss: 3.093385 / acc: 0.06667\n",
      "step: 24540 / loss: 3.095889 / acc: 0.06667\n",
      "step: 24550 / loss: 3.111536 / acc: 0.00000\n",
      "step: 24560 / loss: 3.080969 / acc: 0.10000\n",
      "step: 24570 / loss: 3.084411 / acc: 0.03333\n",
      "step: 24580 / loss: 3.096156 / acc: 0.06667\n",
      "step: 24590 / loss: 3.090457 / acc: 0.03333\n",
      "step: 24600 / loss: 3.088220 / acc: 0.00000\n",
      "step: 24610 / loss: 3.102199 / acc: 0.00000\n",
      "step: 24620 / loss: 3.091716 / acc: 0.06667\n",
      "step: 24630 / loss: 3.074667 / acc: 0.03333\n",
      "step: 24640 / loss: 3.091465 / acc: 0.03333\n",
      "step: 24650 / loss: 3.114115 / acc: 0.00000\n",
      "step: 24660 / loss: 3.082982 / acc: 0.10000\n",
      "step: 24670 / loss: 3.090029 / acc: 0.06667\n",
      "step: 24680 / loss: 3.091123 / acc: 0.00000\n",
      "step: 24690 / loss: 3.079391 / acc: 0.16667\n",
      "step: 24700 / loss: 3.103335 / acc: 0.00000\n",
      "step: 24710 / loss: 3.081889 / acc: 0.06667\n",
      "step: 24720 / loss: 3.106970 / acc: 0.00000\n",
      "step: 24730 / loss: 3.096492 / acc: 0.06667\n",
      "step: 24740 / loss: 3.084822 / acc: 0.10000\n",
      "step: 24750 / loss: 3.093775 / acc: 0.00000\n",
      "step: 24760 / loss: 3.092841 / acc: 0.06667\n",
      "step: 24770 / loss: 3.089426 / acc: 0.06667\n",
      "step: 24780 / loss: 3.097918 / acc: 0.03333\n",
      "step: 24790 / loss: 3.086524 / acc: 0.03333\n",
      "step: 24800 / loss: 3.084907 / acc: 0.13333\n",
      "step: 24810 / loss: 3.094915 / acc: 0.03333\n",
      "step: 24820 / loss: 3.094007 / acc: 0.03333\n",
      "step: 24830 / loss: 3.090748 / acc: 0.06667\n",
      "step: 24840 / loss: 3.080010 / acc: 0.13333\n",
      "step: 24850 / loss: 3.105518 / acc: 0.06667\n",
      "step: 24860 / loss: 3.091660 / acc: 0.06667\n",
      "step: 24870 / loss: 3.102107 / acc: 0.06667\n",
      "step: 24880 / loss: 3.091873 / acc: 0.03333\n",
      "step: 24890 / loss: 3.095487 / acc: 0.00000\n",
      "step: 24900 / loss: 3.092917 / acc: 0.06667\n",
      "step: 24910 / loss: 3.097276 / acc: 0.00000\n",
      "step: 24920 / loss: 3.092984 / acc: 0.00000\n",
      "step: 24930 / loss: 3.097468 / acc: 0.03333\n",
      "step: 24940 / loss: 3.097413 / acc: 0.03333\n",
      "step: 24950 / loss: 3.082017 / acc: 0.10000\n"
     ]
    }
   ],
   "source": [
    "m = main()\n",
    "#m.input(os.path.join(m.PATH_DATASET, 'train-male.tfrecords'), m.MFCC_DIM, m.BATCH_SIZE)\n",
    "m.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
