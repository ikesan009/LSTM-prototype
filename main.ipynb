{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "class main(object):\n",
    "    \n",
    "    def __init__(self, n_class, len_seq, dim_mfcc, size_batch, learning_rate, step_train, path_tfr_train, dir_log):\n",
    "        self.n_class = n_class\n",
    "        self.len_seq = len_seq\n",
    "        self.dim_mfcc = dim_mfcc\n",
    "        self.size_batch = size_batch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.step_train = step_train\n",
    "        self.path_tfr_train = path_tfr_train\n",
    "        self.dir_log = dir_log\n",
    "\n",
    "    #TFRecordsからデータセット取り出し\n",
    "    def input(self, path_tfr):\n",
    "        file_name_queue = tf.train.string_input_producer([path_tfr])\n",
    "        reader = tf.TFRecordReader()\n",
    "        _, serialized_example = reader.read(file_name_queue)\n",
    "\n",
    "        features = tf.parse_single_example(\n",
    "            serialized_example,\n",
    "            features={\n",
    "                'label': tf.FixedLenFeature([], tf.int64),\n",
    "                'data': tf.FixedLenFeature([], tf.string),\n",
    "            })\n",
    "        \n",
    "        datas = tf.decode_raw(features['data'], tf.float32)\n",
    "        labels = tf.cast(features['label'], tf.int32)\n",
    "        \n",
    "        datas = tf.reshape(datas, [self.len_seq, self.dim_mfcc])\n",
    "        labels = tf.reshape(labels, [1])\n",
    "\n",
    "        datas, labels = tf.train.shuffle_batch(\n",
    "            [datas, labels],\n",
    "            batch_size=self.size_batch, capacity=1000+self.size_batch*self.dim_mfcc,\n",
    "            min_after_dequeue=1000\n",
    "        )\n",
    "            \n",
    "        return datas, labels\n",
    "    \n",
    "    #dirで指定されたパスが存在しない場合ディレクトリ作成\n",
    "    def make_dir(self,dir,format=False):\n",
    "        if not os.path.exists(dir):\n",
    "            os.makedirs(dir)\n",
    "        if format and os.path.exists(dir):\n",
    "            shutil.rmtree(dir)\n",
    "\n",
    "    #tensorboardのサマリに追加する\n",
    "    def variable_summaries(self, var):\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar('mean', mean)\n",
    "            with tf.name_scope('stddev'):\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            tf.summary.histogram('histogram', var)\n",
    "        \n",
    "    #重みベクトルを初期化して返す\n",
    "    def variable(self, name, shape, stddev):\n",
    "        var = tf.get_variable(name, shape=shape, initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "        return var\n",
    "        \n",
    "    #Dense\n",
    "    def Dense(self, x, s_from, s_to, stddev, l_name):\n",
    "        with tf.variable_scope(l_name) as scope:\n",
    "            weights = self.variable('weights', shape=[s_from, s_to], stddev=stddev)\n",
    "            biases = tf.get_variable('biases', shape=[s_to], initializer=tf.constant_initializer(0.0))\n",
    "            dense = tf.nn.bias_add(tf.matmul(x, weights), biases, name=scope.name)\n",
    "            self.variable_summaries(dense)\n",
    "            return dense\n",
    "        \n",
    "    #RNN\n",
    "    def RNN(self, x, l_name):\n",
    "        with tf.variable_scope(l_name) as scope:\n",
    "            x = tf.unstack(x, self.len_seq, 1)\n",
    "            lstm_cell = rnn.BasicLSTMCell(256, forget_bias=1.0)\n",
    "            outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "            self.variable_summaries(outputs[-1])\n",
    "            return outputs, states\n",
    "\n",
    "    #Model\n",
    "    def model(self, x):\n",
    "        rnn_outputs, rnn_states = self.RNN(x, 'rnn')\n",
    "        dense = self.Dense(rnn_outputs[-1], s_from=256, s_to=self.n_class, stddev=0.01, l_name='dense')\n",
    "        return dense\n",
    "        \n",
    "    #トレーニング\n",
    "    def train(self):\n",
    "        sess = tf.InteractiveSession()\n",
    "        \n",
    "        x = tf.placeholder(tf.float32, shape=[None, self.len_seq, self.dim_mfcc])\n",
    "        y_ = tf.placeholder(tf.float32, shape=[None, self.n_class])\n",
    "\n",
    "        preds = self.model(x)\n",
    "        \n",
    "        with tf.name_scope('cross_entropy'):\n",
    "            cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=y_))\n",
    "            tf.add_to_collection('losses', cross_entropy)\n",
    "            error=tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "            self.variable_summaries(error)\n",
    "\n",
    "        with tf.name_scope('accuracy'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(error)\n",
    "            correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(y_, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "            self.variable_summaries(accuracy)\n",
    "            \n",
    "        merged = tf.summary.merge_all()\n",
    "        dir_log = os.path.join(self.dir_log, 'train')\n",
    "        self.make_dir(dir_log)\n",
    "        writer = tf.summary.FileWriter(dir_log, sess.graph)\n",
    "        \n",
    "        saver = tf.train.Saver(max_to_keep=1000)\n",
    "        \n",
    "        datas, labels = self.input(self.path_tfr_train)\n",
    "        labels = tf.one_hot(labels, depth=self.n_class, dtype=tf.float32)\n",
    "        \n",
    "        n_training_iters = self.step_train * self.size_batch\n",
    "        init_op = [tf.global_variables_initializer(), tf.local_variables_initializer()]\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_op)\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "            \n",
    "            step = 1\n",
    "            while step * self.size_batch <= n_training_iters:\n",
    "                batch = sess.run([datas, labels])\n",
    "                batch[1] = batch[1].reshape([-1, self.n_class])\n",
    "                sess.run(optimizer, feed_dict={x: batch[0], y_:batch[1]})\n",
    "                if step % 100 == 0:\n",
    "                    run_options  = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                    run_metadata = tf.RunMetadata()\n",
    "                    summary = sess.run(merged,\n",
    "                        feed_dict={x: batch[0], y_:batch[1]},\n",
    "                        options=run_options, run_metadata=run_metadata)\n",
    "                    writer.add_summary(summary, step)\n",
    "                    acc = sess.run(accuracy, feed_dict={x: batch[0], y_: batch[1]})\n",
    "                    loss = sess.run(cross_entropy, feed_dict={x: batch[0], y_: batch[1]})\n",
    "                    print('step: {} / loss: {:.6f} / acc: {:.5f}'.format(step, loss, acc))\n",
    "                    dir_ckpt = os.path.join(self.dir_log, 'save_files')\n",
    "                    self.make_dir(dir_ckpt)\n",
    "                    saver.save(sess, os.path.join(dir_ckpt, 'model.ckpt'), global_step=(step))\n",
    "                step += 1\n",
    "\n",
    "            \"\"\"\n",
    "            # テスト\n",
    "            test_len = 128\n",
    "            test_batch = self.input(os.path.join(self.PATH_DATASET, 'test-male.tfrecords'), self.MFCC_DIM, test_len)\n",
    "            test_data, test_label = sess.run(test_batch)\n",
    "            test_acc = sess.run(accuracy, feed_dict={x: test_data, y: test_label})\n",
    "            print(\"Test Accuracy: {}\".format(test_acc))\n",
    "            \"\"\"\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "            writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-e4bd8b1eee19>:103: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "step: 100 / loss: 3.093469 / acc: 0.03333\n",
      "step: 200 / loss: 3.091557 / acc: 0.06667\n",
      "step: 300 / loss: 3.092114 / acc: 0.10000\n",
      "step: 400 / loss: 3.092538 / acc: 0.03333\n",
      "step: 500 / loss: 3.094308 / acc: 0.03333\n",
      "step: 600 / loss: 3.091283 / acc: 0.10000\n",
      "step: 700 / loss: 3.091805 / acc: 0.03333\n",
      "step: 800 / loss: 3.091657 / acc: 0.10000\n",
      "step: 900 / loss: 3.092301 / acc: 0.00000\n",
      "step: 1000 / loss: 3.090227 / acc: 0.00000\n"
     ]
    }
   ],
   "source": [
    "m = main(\n",
    "    n_class = 22,\n",
    "    len_seq = 300,\n",
    "    dim_mfcc = 39,\n",
    "    size_batch = 30,\n",
    "    learning_rate = 0.001,\n",
    "    step_train = 1000,\n",
    "    path_tfr_train = '/media/ikesan009/B418B4D718B499B6/research/CENSREC/dataset/train-male.tfrecords',\n",
    "    dir_log = '/media/ikesan009/B418B4D718B499B6/research/CENSREC/log'\n",
    "    )\n",
    "m.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
