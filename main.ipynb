{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "class main(object):\n",
    "    \n",
    "    def __init__(self, n_class, len_seq, dim_mfcc, size_batch, learning_rate, step_train, path_tfr_train,):\n",
    "        self.n_class = n_class\n",
    "        self.len_seq = len_seq\n",
    "        self.dim_mfcc = dim_mfcc\n",
    "        self.size_batch = size_batch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.step_train = step_train\n",
    "        self.path_tfr_train = path_tfr_train\n",
    "\n",
    "    #TFRecordsからデータセット取り出し\n",
    "    def input(self, path_tfr):\n",
    "        file_name_queue = tf.train.string_input_producer([path_tfr])\n",
    "        reader = tf.TFRecordReader()\n",
    "        _, serialized_example = reader.read(file_name_queue)\n",
    "\n",
    "        features = tf.parse_single_example(\n",
    "            serialized_example,\n",
    "            features={\n",
    "                'label': tf.FixedLenFeature([], tf.int64),\n",
    "                'data': tf.FixedLenFeature([], tf.string),\n",
    "            })\n",
    "        \n",
    "        datas = tf.decode_raw(features['data'], tf.float32)\n",
    "        labels = tf.cast(features['label'], tf.int32)\n",
    "        \n",
    "        datas = tf.reshape(datas, [self.len_seq, self.dim_mfcc])\n",
    "        labels = tf.reshape(labels, [1])\n",
    "\n",
    "        datas, labels = tf.train.shuffle_batch(\n",
    "            [datas, labels],\n",
    "            batch_size=self.size_batch, capacity=1000+self.size_batch*self.dim_mfcc,\n",
    "            min_after_dequeue=1000\n",
    "        )\n",
    "            \n",
    "        return datas, labels\n",
    "\n",
    "    #RNNモデル\n",
    "    def RNN(self, x):\n",
    "        x = tf.unstack(x, self.len_seq, 1)\n",
    "        lstm_cell = rnn.BasicLSTMCell(256, forget_bias=1.0)\n",
    "        outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "        weight = tf.Variable(tf.random_normal([256, self.n_class]))\n",
    "        bias = tf.Variable(tf.random_normal([self.n_class]))\n",
    "\n",
    "        return tf.matmul(outputs[-1], weight) + bias\n",
    "    \n",
    "    #トレーニング\n",
    "    def train(self):\n",
    "        x = tf.placeholder(tf.float32, shape=[None, self.len_seq, self.dim_mfcc])\n",
    "        y_ = tf.placeholder(tf.float32, shape=[None, self.n_class])\n",
    "\n",
    "        preds = self.RNN(x)\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=y_))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(cost)\n",
    "        correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "        datas, labels = self.input(self.path_tfr_train)\n",
    "        labels = tf.one_hot(labels, depth=self.n_class, dtype=tf.float32)\n",
    "        \n",
    "        n_training_iters = self.step_train * self.size_batch\n",
    "        init_op = [tf.global_variables_initializer(), tf.local_variables_initializer()]\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_op)\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "            step = 1\n",
    "            while step * self.size_batch < n_training_iters:\n",
    "                batch = sess.run([datas, labels])\n",
    "                batch[1] = batch[1].reshape([-1, self.n_class])\n",
    "                sess.run(optimizer, feed_dict={x: batch[0], y_:batch[1]})\n",
    "                if step % 100 == 0:\n",
    "                    acc = sess.run(accuracy, feed_dict={x: batch[0], y_: batch[1]})\n",
    "                    loss = sess.run(cost, feed_dict={x: batch[0], y_: batch[1]})\n",
    "                    print('step: {} / loss: {:.6f} / acc: {:.5f}'.format(step, loss, acc))\n",
    "                step += 1\n",
    "\n",
    "            \"\"\"\n",
    "            # テスト\n",
    "            test_len = 128\n",
    "            test_batch = self.input(os.path.join(self.PATH_DATASET, 'test-male.tfrecords'), self.MFCC_DIM, test_len)\n",
    "            test_data, test_label = sess.run(test_batch)\n",
    "            test_acc = sess.run(accuracy, feed_dict={x: test_data, y: test_label})\n",
    "            print(\"Test Accuracy: {}\".format(test_acc))\n",
    "            \"\"\"\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-09c15db23195>:61: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "step: 100 / loss: 3.070546 / acc: 0.06667\n",
      "step: 200 / loss: 3.116491 / acc: 0.03333\n",
      "step: 300 / loss: 3.114882 / acc: 0.03333\n",
      "step: 400 / loss: 3.104908 / acc: 0.06667\n",
      "step: 500 / loss: 3.137856 / acc: 0.03333\n",
      "step: 600 / loss: 3.017043 / acc: 0.13333\n",
      "step: 700 / loss: 3.086022 / acc: 0.06667\n",
      "step: 800 / loss: 3.089572 / acc: 0.03333\n",
      "step: 900 / loss: 3.099506 / acc: 0.00000\n",
      "step: 1000 / loss: 3.094058 / acc: 0.03333\n",
      "step: 1100 / loss: 3.082890 / acc: 0.10000\n",
      "step: 1200 / loss: 3.080096 / acc: 0.03333\n",
      "step: 1300 / loss: 3.063076 / acc: 0.03333\n",
      "step: 1400 / loss: 3.102589 / acc: 0.03333\n",
      "step: 1500 / loss: 3.036189 / acc: 0.00000\n",
      "step: 1600 / loss: 3.050397 / acc: 0.13333\n",
      "step: 1700 / loss: 3.136124 / acc: 0.03333\n",
      "step: 1800 / loss: 3.115227 / acc: 0.03333\n",
      "step: 1900 / loss: 3.093938 / acc: 0.06667\n",
      "step: 2000 / loss: 3.132204 / acc: 0.06667\n",
      "step: 2100 / loss: 3.095791 / acc: 0.06667\n",
      "step: 2200 / loss: 3.114536 / acc: 0.03333\n",
      "step: 2300 / loss: 3.105573 / acc: 0.00000\n",
      "step: 2400 / loss: 3.097474 / acc: 0.10000\n",
      "step: 2500 / loss: 3.080040 / acc: 0.06667\n",
      "step: 2600 / loss: 3.130812 / acc: 0.03333\n",
      "step: 2700 / loss: 3.125699 / acc: 0.03333\n",
      "step: 2800 / loss: 3.101583 / acc: 0.00000\n",
      "step: 2900 / loss: 3.126945 / acc: 0.00000\n",
      "step: 3000 / loss: 3.090624 / acc: 0.00000\n",
      "step: 3100 / loss: 3.122464 / acc: 0.00000\n",
      "step: 3200 / loss: 3.082961 / acc: 0.06667\n",
      "step: 3300 / loss: 3.126293 / acc: 0.00000\n",
      "step: 3400 / loss: 3.045997 / acc: 0.10000\n",
      "step: 3500 / loss: 3.084876 / acc: 0.03333\n",
      "step: 3600 / loss: 3.034323 / acc: 0.03333\n",
      "step: 3700 / loss: 3.076982 / acc: 0.06667\n",
      "step: 3800 / loss: 3.107311 / acc: 0.03333\n",
      "step: 3900 / loss: 3.107128 / acc: 0.00000\n",
      "step: 4000 / loss: 3.111693 / acc: 0.06667\n",
      "step: 4100 / loss: 3.087916 / acc: 0.03333\n",
      "step: 4200 / loss: 3.104285 / acc: 0.10000\n",
      "step: 4300 / loss: 3.069271 / acc: 0.03333\n",
      "step: 4400 / loss: 3.112994 / acc: 0.00000\n",
      "step: 4500 / loss: 3.123871 / acc: 0.00000\n",
      "step: 4600 / loss: 3.104259 / acc: 0.10000\n",
      "step: 4700 / loss: 3.100681 / acc: 0.00000\n",
      "step: 4800 / loss: 3.105645 / acc: 0.00000\n",
      "step: 4900 / loss: 3.096671 / acc: 0.03333\n",
      "step: 5000 / loss: 3.120100 / acc: 0.06667\n",
      "step: 5100 / loss: 3.071815 / acc: 0.10000\n",
      "step: 5200 / loss: 3.115989 / acc: 0.00000\n",
      "step: 5300 / loss: 3.073510 / acc: 0.16667\n",
      "step: 5400 / loss: 3.131582 / acc: 0.00000\n",
      "step: 5500 / loss: 3.083860 / acc: 0.06667\n",
      "step: 5600 / loss: 3.066621 / acc: 0.13333\n",
      "step: 5700 / loss: 3.090812 / acc: 0.10000\n",
      "step: 5800 / loss: 3.066947 / acc: 0.00000\n",
      "step: 5900 / loss: 3.092537 / acc: 0.03333\n",
      "step: 6000 / loss: 3.108281 / acc: 0.00000\n",
      "step: 6100 / loss: 3.101845 / acc: 0.00000\n",
      "step: 6200 / loss: 3.116699 / acc: 0.10000\n",
      "step: 6300 / loss: 3.098550 / acc: 0.03333\n",
      "step: 6400 / loss: 3.082669 / acc: 0.06667\n",
      "step: 6500 / loss: 3.103848 / acc: 0.00000\n",
      "step: 6600 / loss: 3.073962 / acc: 0.10000\n",
      "step: 6700 / loss: 3.112512 / acc: 0.06667\n",
      "step: 6800 / loss: 3.097989 / acc: 0.03333\n",
      "step: 6900 / loss: 3.110142 / acc: 0.03333\n",
      "step: 7000 / loss: 3.070657 / acc: 0.03333\n",
      "step: 7100 / loss: 3.098424 / acc: 0.13333\n",
      "step: 7200 / loss: 3.093501 / acc: 0.00000\n",
      "step: 7300 / loss: 3.104012 / acc: 0.00000\n",
      "step: 7400 / loss: 3.109977 / acc: 0.03333\n",
      "step: 7500 / loss: 3.070447 / acc: 0.03333\n",
      "step: 7600 / loss: 3.092185 / acc: 0.00000\n",
      "step: 7700 / loss: 3.064144 / acc: 0.10000\n",
      "step: 7800 / loss: 3.084510 / acc: 0.06667\n",
      "step: 7900 / loss: 3.071408 / acc: 0.10000\n",
      "step: 8000 / loss: 3.091918 / acc: 0.00000\n",
      "step: 8100 / loss: 3.096892 / acc: 0.00000\n",
      "step: 8200 / loss: 3.123078 / acc: 0.00000\n",
      "step: 8300 / loss: 3.084370 / acc: 0.10000\n",
      "step: 8400 / loss: 3.110157 / acc: 0.03333\n",
      "step: 8500 / loss: 3.085374 / acc: 0.06667\n",
      "step: 8600 / loss: 3.094330 / acc: 0.06667\n",
      "step: 8700 / loss: 3.086814 / acc: 0.06667\n",
      "step: 8800 / loss: 3.097357 / acc: 0.06667\n",
      "step: 8900 / loss: 3.103081 / acc: 0.03333\n",
      "step: 9000 / loss: 3.073470 / acc: 0.03333\n",
      "step: 9100 / loss: 3.098272 / acc: 0.03333\n",
      "step: 9200 / loss: 3.097757 / acc: 0.00000\n",
      "step: 9300 / loss: 3.083473 / acc: 0.00000\n",
      "step: 9400 / loss: 3.088143 / acc: 0.03333\n",
      "step: 9500 / loss: 3.077613 / acc: 0.06667\n",
      "step: 9600 / loss: 3.068956 / acc: 0.13333\n",
      "step: 9700 / loss: 3.095373 / acc: 0.00000\n",
      "step: 9800 / loss: 3.098486 / acc: 0.00000\n",
      "step: 9900 / loss: 3.088911 / acc: 0.00000\n"
     ]
    }
   ],
   "source": [
    "m = main(\n",
    "    n_class = 22,\n",
    "    len_seq = 300,\n",
    "    dim_mfcc = 39,\n",
    "    size_batch = 30,\n",
    "    learning_rate = 0.001,\n",
    "    step_train = 10000,\n",
    "    path_tfr_train = '/media/ikesan009/B418B4D718B499B6/research/CENSREC/dataset/train-male.tfrecords'\n",
    "    )\n",
    "m.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
