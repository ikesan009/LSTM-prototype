{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "class main(object):\n",
    "    \n",
    "    def __init__(self, n_class, len_seq, dim_mfcc, batch_train, learning_rate, start_step, step_train, path_tfr_train, dir_log,\n",
    "                 batch_test, step_test, start_ckpt, end_ckpt, step_ckpt, path_tfr_test):\n",
    "        self.n_class = n_class\n",
    "        self.len_seq = len_seq\n",
    "        self.dim_mfcc = dim_mfcc\n",
    "        self.batch_train = batch_train\n",
    "        self.learning_rate = learning_rate\n",
    "        self.start_step = start_step\n",
    "        self.step_train = step_train\n",
    "        self.path_tfr_train = path_tfr_train\n",
    "        self.dir_log = dir_log\n",
    "        self.batch_test = batch_test\n",
    "        self.step_test = step_test\n",
    "        self.start_ckpt = start_ckpt\n",
    "        self.end_ckpt = end_ckpt\n",
    "        self.step_ckpt = step_ckpt\n",
    "        self.path_tfr_test = path_tfr_test\n",
    "\n",
    "    #TFRecordsからデータセット取り出し\n",
    "    def input(self, path_tfr):\n",
    "        file_name_queue = tf.train.string_input_producer([path_tfr])\n",
    "        reader = tf.TFRecordReader()\n",
    "        _, serialized_example = reader.read(file_name_queue)\n",
    "\n",
    "        features = tf.parse_single_example(\n",
    "            serialized_example,\n",
    "            features={\n",
    "                'label': tf.FixedLenFeature([], tf.int64),\n",
    "                'data': tf.FixedLenFeature([], tf.string),\n",
    "            })\n",
    "\n",
    "        datas = tf.decode_raw(features['data'], tf.float32)\n",
    "        labels = tf.cast(features['label'], tf.int32)\n",
    "\n",
    "        datas = tf.reshape(datas, [self.len_seq, self.dim_mfcc])\n",
    "        labels = tf.reshape(labels, [1])\n",
    "\n",
    "        datas, labels = tf.train.batch(\n",
    "            [datas, labels],\n",
    "            batch_size=self.batch_test, capacity=1000+self.batch_test*self.dim_mfcc\n",
    "        )\n",
    "\n",
    "        return datas, labels\n",
    "\n",
    "    #TFRecordsからデータセット取り出し(shuffle)\n",
    "    def input_shuffle(self, path_tfr):\n",
    "        file_name_queue = tf.train.string_input_producer([path_tfr])\n",
    "        reader = tf.TFRecordReader()\n",
    "        _, serialized_example = reader.read(file_name_queue)\n",
    "\n",
    "        features = tf.parse_single_example(\n",
    "            serialized_example,\n",
    "            features={\n",
    "                'label': tf.FixedLenFeature([], tf.int64),\n",
    "                'data': tf.FixedLenFeature([], tf.string),\n",
    "            })\n",
    "        \n",
    "        datas = tf.decode_raw(features['data'], tf.float32)\n",
    "        labels = tf.cast(features['label'], tf.int32)\n",
    "        \n",
    "        datas = tf.reshape(datas, [self.len_seq, self.dim_mfcc])\n",
    "        labels = tf.reshape(labels, [1])\n",
    "\n",
    "        datas, labels = tf.train.shuffle_batch(\n",
    "            [datas, labels],\n",
    "            batch_size=self.batch_train, capacity=1000+self.batch_train*self.dim_mfcc,\n",
    "            min_after_dequeue=1000\n",
    "        )\n",
    "            \n",
    "        return datas, labels\n",
    "    \n",
    "    #dirで指定されたパスが存在しない場合ディレクトリ作成\n",
    "    def make_dir(self,dir,format=False):\n",
    "        if not os.path.exists(dir):\n",
    "            os.makedirs(dir)\n",
    "        if format and os.path.exists(dir):\n",
    "            shutil.rmtree(dir)\n",
    "\n",
    "    #tensorboardのサマリに追加する\n",
    "    def variable_summaries(self, var):\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar('mean', mean)\n",
    "            with tf.name_scope('stddev'):\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            tf.summary.histogram('histogram', var)\n",
    "        \n",
    "    #重みベクトルを初期化して返す\n",
    "    def variable(self, name, shape, stddev):\n",
    "        var = tf.get_variable(name, shape=shape, initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "        return var\n",
    "        \n",
    "    #Linear\n",
    "    def Linear(self, x, n_inputs, n_units, stddev, l_name):\n",
    "        with tf.variable_scope(l_name) as scope:\n",
    "            weights = self.variable('weights', shape=[n_inputs, n_units], stddev=stddev)\n",
    "            biases = tf.get_variable('biases', shape=[n_units], initializer=tf.constant_initializer(0.0))\n",
    "            linear = tf.nn.bias_add(tf.matmul(x, weights), biases, name=scope.name)\n",
    "            self.variable_summaries(linear)\n",
    "            return linear\n",
    "        \n",
    "    #LSTM\n",
    "    def LSTM(self, x, n_units, l_name):\n",
    "        with tf.variable_scope(l_name) as scope:\n",
    "            lstm_cell = rnn.BasicLSTMCell(n_units, forget_bias=1.0)\n",
    "            outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "            self.variable_summaries(outputs[-1])\n",
    "            return outputs, states\n",
    "    \n",
    "    #畳込み層\n",
    "    def Conv(self, x, size, channel, n_units, stddev, l_name):\n",
    "        with tf.variable_scope(l_name) as scope:\n",
    "            filter = self.variable('weights', shape=[size, size, channel, n_units], stddev=stddev)\n",
    "            conv = tf.nn.conv2d(x, filter, strides=[1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.get_variable('biases', shape=[n_units], initializer=tf.constant_initializer(0.0))\n",
    "            bias = tf.nn.bias_add(conv, biases)\n",
    "            conv = tf.nn.relu(bias, name=scope.name)\n",
    "            self.variable_summaries(conv)\n",
    "            return conv\n",
    "        \n",
    "    #Model\n",
    "    def model(self, x):\n",
    "        x_unstack = tf.unstack(x, self.len_seq, 1)\n",
    "        x_unstack_rev = x_unstack[::-1]\n",
    "        \n",
    "        network1 = self.LSTM(x_unstack, n_units=256, l_name='lstm1')\n",
    "        network1 = self.LSTM(network1[0], n_units=256, l_name='lstm2')\n",
    "        network1 = self.LSTM(network1[0], n_units=256, l_name='lstm3')\n",
    "        #network1 = self.LSTM(network1[0], n_units=256, l_name='lstm4')        \n",
    "        \n",
    "        network2 = self.LSTM(x_unstack_rev, n_units=256, l_name='lstm_rev1')\n",
    "        network2 = self.LSTM(network2[0], n_units=256, l_name='lstm_rev2')\n",
    "        network2 = self.LSTM(network2[0], n_units=256, l_name='lstm_rev3')\n",
    "        #network2 = self.LSTM(network2[0], n_units=256, l_name='lstm_rev4')\n",
    "        network2 = network2[::-1]\n",
    "\n",
    "        network = tf.concat([network1[0][-1], network2[0][-1]], axis=1, name='concat')\n",
    "        \n",
    "        network = self.Linear(network, n_inputs=512, n_units=self.n_class, stddev=0.01, l_name='linear1')\n",
    "        \n",
    "        return network\n",
    "        \n",
    "    #トレーニング\n",
    "    def train(self):\n",
    "        sess = tf.InteractiveSession()\n",
    "        \n",
    "        x = tf.placeholder(tf.float32, shape=[None, self.len_seq, self.dim_mfcc])\n",
    "        y_ = tf.placeholder(tf.float32, shape=[None, self.n_class])\n",
    "\n",
    "        preds = self.model(x)\n",
    "        \n",
    "        with tf.name_scope('cross_entropy'):\n",
    "            cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=preds, labels=y_))\n",
    "            tf.add_to_collection('losses', cross_entropy)\n",
    "            error=tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "            self.variable_summaries(error)\n",
    "\n",
    "        with tf.name_scope('accuracy'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(error)\n",
    "            correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(y_, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "            self.variable_summaries(accuracy)\n",
    "            \n",
    "        merged = tf.summary.merge_all()\n",
    "        dir_log = os.path.join(self.dir_log, 'train')\n",
    "        self.make_dir(dir_log)\n",
    "        writer = tf.summary.FileWriter(dir_log, sess.graph)\n",
    "        \n",
    "        saver = tf.train.Saver(max_to_keep=1000)\n",
    "        \n",
    "        datas, labels = self.input_shuffle(self.path_tfr_train)\n",
    "        labels = tf.one_hot(labels, depth=self.n_class, dtype=tf.float32)\n",
    "        \n",
    "        n_training_iters = self.step_train * self.batch_train\n",
    "        init_op = [tf.global_variables_initializer(), tf.local_variables_initializer()]\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_op)\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "            \n",
    "            if(not self.start_step == 1):\n",
    "                saver.restore(sess, os.path.join(self.dir_log, 'save_files/model.ckpt-'+str(self.start_step)))\n",
    "            step = self.start_step\n",
    "            while step * self.batch_train <= n_training_iters:\n",
    "                batch = sess.run([datas, labels])\n",
    "                batch[1] = batch[1].reshape([-1, self.n_class])\n",
    "                sess.run(optimizer, feed_dict={x: batch[0], y_:batch[1]})\n",
    "                if step % 100 == 0:\n",
    "                    run_options  = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                    run_metadata = tf.RunMetadata()\n",
    "                    summary = sess.run(merged,\n",
    "                        feed_dict={x: batch[0], y_:batch[1]},\n",
    "                        options=run_options, run_metadata=run_metadata)\n",
    "                    writer.add_summary(summary, step)\n",
    "                    acc = sess.run(accuracy, feed_dict={x: batch[0], y_: batch[1]})\n",
    "                    loss = sess.run(cross_entropy, feed_dict={x: batch[0], y_: batch[1]})\n",
    "                    print('step: {} / loss: {:.6f} / acc: {:.5f}'.format(step, loss, acc))\n",
    "                    dir_ckpt = os.path.join(self.dir_log, 'save_files')\n",
    "                    self.make_dir(dir_ckpt)\n",
    "                    saver.save(sess, os.path.join(dir_ckpt, 'model.ckpt'), global_step=(step))\n",
    "                step += 1\n",
    "\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "            writer.close()\n",
    "            \n",
    "    #テスト\n",
    "    def test(self):\n",
    "        sess = tf.InteractiveSession()\n",
    "\n",
    "        x = tf.placeholder(tf.float32, shape=[None, self.len_seq, self.dim_mfcc])\n",
    "        y_ = tf.placeholder(tf.float32, shape=[None, self.n_class])\n",
    "\n",
    "        preds = self.model(x)\n",
    "\n",
    "        with tf.name_scope('cross_entropy'):\n",
    "            cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=preds, labels=y_))\n",
    "            tf.add_to_collection('losses', cross_entropy)\n",
    "            error=tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "            self.variable_summaries(error)\n",
    "\n",
    "        with tf.name_scope('accuracy'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(error)\n",
    "            correct_pred = tf.equal(tf.argmax(preds, 1), tf.argmax(y_, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "            self.variable_summaries(accuracy)\n",
    "\n",
    "        merged = tf.summary.merge_all()\n",
    "        dir_log = os.path.join(self.dir_log, 'test')\n",
    "        self.make_dir(dir_log)\n",
    "        writer = tf.summary.FileWriter(dir_log, sess.graph)\n",
    "\n",
    "        saver = tf.train.Saver(max_to_keep=1000)\n",
    "\n",
    "        datas, labels = self.input(self.path_tfr_test)\n",
    "        labels = tf.one_hot(labels, depth=self.n_class, dtype=tf.float32)\n",
    "\n",
    "        n_test_iters = self.step_test * self.batch_test\n",
    "        init_op = [tf.global_variables_initializer(), tf.local_variables_initializer()]\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_op)\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "            for i in range(int((self.end_ckpt-self.start_ckpt)/self.step_ckpt+1)):\n",
    "                ckpt_current = self.start_ckpt+self.step_ckpt*i\n",
    "                saver.restore(sess, os.path.join(self.dir_log, 'save_files/model.ckpt-'+str(ckpt_current)))\n",
    "                step = 1\n",
    "                acc_all = 0.0\n",
    "                while step * self.batch_test <= n_test_iters:\n",
    "                    batch = sess.run([datas, labels])\n",
    "                    batch[1] = batch[1].reshape([-1, self.n_class])\n",
    "                    sess.run(optimizer, feed_dict={x: batch[0], y_:batch[1]})\n",
    "                    run_options  = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                    run_metadata = tf.RunMetadata()\n",
    "                    summary = sess.run(merged,\n",
    "                        feed_dict={x: batch[0], y_:batch[1]},\n",
    "                        options=run_options, run_metadata=run_metadata)\n",
    "                    writer.add_summary(summary, step)\n",
    "                    acc = sess.run(accuracy, feed_dict={x: batch[0], y_: batch[1]})\n",
    "                    acc_all += acc\n",
    "                    loss = sess.run(cross_entropy, feed_dict={x: batch[0], y_: batch[1]})\n",
    "                    #print('ckpt: {} / step: {} / loss: {:.6f} / test_acc: {:.5f}'.format(ckpt_current, step, loss, acc))\n",
    "                    step += 1\n",
    "                acc_all /= step - 1\n",
    "                print('ckpt: {} / ALL_TEST_ACC: {:.5f}'.format(ckpt_current, acc_all))\n",
    "\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "            writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-100\n",
      "ckpt: 100 / ALL_TEST_ACC: 0.09091\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-200\n",
      "ckpt: 200 / ALL_TEST_ACC: 0.09818\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-300\n",
      "ckpt: 300 / ALL_TEST_ACC: 0.09091\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-400\n",
      "ckpt: 400 / ALL_TEST_ACC: 0.09091\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-500\n",
      "ckpt: 500 / ALL_TEST_ACC: 0.09091\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-600\n",
      "ckpt: 600 / ALL_TEST_ACC: 0.00000\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-700\n",
      "ckpt: 700 / ALL_TEST_ACC: 0.20000\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-800\n",
      "ckpt: 800 / ALL_TEST_ACC: 0.19273\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-900\n",
      "ckpt: 900 / ALL_TEST_ACC: 0.17455\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-1000\n",
      "ckpt: 1000 / ALL_TEST_ACC: 0.36000\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-1100\n",
      "ckpt: 1100 / ALL_TEST_ACC: 0.30545\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-1200\n",
      "ckpt: 1200 / ALL_TEST_ACC: 0.34182\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-1300\n",
      "ckpt: 1300 / ALL_TEST_ACC: 0.37455\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-1400\n",
      "ckpt: 1400 / ALL_TEST_ACC: 0.54182\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-1500\n",
      "ckpt: 1500 / ALL_TEST_ACC: 0.60000\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-1600\n",
      "ckpt: 1600 / ALL_TEST_ACC: 0.63273\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-1700\n",
      "ckpt: 1700 / ALL_TEST_ACC: 0.69091\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-1800\n",
      "ckpt: 1800 / ALL_TEST_ACC: 0.60364\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-1900\n",
      "ckpt: 1900 / ALL_TEST_ACC: 0.68000\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-2000\n",
      "ckpt: 2000 / ALL_TEST_ACC: 0.77091\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-2100\n",
      "ckpt: 2100 / ALL_TEST_ACC: 0.73455\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-2200\n",
      "ckpt: 2200 / ALL_TEST_ACC: 0.86545\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-2300\n",
      "ckpt: 2300 / ALL_TEST_ACC: 0.78545\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-2400\n",
      "ckpt: 2400 / ALL_TEST_ACC: 0.82545\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-2500\n",
      "ckpt: 2500 / ALL_TEST_ACC: 0.87273\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-2600\n",
      "ckpt: 2600 / ALL_TEST_ACC: 0.88727\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-2700\n",
      "ckpt: 2700 / ALL_TEST_ACC: 0.85091\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-2800\n",
      "ckpt: 2800 / ALL_TEST_ACC: 0.84000\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-2900\n",
      "ckpt: 2900 / ALL_TEST_ACC: 0.90545\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-3000\n",
      "ckpt: 3000 / ALL_TEST_ACC: 0.89455\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-3100\n",
      "ckpt: 3100 / ALL_TEST_ACC: 0.89818\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-3200\n",
      "ckpt: 3200 / ALL_TEST_ACC: 0.90182\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-3300\n",
      "ckpt: 3300 / ALL_TEST_ACC: 0.93091\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-3400\n",
      "ckpt: 3400 / ALL_TEST_ACC: 0.92727\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-3500\n",
      "ckpt: 3500 / ALL_TEST_ACC: 0.88727\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-3600\n",
      "ckpt: 3600 / ALL_TEST_ACC: 0.90182\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-3700\n",
      "ckpt: 3700 / ALL_TEST_ACC: 0.89455\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-3800\n",
      "ckpt: 3800 / ALL_TEST_ACC: 0.77455\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-3900\n",
      "ckpt: 3900 / ALL_TEST_ACC: 0.92364\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-4000\n",
      "ckpt: 4000 / ALL_TEST_ACC: 0.87273\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-4100\n",
      "ckpt: 4100 / ALL_TEST_ACC: 0.85091\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-4200\n",
      "ckpt: 4200 / ALL_TEST_ACC: 0.79273\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-4300\n",
      "ckpt: 4300 / ALL_TEST_ACC: 0.78545\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-4400\n",
      "ckpt: 4400 / ALL_TEST_ACC: 0.72727\n",
      "INFO:tensorflow:Restoring parameters from /media/ikesan009/B418B4D718B499B6/research/CENSREC/log/save_files/model.ckpt-4500\n",
      "ckpt: 4500 / ALL_TEST_ACC: 0.75636\n"
     ]
    }
   ],
   "source": [
    "m = main(\n",
    "    n_class = 11,\n",
    "    len_seq = 300,\n",
    "    dim_mfcc = 42,\n",
    "    batch_train = 20,\n",
    "    learning_rate = 0.001,\n",
    "    start_step = 1,\n",
    "    step_train = 5000,\n",
    "    path_tfr_train = '/media/ikesan009/B418B4D718B499B6/research/CENSREC/dataset/train-male.tfrecords',\n",
    "    dir_log = '/media/ikesan009/B418B4D718B499B6/research/CENSREC/log',\n",
    "    batch_test = 25,\n",
    "    step_test = 11,\n",
    "    start_ckpt = 100,\n",
    "    end_ckpt = 4500,\n",
    "    step_ckpt = 100,\n",
    "    path_tfr_test = '/media/ikesan009/B418B4D718B499B6/research/CENSREC/dataset/test-male.tfrecords'\n",
    "    )\n",
    "#m.train()\n",
    "m.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
